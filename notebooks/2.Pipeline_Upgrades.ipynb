{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edcb553c-54f0-4f90-9dc8-a79c4fdbb2e9",
   "metadata": {},
   "source": [
    "# Upgrading our System\n",
    "\n",
    "In this notebook, we look at how we can use the previous building blocks that we put together to test different pipeline combinations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea3fcc-d12d-4f5d-8b2d-af26d559a2f7",
   "metadata": {},
   "source": [
    "In our previous notebook, we saw how we could evaluate a RAG system using some simple metrics such as recall. \n",
    "\n",
    "Let's see how we can use this to quickly experiment with different pipelines and get a quantiative idea of how good or bad different metrics stack up against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724d2420-e1d1-48d2-beed-f48df022ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45b7c677-d62e-4074-887b-31997603a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.models import QueryItem\n",
    "import lancedb\n",
    "from lib.eval import score_retrieval, calculate_reciprocal_rank, calculate_recall\n",
    "from lib.data import get_labels\n",
    "from lib.db import get_table\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define Scoring Parameters\n",
    "sizes = [3, 5, 10, 15, 25]\n",
    "\n",
    "eval_fns = {\"mrr\": calculate_reciprocal_rank, \"recall\": calculate_recall}\n",
    "\n",
    "# Setup Test Files\n",
    "db = lancedb.connect(\"../lance\")\n",
    "queries = get_labels(\"../data/queries_single_label.jsonl\")[:30]\n",
    "queries = [\n",
    "    QueryItem(\n",
    "        **{\"query\": item[\"query\"], \"selected_chunk_ids\": [item[\"selected_chunk_ids\"]]}\n",
    "    )\n",
    "    for item in queries\n",
    "]\n",
    "table = get_table(db, \"ms_marco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9862ed5-0217-47bf-8a50-82733de8bc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing Full Text Search now...: 100%|█████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 37.68it/s]\n",
      "Generating Embeddings for 30 queries: 100%|██████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 7326.30it/s]\n",
      "Executing Vector Search now...: 100%|████████████████████████████████████████████████████████████████████| 30/30 [00:02<00:00, 12.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------------+\n",
      "|           |   Full Text Search |   Semantic Search |\n",
      "+===========+====================+===================+\n",
      "| mrr@3     |               0.25 |              0.37 |\n",
      "+-----------+--------------------+-------------------+\n",
      "| mrr@5     |               0.29 |              0.41 |\n",
      "+-----------+--------------------+-------------------+\n",
      "| mrr@10    |               0.34 |              0.42 |\n",
      "+-----------+--------------------+-------------------+\n",
      "| mrr@15    |               0.34 |              0.42 |\n",
      "+-----------+--------------------+-------------------+\n",
      "| mrr@25    |               0.34 |              0.42 |\n",
      "+-----------+--------------------+-------------------+\n",
      "| recall@3  |               0.33 |              0.67 |\n",
      "+-----------+--------------------+-------------------+\n",
      "| recall@5  |               0.5  |              0.87 |\n",
      "+-----------+--------------------+-------------------+\n",
      "| recall@10 |               0.8  |              0.93 |\n",
      "+-----------+--------------------+-------------------+\n",
      "| recall@15 |               0.83 |              1    |\n",
      "+-----------+--------------------+-------------------+\n",
      "| recall@25 |               0.9  |              1    |\n",
      "+-----------+--------------------+-------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.query import fts_search, vector_search\n",
    "\n",
    "# Define Candidate Search Methods\n",
    "candidates = {\"Full Text Search\": fts_search, \"Semantic Search\": vector_search}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for candidate, search_fn in candidates.items():\n",
    "    search_results = search_fn(table, queries, 25)\n",
    "    chunk_ids = [\n",
    "        [item[\"chunk_id\"] for item in retrieved_items]\n",
    "        for retrieved_items in search_results\n",
    "    ]\n",
    "    evaluation_metrics = [\n",
    "        score_retrieval(retrieved_chunk_ids, query.selected_chunk_ids, sizes, eval_fns)\n",
    "        for retrieved_chunk_ids, query in zip(chunk_ids, queries)\n",
    "    ]\n",
    "    results[f\"{candidate}\"] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(df.round(2), headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979dd59b-3373-4c81-8cc2-0c9620679ce9",
   "metadata": {},
   "source": [
    "We can immediately notice a few different things for our system\n",
    "\n",
    "- We can match `recall@5` for Semantic Search using say a larger value of `k` for Full Text Search. The `recall@10` for full text search is 0.9 while the `recall@5` for Semantic search is 0.86\n",
    "- Our MRR isn't the best for Full Text Search and we never quite match up to the performance of full text search.\n",
    "\n",
    "What other options do we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441898af-39f7-4e7a-9ebd-979aacb7c755",
   "metadata": {},
   "source": [
    "# Our Evaluation Dataset\n",
    "\n",
    "What exactly have we been benchmarking our results with and what's included within the MsMarco Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b19d6fd-0840-4847-ac8d-f1f5d40b8fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
       "    n_shards: 1\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\", streaming=True).take(4)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0451c401-7fb7-44a6-8a34-e4a0f682e677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': ['Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.'],\n",
       " 'passages': {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  'passage_text': [\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "   \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "   'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ',\n",
       "   'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       "   'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       "   'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.',\n",
       "   'RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       "   'vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.',\n",
       "   'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       "   'Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. '],\n",
       "  'url': ['https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "   'https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "   'http://acronyms.thefreedictionary.com/RBA',\n",
       "   'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "   'http://rba-africa.com/about/what-is-rba/',\n",
       "   'http://resultsleadership.org/what-is-results-based-accountability-rba/',\n",
       "   'http://rba-africa.com/about/what-is-rba/',\n",
       "   'http://searchsecurity.techtarget.com/definition/risk-based-authentication-RBA',\n",
       "   'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "   'http://www.rbaconsulting.com/']},\n",
       " 'query': 'what is rba',\n",
       " 'query_id': 19699,\n",
       " 'query_type': 'description',\n",
       " 'wellFormedAnswers': []}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = next(iter(dataset))\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93f84c3f-24de-43da-92ac-a65882fb881f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " 'passage_text': [\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "  \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "  'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ',\n",
       "  'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       "  'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       "  'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.',\n",
       "  'RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       "  'vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.',\n",
       "  'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       "  'Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. '],\n",
       " 'url': ['https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "  'https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "  'http://acronyms.thefreedictionary.com/RBA',\n",
       "  'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "  'http://rba-africa.com/about/what-is-rba/',\n",
       "  'http://resultsleadership.org/what-is-results-based-accountability-rba/',\n",
       "  'http://rba-africa.com/about/what-is-rba/',\n",
       "  'http://searchsecurity.techtarget.com/definition/risk-based-authentication-RBA',\n",
       "  'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "  'http://www.rbaconsulting.com/']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[\"passages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b01a1e4-dabb-450d-9416-4ba0c0daf792",
   "metadata": {},
   "source": [
    "The MS-Marco dataset is a dataset that benchmarks the ability of models to do information retrieval. \n",
    "\n",
    "Each item in the dataset consists of a few different bits of information\n",
    "\n",
    "- `query` : This is the query that the original user made on Bing\n",
    "- `passages` \n",
    "  - `is_selected` : This is a binary label that indicates whether a human annotator found this query relevant when composing a response to the query. A 1 indicates that it's relevant and 0 indicates that it's not. **Note that multiple passages can be selected as relevant**\n",
    "  - `passage_test` : This is a list of passages that was returned which corresponds to the `is_selected` index\n",
    "\n",
    "We then compute a unique chunk_id for each passage using the `hashlib` library with the `md5` hash. This then allows us to use each item and its corresponding hash to generate two small test files in `.jsonl` format.\n",
    "\n",
    "- `queries_multi_label` : This contains a mapping of query to one or more selected passages\n",
    "\n",
    "    ```\n",
    "    {\"query\": \"in animals somatic cells are produced by and gametic cells are produced by\", \"selected_chunk_ids\":  [\"4ff0ed20afce65c76bdb0df809ef5025\", \"0f556defd6442767c1fee0e729f942fb\"]}\n",
    "    ```\n",
    "  \n",
    "- `queries_single_label`: This contains a mapping of a query to a single selected passage\n",
    "\n",
    "    ```\n",
    "    {\"query\": \"how much time can you go between oil changes\", \"selected_chunk_id\": \"c944888dc7bb30a1a01cf5c19776a19b\"}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f17733f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5eb63bbbe01eeed093cb22bb8f5acdc3', '68df723b3e61541ec5af9c6053357942')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def compute_md5_hash(input_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Compute the MD5 hash of a given string.\n",
    "\n",
    "    Parameters:\n",
    "    input_string (str): The input string to hash.\n",
    "\n",
    "    Returns:\n",
    "    str: The MD5 hash of the input string.\n",
    "    \"\"\"\n",
    "    md5_hash = hashlib.md5(input_string.encode())\n",
    "    return md5_hash.hexdigest()\n",
    "\n",
    "\n",
    "compute_md5_hash(\"hello world\"), compute_md5_hash(\"HEllo world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee5dff-b0d3-4e58-afbc-4747af22a78b",
   "metadata": {},
   "source": [
    "## Hybrid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876555f-2b0c-4dac-b2fc-8b18d9926e4a",
   "metadata": {},
   "source": [
    "If we look at the `query_type` for LanceDB, we'll notice that there's an additional query_type called `hybrid`. What's this new query type and how does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4495b046-9248-4daa-a7f8-6f407b88e0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing Full Text Search now...: 100%|█████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 39.98it/s]\n",
      "Generating Embeddings for 30 queries: 100%|██████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 7876.63it/s]\n",
      "Executing Vector Search now...: 100%|████████████████████████████████████████████████████████████████████| 30/30 [00:02<00:00, 12.02it/s]\n",
      "Executing Hybrid Search now...: 100%|████████████████████████████████████████████████████████████████████| 30/30 [00:18<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------------+-----------------+\n",
      "|           |   Full Text Search |   Semantic Search |   Hybrid Search |\n",
      "+===========+====================+===================+=================+\n",
      "| mrr@3     |               0.25 |              0.37 |            0.36 |\n",
      "+-----------+--------------------+-------------------+-----------------+\n",
      "| mrr@10    |               0.34 |              0.42 |            0.42 |\n",
      "+-----------+--------------------+-------------------+-----------------+\n",
      "| mrr@15    |               0.34 |              0.42 |            0.42 |\n",
      "+-----------+--------------------+-------------------+-----------------+\n",
      "| mrr@25    |               0.34 |              0.42 |            0.42 |\n",
      "+-----------+--------------------+-------------------+-----------------+\n",
      "| recall@3  |               0.33 |              0.67 |            0.63 |\n",
      "+-----------+--------------------+-------------------+-----------------+\n",
      "| recall@10 |               0.8  |              0.93 |            0.97 |\n",
      "+-----------+--------------------+-------------------+-----------------+\n",
      "| recall@15 |               0.83 |              1    |            1    |\n",
      "+-----------+--------------------+-------------------+-----------------+\n",
      "| recall@25 |               0.9  |              1    |            1    |\n",
      "+-----------+--------------------+-------------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.models import QueryItem\n",
    "from lib.query import fts_search, vector_search, hybrid_search\n",
    "import lancedb\n",
    "from lib.eval import score_retrieval, calculate_reciprocal_rank, calculate_recall\n",
    "from lib.data import get_labels\n",
    "from lib.db import get_table\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define Scoring Parameters\n",
    "sizes = [3, 10, 15, 25]\n",
    "\n",
    "eval_fns = {\"mrr\": calculate_reciprocal_rank, \"recall\": calculate_recall}\n",
    "\n",
    "# Setup Test Files\n",
    "db = lancedb.connect(\"../lance\")\n",
    "queries = get_labels(\"../data/queries_single_label.jsonl\")[:30]\n",
    "queries = [\n",
    "    QueryItem(\n",
    "        **{\"query\": item[\"query\"], \"selected_chunk_ids\": [item[\"selected_chunk_ids\"]]}\n",
    "    )\n",
    "    for item in queries\n",
    "]\n",
    "table = get_table(db, \"ms_marco\")\n",
    "\n",
    "# Define Candidate Search Methods\n",
    "candidates = {\"Full Text Search\": fts_search, \"Semantic Search\": vector_search,\"Hybrid Search\":hybrid_search}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for candidate, search_fn in candidates.items():\n",
    "    search_results = search_fn(table, queries, 25)\n",
    "    chunk_ids = [\n",
    "        [item[\"chunk_id\"] for item in retrieved_items]\n",
    "        for retrieved_items in search_results\n",
    "    ]\n",
    "    evaluation_metrics = [\n",
    "        score_retrieval(retrieved_chunk_ids, query.selected_chunk_ids, sizes, eval_fns)\n",
    "        for retrieved_chunk_ids, query in zip(chunk_ids, queries)\n",
    "    ]\n",
    "    results[f\"{candidate}\"] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(df.round(2), headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53515bed-31c0-4dd9-93dd-cad568952d8c",
   "metadata": {},
   "source": [
    "But what's happening under the hood? Why are our new results in between Semantic Search and Full Text Search?\n",
    "\n",
    "Turns out LanceDB is actually using a linear_combination_search reranker under the hood, combining the value of semantic search + full text search distance metrics with a weight of 0.7 for semantic search and 0.3 for full text search by default. [Link to Documentation](https://lancedb.github.io/lancedb/hybrid_search/hybrid_search/#arguments)\n",
    "\n",
    "Can we tune this weight hyper-parameter and get better results using this naive linear combination reranker?\n",
    "\n",
    "```python\n",
    "from lancedb.rerankers import LinearCombinationReranker\n",
    "\n",
    "reranker = LinearCombinationReranker(weight=0.3) # Use 0.3 as the weight for vector search\n",
    "\n",
    "# We can pass in a linear reranker here to do the re-ranking\n",
    "results = table.search(\"rebel\", query_type=\"hybrid\").rerank(reranker=reranker).to_pandas()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a0533a3-d932-4fe4-93d7-0a0650e94f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Linear Combination (Weight 0.3): 100%|███████████████████████████████████████████████████████████████████| 30/30 [00:20<00:00,  1.50it/s]\n",
      "Linear Combination (Weight 0.7): 100%|███████████████████████████████████████████████████████████████████| 30/30 [00:21<00:00,  1.39it/s]\n",
      "Linear Combination (Weight 0.9): 100%|███████████████████████████████████████████████████████████████████| 30/30 [00:21<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------+----------------------------+----------------------------+\n",
      "|           |   Linear Combination (0.3) |   Linear Combination (0.7) |   Linear Combination (0.9) |\n",
      "+===========+============================+============================+============================+\n",
      "| mrr@3     |                       0.34 |                       0.36 |                       0.36 |\n",
      "+-----------+----------------------------+----------------------------+----------------------------+\n",
      "| mrr@10    |                       0.4  |                       0.42 |                       0.42 |\n",
      "+-----------+----------------------------+----------------------------+----------------------------+\n",
      "| mrr@15    |                       0.4  |                       0.42 |                       0.42 |\n",
      "+-----------+----------------------------+----------------------------+----------------------------+\n",
      "| mrr@25    |                       0.4  |                       0.42 |                       0.42 |\n",
      "+-----------+----------------------------+----------------------------+----------------------------+\n",
      "| recall@3  |                       0.63 |                       0.63 |                       0.63 |\n",
      "+-----------+----------------------------+----------------------------+----------------------------+\n",
      "| recall@10 |                       0.97 |                       0.97 |                       0.97 |\n",
      "+-----------+----------------------------+----------------------------+----------------------------+\n",
      "| recall@15 |                       1    |                       1    |                       1    |\n",
      "+-----------+----------------------------+----------------------------+----------------------------+\n",
      "| recall@25 |                       1    |                       1    |                       1    |\n",
      "+-----------+----------------------------+----------------------------+----------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.query import linear_combination_search, semantic_search\n",
    "from lib.data import get_labels\n",
    "from lib.eval import score_retrieval, calculate_reciprocal_rank, calculate_recall\n",
    "from lib.models import EmbeddedPassage\n",
    "from lib.db import get_table\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "from tabulate import tabulate\n",
    "\n",
    "db = lancedb.connect(\"../lance\")\n",
    "\n",
    "weightage = [0.3, 0.7, 0.9]\n",
    "\n",
    "queries = get_labels(\"../data/queries_single_label.jsonl\")[:30]\n",
    "queries = [\n",
    "    QueryItem(\n",
    "        **{\"query\": item[\"query\"], \"selected_chunk_ids\": [item[\"selected_chunk_ids\"]]}\n",
    "    )\n",
    "    for item in queries\n",
    "]\n",
    "table = get_table(db, \"ms_marco\", EmbeddedPassage)\n",
    "\n",
    "# Define Scoring Parameters\n",
    "sizes = [3, 10, 15, 25]\n",
    "\n",
    "eval_fns = {\"mrr\": calculate_reciprocal_rank, \"recall\": calculate_recall}\n",
    "\n",
    "# Run test_data against candidates\n",
    "results = {}\n",
    "\n",
    "for weight in weightage:\n",
    "    search_results = linear_combination_search(table, queries, 25, weight)\n",
    "    chunk_ids = [\n",
    "        [item[\"chunk_id\"] for item in retrieved_items]\n",
    "        for retrieved_items in search_results\n",
    "    ]\n",
    "    evaluation_metrics = [\n",
    "        score_retrieval(retrieved_chunk_ids, query.selected_chunk_ids, sizes, eval_fns)\n",
    "        for retrieved_chunk_ids, query in zip(chunk_ids, queries)\n",
    "    ]\n",
    "    results[f\"Linear Combination ({weight})\"] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(df.round(2), headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a1f95-3de6-408f-970a-dd636c75c551",
   "metadata": {},
   "source": [
    "# Re-Rankers\n",
    "\n",
    "Re-Rankers will often be much more accurate at finding relevant documents as compared to simple embedding search because they're able to extract out significantly more information from the query (and the text itself)\n",
    "\n",
    "Typically we'd utilise a re-ranker in a two step re-ranking process\n",
    "\n",
    "1. First fetch the relevant chunks\n",
    "2. Throw into a re-ranker\n",
    "3. Then return a subset of the re-ranked elements\n",
    "\n",
    "This will allow us to take advantage of the fast retrieval of embedding search to quickly narrow down the subset of evaluated chunks while combining the increased accuracy of a re-ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5b39f-ef0c-4dcc-b252-60b90444dfea",
   "metadata": {},
   "source": [
    "## Cohere Re-Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f1ca917-5ad9-4983-85e2-a00428e8a40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.query import linear_combination_search, semantic_search\n",
    "from lib.data import get_labels\n",
    "from lib.models import EmbeddedPassage\n",
    "from lib.db import get_table\n",
    "import lancedb\n",
    "from lib.eval import score_retrieval, calculate_reciprocal_rank, calculate_recall\n",
    "\n",
    "db = lancedb.connect(\"../lance\")\n",
    "test_data = get_labels(\"../data/queries_single_label.jsonl\")[:20]\n",
    "table = get_table(db, \"ms_marco\", EmbeddedPassage)\n",
    "\n",
    "sizes = [3,5,10]\n",
    "eval_fns = {\"mrr\": calculate_reciprocal_rank, \"recall\": calculate_recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffd0d9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is rba',\n",
       " 'selected_chunk_ids': 'ca869ae1ed3f5021cb5b2a0b78cc846c'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85be5c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@3': 0,\n",
       " 'mrr@5': 0,\n",
       " 'mrr@10': 0.167,\n",
       " 'recall@3': 0.0,\n",
       " 'recall@5': 0.0,\n",
       " 'recall@10': 1.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normal_search(query, table, limit):\n",
    "    return [\n",
    "        item[\"chunk_id\"]\n",
    "        for item in table.search(query, query_type=\"fts\")\n",
    "        .limit(limit)\n",
    "        .to_list()\n",
    "    ]\n",
    "\n",
    "\n",
    "query = test_data[0]\n",
    "retrieved_chunk_ids = normal_search(query[\"query\"], table, 25)\n",
    "evaluation_metrics = score_retrieval(retrieved_chunk_ids, query[\"selected_chunk_ids\"],sizes,eval_fns)\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1dd4dcf-fa80-41c6-aed0-f2b463c9578e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@3': 1.0,\n",
       " 'mrr@5': 1.0,\n",
       " 'mrr@10': 1.0,\n",
       " 'recall@3': 1.0,\n",
       " 'recall@5': 1.0,\n",
       " 'recall@10': 1.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lancedb.rerankers import CohereReranker\n",
    "\n",
    "\n",
    "def rerank_search(query, table, limit):\n",
    "    reranker = CohereReranker(\n",
    "        model_name=\"rerank-english-v2.0\"\n",
    "    )  # This uses the rerank-english\n",
    "    return [\n",
    "        item[\"chunk_id\"]\n",
    "        for item in table.search(query, query_type=\"fts\")\n",
    "        .limit(limit)\n",
    "        .rerank(reranker=reranker)\n",
    "        .to_list()\n",
    "    ]\n",
    "\n",
    "\n",
    "query = test_data[0]\n",
    "retrieved_chunk_ids = rerank_search(query[\"query\"], table, 25)\n",
    "evaluation_metrics = score_retrieval(retrieved_chunk_ids, query[\"selected_chunk_ids\"],sizes,eval_fns)\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afce8f-1ae5-47e5-9277-e7f7aab4fc7f",
   "metadata": {},
   "source": [
    "How does a reranker improve the quality of our retrieval vs simple full text search for MRR and Recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "538759b7-b779-4754-aae0-c43bbc423299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.models import QueryItem\n",
    "from lib.query import fts_search\n",
    "import lancedb\n",
    "from lib.eval import score_retrieval, calculate_reciprocal_rank, calculate_recall\n",
    "from lib.data import get_labels\n",
    "from lib.db import get_table\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define Scoring Parameters\n",
    "sizes = [3, 10, 15, 25]\n",
    "\n",
    "eval_fns = {\"mrr\": calculate_reciprocal_rank, \"recall\": calculate_recall}\n",
    "\n",
    "# Setup Test Files\n",
    "db = lancedb.connect(\"../lance\")\n",
    "queries = get_labels(\"../data/queries_single_label.jsonl\")[:30]\n",
    "queries = [\n",
    "    QueryItem(\n",
    "        **{\"query\": item[\"query\"], \"selected_chunk_ids\": [item[\"selected_chunk_ids\"]]}\n",
    "    )\n",
    "    for item in queries\n",
    "]\n",
    "table = get_table(db, \"ms_marco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f4373f9-cc26-4e3d-849e-336c26abb034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing Full Text Search now...: 100%|█████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 38.04it/s]\n",
      "Cohere Reranker (rerank-english-v2.0): 100%|█████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------------+\n",
      "|           |   Full Text Search |   Cohere Rerank |\n",
      "+===========+====================+=================+\n",
      "| mrr@3     |               0.25 |            0.67 |\n",
      "+-----------+--------------------+-----------------+\n",
      "| mrr@10    |               0.34 |            0.69 |\n",
      "+-----------+--------------------+-----------------+\n",
      "| mrr@15    |               0.34 |            0.69 |\n",
      "+-----------+--------------------+-----------------+\n",
      "| mrr@25    |               0.34 |            0.69 |\n",
      "+-----------+--------------------+-----------------+\n",
      "| recall@3  |               0.4  |            0.9  |\n",
      "+-----------+--------------------+-----------------+\n",
      "| recall@10 |               0.9  |            1    |\n",
      "+-----------+--------------------+-----------------+\n",
      "| recall@15 |               0.9  |            1    |\n",
      "+-----------+--------------------+-----------------+\n",
      "| recall@25 |               1    |            1    |\n",
      "+-----------+--------------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.query import fts_search, cohere_rerank_search\n",
    "\n",
    "# Define Candidate Search Methods\n",
    "candidates = {\"Full Text Search\": fts_search, \"Cohere Rerank\": cohere_rerank_search}\n",
    "\n",
    "results = {}\n",
    "\n",
    "queries = queries[:10]\n",
    "for candidate, search_fn in candidates.items():\n",
    "    search_results = search_fn(table, queries, 25)\n",
    "    chunk_ids = [\n",
    "        [item[\"chunk_id\"] for item in retrieved_items]\n",
    "        for retrieved_items in search_results\n",
    "    ]\n",
    "    evaluation_metrics = [\n",
    "        score_retrieval(retrieved_chunk_ids, query.selected_chunk_ids, sizes, eval_fns)\n",
    "        for retrieved_chunk_ids, query in zip(chunk_ids, queries)\n",
    "    ]\n",
    "    results[f\"{candidate}\"] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(df.round(2), headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ba3b8-ce01-4397-98af-73b86459d9ed",
   "metadata": {},
   "source": [
    "### Sample Evaluation : Which Model to use?\n",
    "\n",
    "Cohere ships a few different re-ranker models ( `rerank-english-v3.0`, `rerank-multilingual-v3.0`, `rerank-english-v2.0`, `rerank-multilingual-v2.0`) that perform slightly differently based on the use-case. How can we determine what might work best for our use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a71405-faae-4309-8c2c-6bd6db70b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.models import QueryItem\n",
    "from lib.query import cohere_rerank_search, vector_search\n",
    "import lancedb\n",
    "from lib.eval import score_retrieval, calculate_reciprocal_rank, calculate_recall\n",
    "from lib.data import get_labels\n",
    "from lib.db import get_table\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define Scoring Parameters\n",
    "sizes = [3, 10, 15, 25]\n",
    "\n",
    "eval_fns = {\"mrr\": calculate_reciprocal_rank, \"recall\": calculate_recall}\n",
    "\n",
    "# Setup Test Files\n",
    "db = lancedb.connect(\"../lance\")\n",
    "queries = get_labels(\"../data/queries_single_label.jsonl\")[:30]\n",
    "queries = [\n",
    "    QueryItem(\n",
    "        **{\"query\": item[\"query\"], \"selected_chunk_ids\": [item[\"selected_chunk_ids\"]]}\n",
    "    )\n",
    "    for item in queries\n",
    "]\n",
    "table = get_table(db, \"ms_marco\")\n",
    "\n",
    "# Define Candidate Search Methods\n",
    "model_names = {\n",
    "    \"rr-eng-3\": \"rerank-english-v3.0\",\n",
    "    \"rr-mul-3\": \"rerank-multilingual-v3.0\",\n",
    "    \"rr-eng-2\": \"rerank-english-v2.0\",\n",
    "    \"rr-mul-2\": \"rerank-multilingual-v2.0\",\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for candidate, search_fn in candidates.items():\n",
    "    search_results = search_fn(table, queries, 25)\n",
    "    chunk_ids = [\n",
    "        [item[\"chunk_id\"] for item in retrieved_items]\n",
    "        for retrieved_items in search_results\n",
    "    ]\n",
    "    evaluation_metrics = [\n",
    "        score_retrieval(retrieved_chunk_ids, query.selected_chunk_ids, sizes, eval_fns)\n",
    "        for retrieved_chunk_ids, query in zip(chunk_ids, queries)\n",
    "    ]\n",
    "    results[f\"{candidate}\"] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(df.round(2), headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f2b4467-b890-4c9c-b2ee-0c4b38ee4cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings for 5 queries: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2807.43it/s]\n",
      "Executing Vector Search now...: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.56it/s]\n",
      "Cohere Reranker (rerank-english-v3.0): 100%|███████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.09s/it]\n",
      "Cohere Reranker (rerank-multilingual-v3.0): 100%|██████████████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.39s/it]\n",
      "Cohere Reranker (rerank-english-v2.0): 100%|███████████████████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.39s/it]\n",
      "Cohere Reranker (rerank-multilingual-v2.0): 100%|██████████████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------------+------------+------------+------------+\n",
      "|           |   Semantic Search |   rr-eng-3 |   rr-mul-3 |   rr-eng-2 |   rr-mul-2 |\n",
      "+===========+===================+============+============+============+============+\n",
      "| mrr@3     |              0.47 |       0.4  |       0.77 |       0.6  |       0.53 |\n",
      "+-----------+-------------------+------------+------------+------------+------------+\n",
      "| mrr@10    |              0.47 |       0.52 |       0.77 |       0.67 |       0.53 |\n",
      "+-----------+-------------------+------------+------------+------------+------------+\n",
      "| mrr@15    |              0.47 |       0.52 |       0.77 |       0.67 |       0.53 |\n",
      "+-----------+-------------------+------------+------------+------------+------------+\n",
      "| mrr@25    |              0.47 |       0.52 |       0.77 |       0.67 |       0.53 |\n",
      "+-----------+-------------------+------------+------------+------------+------------+\n",
      "| recall@3  |              1    |       0.4  |       1    |       0.6  |       1    |\n",
      "+-----------+-------------------+------------+------------+------------+------------+\n",
      "| recall@10 |              1    |       1    |       1    |       1    |       1    |\n",
      "+-----------+-------------------+------------+------------+------------+------------+\n",
      "| recall@15 |              1    |       1    |       1    |       1    |       1    |\n",
      "+-----------+-------------------+------------+------------+------------+------------+\n",
      "| recall@25 |              1    |       1    |       1    |       1    |       1    |\n",
      "+-----------+-------------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.query import cohere_rerank_search, vector_search\n",
    "from lib.data import get_labels\n",
    "from lib.models import EmbeddedPassage\n",
    "from lib.db import get_table\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define Scoring Parameters\n",
    "sizes = [3, 10, 15, 25]\n",
    "\n",
    "eval_fns = {\"mrr\": calculate_reciprocal_rank, \"recall\": calculate_recall}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Setup Test Files\n",
    "db = lancedb.connect(\"../lance\")\n",
    "queries = get_labels(\"../data/queries_single_label.jsonl\")[:5]\n",
    "queries = [\n",
    "    QueryItem(\n",
    "        **{\"query\": item[\"query\"], \"selected_chunk_ids\": [item[\"selected_chunk_ids\"]]}\n",
    "    )\n",
    "    for item in queries\n",
    "]\n",
    "table = get_table(db, \"ms_marco\")\n",
    "\n",
    "# Define Candidate Search Methods\n",
    "model_names = {\n",
    "    \"rr-eng-3\": \"rerank-english-v3.0\",\n",
    "    \"rr-mul-3\": \"rerank-multilingual-v3.0\",\n",
    "    \"rr-eng-2\": \"rerank-english-v2.0\",\n",
    "    \"rr-mul-2\": \"rerank-multilingual-v2.0\",\n",
    "}\n",
    "\n",
    "# Do Semantic Search\n",
    "search_results = vector_search(table, queries, 25)\n",
    "chunk_ids = [\n",
    "    [item[\"chunk_id\"] for item in retrieved_items]\n",
    "    for retrieved_items in search_results\n",
    "]\n",
    "evaluation_metrics = [\n",
    "    score_retrieval(retrieved_chunk_ids, query.selected_chunk_ids, sizes, eval_fns)\n",
    "    for retrieved_chunk_ids, query in zip(chunk_ids, queries)\n",
    "]\n",
    "results[\"Semantic Search\"] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "for header_name, model_name in model_names.items():\n",
    "    search_results = cohere_rerank_search(table, queries, 50, model_name,\"hybrid\")\n",
    "    chunk_ids = [\n",
    "        [item[\"chunk_id\"] for item in retrieved_items]\n",
    "        for retrieved_items in search_results\n",
    "    ]\n",
    "    evaluation_metrics = [\n",
    "        score_retrieval(retrieved_chunk_ids, query.selected_chunk_ids, sizes, eval_fns)\n",
    "        for retrieved_chunk_ids, query in zip(chunk_ids, queries)\n",
    "    ]\n",
    "    results[f\"{header_name}\"] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(df.round(2), headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20482164-2802-47be-864d-48134dbf67e4",
   "metadata": {},
   "source": [
    "# Metadata Ingestion\n",
    "\n",
    "We've looked at different ways that we can set up our retrieval pipeline. Let's now switch gears and see how we can experiment with metadata ingestion to improve the quality of our search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d7a57-9085-431d-98b8-2cef1d4c0e02",
   "metadata": {},
   "source": [
    "## Creating Question-Answer Pairs\n",
    "\n",
    "We previously used Instructor to generate synthethic questions and answers. Let's see how we can expand on this to improve our retrieval pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf91c53-ffe7-4cba-83ad-d6dc66fe6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"../lance\")\n",
    "table = db.open_table(\"ms_marco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27bb41ab-0c82-426b-8c5b-388dd7682a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Between any two values of a continuous random variable, there are an infinite number of other valid values. This is not the case for discrete random variables, because between any two discrete values, there is an integer number (0, 1, 2, ...) of',\n",
       " 'score': 12.768003463745117}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = table.search(\"this is a random chunk\",query_type=\"fts\").select([\"text\"]).limit(3).to_list()\n",
    "# Now for each chunk, we'll generate a question and answer pair that we'll embed into our database\n",
    "chunk = chunks[0]\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15f4461b-fb4c-494c-a1ca-4a1d5331bee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'response': QuestionAnswerResponse(chain_of_thought='The text discusses the difference between continuous and discrete random variables, specifically focusing on the fact that continuous random variables have an infinite number of possible values between any two values, whereas discrete random variables have a finite set of values between any two points. This detail will help in crafting a specific question and answer pair.', question='What is the difference between continuous and discrete random variables in terms of the possible values between any two points?', answer='Between any two values of a continuous random variable, there are an infinite number of other valid values. In contrast, for discrete random variables, between any two discrete values, there is an integer number (0, 1, 2, ...) of valid values.'),\n",
       "  'source': {'text': 'Between any two values of a continuous random variable, there are an infinite number of other valid values. This is not the case for discrete random variables, because between any two discrete values, there is an integer number (0, 1, 2, ...) of',\n",
       "   'score': 12.768003463745117}}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.synthethic import generate_question_batch\n",
    "\n",
    "question = await generate_question_batch([chunk], 20)\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23659173-a7fa-47c0-b88c-0f3b87e47a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "class EmbeddedPassageWithQA(LanceModel):\n",
    "    vector: Vector(func.ndims()) = func.VectorField()\n",
    "    chunk_id: str\n",
    "    text: str = func.SourceField()\n",
    "    source_text: str\n",
    "\n",
    "\n",
    "table_withqa = db.create_table(\n",
    "    \"ms_marco_qa_4\", schema=EmbeddedPassageWithQA, mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e469a8-32d1-43c2-82cd-9ff75263a7ca",
   "metadata": {},
   "source": [
    "We generated ~1600 questions ahead of time with GPT-4o "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981518b-55ae-4ced-af4f-53a3fdf56619",
   "metadata": {},
   "source": [
    "This took me ~1.5 mins to execute on my local computer for ~11k chunks with a semaphore of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3bf685a-081a-4c7a-ae70-254543634763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [01:33<00:00,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 11471 items in 93.18293404579163s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import lancedb\n",
    "from itertools import batched,islice\n",
    "from tqdm.asyncio import tqdm_asyncio as asyncio\n",
    "import time\n",
    "from lib.data import get_labels\n",
    "from asyncio import Semaphore\n",
    "\n",
    "cached_questions = get_labels(\"../data/synth-questions-4o.jsonl\")\n",
    "\n",
    "def get_chunks(questions,chunks):\n",
    "    for chunk in chunks:\n",
    "        chunk_id = hashlib.md5(chunk.encode()).hexdigest()\n",
    "        yield {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": chunk,\n",
    "            \"source_text\": chunk,\n",
    "        }\n",
    "        \n",
    "    for question in questions:\n",
    "        chunk_id = hashlib.md5(question[\"chunk\"].encode()).hexdigest()\n",
    "        yield {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": question[\"question\"],\n",
    "            \"source_text\": question[\"chunk\"],\n",
    "        }\n",
    "        yield {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": question[\"answer\"],\n",
    "            \"source_text\": question[\"chunk\"],\n",
    "        }\n",
    "\n",
    "text_chunks = map(lambda x:str(x),table.to_arrow()['text'])\n",
    "items = list(get_chunks(cached_questions,text_chunks))\n",
    "\n",
    "batched_items = batched(items,500)\n",
    "async_db = await lancedb.connect_async(\"../lance\")\n",
    "async_tbl = await async_db.open_table(\"ms_marco_qa_4\")\n",
    "\n",
    "start = time.time()\n",
    "await asyncio.gather(*[async_tbl.add(list(batch)) for batch in batched_items])\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Inserted {len(items)} items in {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "010328f4-a327-46de-96f8-7085113ada47",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_withqa.create_fts_index([\"text\", \"source_text\"],replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d43c456d-2b5e-4a87-9df9-741a53604438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing Full Text Search now...: 100%|███████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 44.59it/s]\n",
      "Executing Full Text Search now...: 100%|███████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 42.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+----------------+\n",
      "|           |   FTS (With Q) |   FTS (W/o  Q) |\n",
      "+===========+================+================+\n",
      "| mrr@3     |           0.28 |           0.35 |\n",
      "+-----------+----------------+----------------+\n",
      "| mrr@10    |           0.36 |           0.42 |\n",
      "+-----------+----------------+----------------+\n",
      "| mrr@15    |           0.36 |           0.42 |\n",
      "+-----------+----------------+----------------+\n",
      "| mrr@25    |           0.37 |           0.42 |\n",
      "+-----------+----------------+----------------+\n",
      "| recall@3  |           0.34 |           0.5  |\n",
      "+-----------+----------------+----------------+\n",
      "| recall@10 |           0.75 |           0.88 |\n",
      "+-----------+----------------+----------------+\n",
      "| recall@15 |           0.86 |           0.89 |\n",
      "+-----------+----------------+----------------+\n",
      "| recall@25 |           0.93 |           0.93 |\n",
      "+-----------+----------------+----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.models import QueryItem\n",
    "from lib.query import fts_search\n",
    "import lancedb\n",
    "from lib.eval import score_retrieval, calculate_reciprocal_rank, calculate_recall\n",
    "from lib.data import get_labels\n",
    "from lib.db import get_table\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "sizes = [3, 10, 15, 25]\n",
    "eval_fns = {\"mrr\": calculate_reciprocal_rank, \"recall\": calculate_recall}\n",
    "\n",
    "\n",
    "db = lancedb.connect(\"../lance\")\n",
    "queries = get_labels(\"../data/queries_single_label.jsonl\")[:100]\n",
    "queries = [\n",
    "    QueryItem(\n",
    "        **{\"query\": item[\"query\"], \"selected_chunk_ids\": [item[\"selected_chunk_ids\"]]}\n",
    "    )\n",
    "    for item in queries\n",
    "]\n",
    "table_withqa = get_table(db, \"ms_marco_qa_4\")\n",
    "table = get_table(db, \"ms_marco\")\n",
    "\n",
    "\n",
    "candidates = {\n",
    "    \"FTS\": fts_search,\n",
    "}\n",
    "\n",
    "tables = {\"With Q\": table_withqa, \"W/o  Q\": table}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for (\n",
    "    candidate_fn_tuple,\n",
    "    db_table_tuple,\n",
    ") in product(candidates.items(), tables.items()):\n",
    "    db_name, db_table = db_table_tuple\n",
    "    candidate_name, search_fn = candidate_fn_tuple\n",
    "    search_results = search_fn(db_table, queries, 25)\n",
    "    chunk_ids = [\n",
    "        [item[\"chunk_id\"] for item in retrieved_items]\n",
    "        for retrieved_items in search_results\n",
    "    ]\n",
    "    evaluation_metrics = [\n",
    "        score_retrieval(retrieved_chunk_ids, query.selected_chunk_ids, sizes, eval_fns)\n",
    "        for retrieved_chunk_ids, query in zip(chunk_ids, queries)\n",
    "    ]\n",
    "    results[f\"{candidate_name} ({db_name})\"] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(df.round(2), headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09b633c2-ae27-4e94-ba90-f3be4d7f1217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>source_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9dc539403752581ac234e01d2eb77877</td>\n",
       "      <td>The common species of Taraxacum that rapidly c...</td>\n",
       "      <td>Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...</td>\n",
       "      <td>31.960682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9dc539403752581ac234e01d2eb77877</td>\n",
       "      <td>Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...</td>\n",
       "      <td>Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...</td>\n",
       "      <td>31.862806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89c9edbed59513ee50cb83e1cfb1ebbe</td>\n",
       "      <td>Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...</td>\n",
       "      <td>Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...</td>\n",
       "      <td>31.336384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>591eae9a842ee783b157f2b033519a2e</td>\n",
       "      <td>Both species are edible in their entirety. The...</td>\n",
       "      <td>Both species are edible in their entirety. The...</td>\n",
       "      <td>30.430489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0853e1b91ecae08717962457740453fa</td>\n",
       "      <td>Description. There are considered to be about ...</td>\n",
       "      <td>Description. There are considered to be about ...</td>\n",
       "      <td>29.815567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ea830e20c43da3d767c180fee0f061de</td>\n",
       "      <td>About this species. Dandelions are well-known,...</td>\n",
       "      <td>About this species. Dandelions are well-known,...</td>\n",
       "      <td>29.554060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a5872798dffbe3077255ed0dd0dd7a12</td>\n",
       "      <td>Many Taraxacum species, such as the common dan...</td>\n",
       "      <td>Each single flower in a head is called a flore...</td>\n",
       "      <td>29.211123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a75f014e6c6f31495e04f78e8476b54d</td>\n",
       "      <td>Plant Description. Hundreds of species of dand...</td>\n",
       "      <td>Plant Description. Hundreds of species of dand...</td>\n",
       "      <td>29.060694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>89c9edbed59513ee50cb83e1cfb1ebbe</td>\n",
       "      <td>What is the lifecycle of the common dandelion ...</td>\n",
       "      <td>Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...</td>\n",
       "      <td>28.738657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a5872798dffbe3077255ed0dd0dd7a12</td>\n",
       "      <td>Each single flower in a head is called a flore...</td>\n",
       "      <td>Each single flower in a head is called a flore...</td>\n",
       "      <td>28.075951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a75f014e6c6f31495e04f78e8476b54d</td>\n",
       "      <td>What are the physical characteristics and nutr...</td>\n",
       "      <td>Plant Description. Hundreds of species of dand...</td>\n",
       "      <td>26.680515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ea830e20c43da3d767c180fee0f061de</td>\n",
       "      <td>What are some vernacular names for the dandeli...</td>\n",
       "      <td>About this species. Dandelions are well-known,...</td>\n",
       "      <td>26.670486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>89c9edbed59513ee50cb83e1cfb1ebbe</td>\n",
       "      <td>After flowering is finished, the dandelion flo...</td>\n",
       "      <td>Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...</td>\n",
       "      <td>25.896889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ea830e20c43da3d767c180fee0f061de</td>\n",
       "      <td>Some vernacular names for the dandelion includ...</td>\n",
       "      <td>About this species. Dandelions are well-known,...</td>\n",
       "      <td>25.285419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9dc539403752581ac234e01d2eb77877</td>\n",
       "      <td>What is the common species of Taraxacum that r...</td>\n",
       "      <td>Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...</td>\n",
       "      <td>25.155714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>591eae9a842ee783b157f2b033519a2e</td>\n",
       "      <td>Dandelions (Taraxacum) are entirely edible and...</td>\n",
       "      <td>Both species are edible in their entirety. The...</td>\n",
       "      <td>24.342096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>c97bb0fb297bff6c2ea72c07962cf198</td>\n",
       "      <td>What are some uses for dandelion flowers and w...</td>\n",
       "      <td>The common name Dandelion is given to members ...</td>\n",
       "      <td>23.314484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>c97bb0fb297bff6c2ea72c07962cf198</td>\n",
       "      <td>Dandelion flowers, which belong to the family ...</td>\n",
       "      <td>The common name Dandelion is given to members ...</td>\n",
       "      <td>22.346245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>c97bb0fb297bff6c2ea72c07962cf198</td>\n",
       "      <td>The common name Dandelion is given to members ...</td>\n",
       "      <td>The common name Dandelion is given to members ...</td>\n",
       "      <td>21.774485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a75f014e6c6f31495e04f78e8476b54d</td>\n",
       "      <td>Dandelion is a hardy perennial that can grow t...</td>\n",
       "      <td>Plant Description. Hundreds of species of dand...</td>\n",
       "      <td>21.647175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            chunk_id  \\\n",
       "0   9dc539403752581ac234e01d2eb77877   \n",
       "1   9dc539403752581ac234e01d2eb77877   \n",
       "2   89c9edbed59513ee50cb83e1cfb1ebbe   \n",
       "3   591eae9a842ee783b157f2b033519a2e   \n",
       "4   0853e1b91ecae08717962457740453fa   \n",
       "5   ea830e20c43da3d767c180fee0f061de   \n",
       "6   a5872798dffbe3077255ed0dd0dd7a12   \n",
       "7   a75f014e6c6f31495e04f78e8476b54d   \n",
       "8   89c9edbed59513ee50cb83e1cfb1ebbe   \n",
       "9   a5872798dffbe3077255ed0dd0dd7a12   \n",
       "10  a75f014e6c6f31495e04f78e8476b54d   \n",
       "11  ea830e20c43da3d767c180fee0f061de   \n",
       "12  89c9edbed59513ee50cb83e1cfb1ebbe   \n",
       "13  ea830e20c43da3d767c180fee0f061de   \n",
       "14  9dc539403752581ac234e01d2eb77877   \n",
       "15  591eae9a842ee783b157f2b033519a2e   \n",
       "16  c97bb0fb297bff6c2ea72c07962cf198   \n",
       "17  c97bb0fb297bff6c2ea72c07962cf198   \n",
       "18  c97bb0fb297bff6c2ea72c07962cf198   \n",
       "19  a75f014e6c6f31495e04f78e8476b54d   \n",
       "\n",
       "                                                 text  \\\n",
       "0   The common species of Taraxacum that rapidly c...   \n",
       "1   Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...   \n",
       "2   Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...   \n",
       "3   Both species are edible in their entirety. The...   \n",
       "4   Description. There are considered to be about ...   \n",
       "5   About this species. Dandelions are well-known,...   \n",
       "6   Many Taraxacum species, such as the common dan...   \n",
       "7   Plant Description. Hundreds of species of dand...   \n",
       "8   What is the lifecycle of the common dandelion ...   \n",
       "9   Each single flower in a head is called a flore...   \n",
       "10  What are the physical characteristics and nutr...   \n",
       "11  What are some vernacular names for the dandeli...   \n",
       "12  After flowering is finished, the dandelion flo...   \n",
       "13  Some vernacular names for the dandelion includ...   \n",
       "14  What is the common species of Taraxacum that r...   \n",
       "15  Dandelions (Taraxacum) are entirely edible and...   \n",
       "16  What are some uses for dandelion flowers and w...   \n",
       "17  Dandelion flowers, which belong to the family ...   \n",
       "18  The common name Dandelion is given to members ...   \n",
       "19  Dandelion is a hardy perennial that can grow t...   \n",
       "\n",
       "                                          source_text      score  \n",
       "0   Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...  31.960682  \n",
       "1   Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...  31.862806  \n",
       "2   Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...  31.336384  \n",
       "3   Both species are edible in their entirety. The...  30.430489  \n",
       "4   Description. There are considered to be about ...  29.815567  \n",
       "5   About this species. Dandelions are well-known,...  29.554060  \n",
       "6   Each single flower in a head is called a flore...  29.211123  \n",
       "7   Plant Description. Hundreds of species of dand...  29.060694  \n",
       "8   Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...  28.738657  \n",
       "9   Each single flower in a head is called a flore...  28.075951  \n",
       "10  Plant Description. Hundreds of species of dand...  26.680515  \n",
       "11  About this species. Dandelions are well-known,...  26.670486  \n",
       "12  Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...  25.896889  \n",
       "13  About this species. Dandelions are well-known,...  25.285419  \n",
       "14  Taraxacum /təˈraeksəkʉm/ təˈræksəkʉm is a larg...  25.155714  \n",
       "15  Both species are edible in their entirety. The...  24.342096  \n",
       "16  The common name Dandelion is given to members ...  23.314484  \n",
       "17  The common name Dandelion is given to members ...  22.346245  \n",
       "18  The common name Dandelion is given to members ...  21.774485  \n",
       "19  Plant Description. Hundreds of species of dand...  21.647175  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_withqa = get_table(db, \"ms_marco_qa_4\", EmbeddedPassageWithQA)\n",
    "desired_result = \"9dc539403752581ac234e01d2eb77877\"\n",
    "queried_results = (\n",
    "    table_withqa.search(\"what species is a dandelion\", query_type=\"fts\")\n",
    "    .select([\"chunk_id\", \"text\", \"source_text\"])\n",
    "    .limit(20)\n",
    "    .to_pandas()\n",
    ")\n",
    "queried_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf14a2b-cfdd-4670-a340-79a4c8fb0151",
   "metadata": {},
   "source": [
    "## Creating Metadata using GPT4-o \n",
    "\n",
    "Now that we've seen how to create synthethic questions, let's try looking at another method of improving our search pipeline - creating metadata when ingesting data.\n",
    "\n",
    "Documents exist in relation to one another so it's incredibly useful to use metadata when we're dealing with categories, date-based queries or other types of documents which require some form of complex user understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "865a7808-c7c3-48ad-a45e-1ab052b57908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 12.80it/s]\n",
      "20it [00:01, 12.61it/s]\n",
      "Generating Embeddings for 20 queries: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5504.34it/s]\n",
      "Executing Vector Search now...: 100%|████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 45.07it/s]\n",
      "Executing Full Text Search now...: 100%|█████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 43.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+------+-------+\n",
      "|              |   FTS w Metadata |   SS |   FTS |\n",
      "+==============+==================+======+=======+\n",
      "| precision@3  |             0.95 | 0.75 |  0.72 |\n",
      "+--------------+------------------+------+-------+\n",
      "| precision@10 |             0.95 | 0.61 |  0.52 |\n",
      "+--------------+------------------+------+-------+\n",
      "| precision@15 |             0.95 | 0.57 |  0.47 |\n",
      "+--------------+------------------+------+-------+\n",
      "| precision@25 |             0.95 | 0.53 |  0.42 |\n",
      "+--------------+------------------+------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.query import fts_search,vector_search,metadata_search\n",
    "import lancedb\n",
    "from lib.eval import score_retrieval, calculate_precision\n",
    "from lib.data import get_labels\n",
    "from lib.db import get_table\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from lib.models import QueryItem\n",
    "\n",
    "\n",
    "# Define Scoring Parameters\n",
    "sizes = [3, 10, 15, 25]\n",
    "\n",
    "eval_fns = {\"precision\": calculate_precision}\n",
    "\n",
    "# Setup Test Files\n",
    "db = lancedb.connect(\"../lance\")\n",
    "queries = get_labels(\"../data/category_questions.jsonl\")[:20]\n",
    "queries = [\n",
    "    QueryItem(\n",
    "        **{\"query\": item[\"query\"], \"selected_chunk_ids\": [item[\"category\"]]}\n",
    "    )\n",
    "    for item in queries\n",
    "]\n",
    "\n",
    "candidates = {\n",
    "    \"SS\": vector_search,\n",
    "    \"FTS\": fts_search,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "table = db.open_table(\"arxiv_papers\")\n",
    "\n",
    "search_results = await metadata_search(table, queries, 25)\n",
    "chunk_ids = [\n",
    "    [item[\"category\"] for item in retrieved_items]\n",
    "    for retrieved_items in search_results\n",
    "]\n",
    "evaluation_metrics = [\n",
    "    score_retrieval(retrieved_chunk_ids, query.selected_chunk_ids, sizes, eval_fns)\n",
    "    for retrieved_chunk_ids, query in zip(chunk_ids, queries)\n",
    "]\n",
    "\n",
    "results[\"FTS w Metadata\"] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "for candidate, search_fn in candidates.items():\n",
    "    search_results = search_fn(table, queries, 25)\n",
    "    chunk_ids = [\n",
    "        [item[\"category\"] for item in retrieved_items]\n",
    "        for retrieved_items in search_results\n",
    "    ]\n",
    "    evaluation_metrics = [\n",
    "        score_retrieval(retrieved_chunk_ids, query.selected_chunk_ids, sizes, eval_fns)\n",
    "        for retrieved_chunk_ids, query in zip(chunk_ids, queries)\n",
    "    ]\n",
    "    results[f\"{candidate}\"] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(df.round(2), headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d197e16-81c0-4d23-accf-45577c51dc22",
   "metadata": {},
   "source": [
    "### What's happening over here?\n",
    "\n",
    "Metadata can help us to filter out relevant information that cannot be captured within the context of a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7f7c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\"price\" : \"The price was 24\", \"date\": \"24-02-2024\"},\n",
    "    {\"price\":\"The price was 400\", \"date\": \"14-02-2024\"},\n",
    "    {\"price\":\"The price was -200\",\"date\": \"07-02-2024\"}\n",
    "]\n",
    "\n",
    "query = \"What was the price updated to last week?\" # Assuming today is 31-02-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67fc8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_embeddings(data):\n",
    "    return OpenAI().embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=[item[\"price\"] for item in data]\n",
    "    ).data\n",
    "\n",
    "embeddings = generate_embeddings(documents)\n",
    "documents = [\n",
    "    {\n",
    "        **document,\n",
    "        \"embedding\": embedding.embedding\n",
    "    }\n",
    "    for document,embedding in zip(documents,embeddings)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c60456b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The price was -200', 0.44006616151637845),\n",
       " ('The price was 400', 0.41957457665426845),\n",
       " ('The price was 24', 0.39200806304293756)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def similarity_search_w_document(query,documents):\n",
    "    query_embedding = generate_embeddings([{\"price\":query}])[0].embedding\n",
    "    similarity_scores = [\n",
    "        (document, np.inner(query_embedding, document['embedding']))\n",
    "        for document in documents\n",
    "    ]\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarity_scores\n",
    "\n",
    "[(item[0]['price'],item[1]) for item in similarity_search_w_document(query,documents)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64b2a8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Query(start_date='2024-02-19', end_date='2024-02-25')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "class Query(BaseModel):\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "query = \"What was the price updated to last week?\"\n",
    "\n",
    "query_obj = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":\"You are an expert query understanding Artificial Intelligence system.( Today is 02-03-2024 and it is a Saturday ). You will need to extract the dates from the query and return the start and end dates for items that are going to be relevant to the user's query. Your start date should be a Monday and the end date a Sunday\"\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":query\n",
    "        }\n",
    "    ],\n",
    "    response_model=Query\n",
    ")\n",
    "query_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af716067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date='2024-02-19' end_date='2024-02-25'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The price was 24', 0.39200775073752137)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def similarity_search_w_document_and_metadata(query,documents):\n",
    "    query_embedding = generate_embeddings([{\"price\":query}])[0].embedding\n",
    "    client = instructor.from_openai(OpenAI())\n",
    "\n",
    "    query_obj = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":\"You are an expert query understanding Artificial Intelligence system.( Today is 02-03-2024 and it is a Saturday ). You will need to extract the dates from the query and return the start and end dates for items that are going to be relevant to the user's query. Your start date should be a Monday and the end date a Sunday\"\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":query\n",
    "            }\n",
    "        ],\n",
    "        response_model=Query\n",
    "    )\n",
    "    print(query_obj)\n",
    "\n",
    "    start_date = datetime.strptime(query_obj.start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(query_obj.end_date, '%Y-%m-%d')\n",
    "\n",
    "    filtered_documents = [\n",
    "        document for document in documents\n",
    "        if start_date <= datetime.strptime(document['date'], '%d-%m-%Y') <= end_date\n",
    "    ]\n",
    "\n",
    "    similarity_scores = [\n",
    "        (document, np.inner(query_embedding, document['embedding']))\n",
    "        for document in filtered_documents\n",
    "    ]\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarity_scores\n",
    "\n",
    "[(item[0]['price'],item[1]) for item in similarity_search_w_document_and_metadata(query,documents)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c39e8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date='2023-02-27' end_date='2023-03-05'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_search_w_document_and_metadata(\"What was the price like last year?\",documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26354df-2ab9-469e-bcf1-17a68f74da27",
   "metadata": {},
   "source": [
    "### What Happened in our Example?\n",
    "\n",
    "What is happening under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb65aec8-907d-426d-becb-ab9539d71b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lib.db import get_table\n",
    "from lib.models import ArxivPaper\n",
    "\n",
    "db = lancedb.connect(\"../lance\")\n",
    "table = get_table(db, \"arxiv_papers\", ArxivPaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "368fee35-13e8-4b63-b360-609c5fb52312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Calculating Valid Domains for BDD-Based Intera...</td>\n",
       "      <td>Tarik Hadzic, Rune Moller Jensen, Henrik Reif ...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>In these notes we formally describe the func...</td>\n",
       "      <td>Title:Calculating Valid Domains for BDD-Based ...</td>\n",
       "      <td>[0.035932083, 0.018666347, 0.061117645, -0.023...</td>\n",
       "      <td>e53b8c4c0077270bbcd70953a75d2628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A study of structural properties on profiles HMMs</td>\n",
       "      <td>Juliana S Bernardes, Alberto Davila, Vitor San...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>Motivation: Profile hidden Markov Models (pH...</td>\n",
       "      <td>Title:A study of structural properties on prof...</td>\n",
       "      <td>[0.007371153, 0.014932285, -0.0031393894, -0.0...</td>\n",
       "      <td>95632f153798757abe735d5882c657b3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bayesian approach to rough set</td>\n",
       "      <td>Tshilidzi Marwala and Bodie Crossingham</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>This paper proposes an approach to training ...</td>\n",
       "      <td>Title:Bayesian approach to rough set\\nAbstract...</td>\n",
       "      <td>[-0.002857872, -0.0032559216, 0.047268394, 0.0...</td>\n",
       "      <td>d6dea0edd048c82504e98cd7e019d9de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comparing Robustness of Pairwise and Multiclas...</td>\n",
       "      <td>J. Uglov, V. Schetinin, C. Maple</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>Noise, corruptions and variations in face im...</td>\n",
       "      <td>Title:Comparing Robustness of Pairwise and Mul...</td>\n",
       "      <td>[-0.0033912328, -0.014125003, 0.036404647, -0....</td>\n",
       "      <td>8119b6693be1f30dd152dac35b8eaa73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Calculating Valid Domains for BDD-Based Intera...   \n",
       "1  A study of structural properties on profiles HMMs   \n",
       "2                     Bayesian approach to rough set   \n",
       "3  Comparing Robustness of Pairwise and Multiclas...   \n",
       "\n",
       "                                             authors category  \\\n",
       "0  Tarik Hadzic, Rune Moller Jensen, Henrik Reif ...    cs.AI   \n",
       "1  Juliana S Bernardes, Alberto Davila, Vitor San...    cs.AI   \n",
       "2            Tshilidzi Marwala and Bodie Crossingham    cs.AI   \n",
       "3                   J. Uglov, V. Schetinin, C. Maple    cs.AI   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    In these notes we formally describe the func...   \n",
       "1    Motivation: Profile hidden Markov Models (pH...   \n",
       "2    This paper proposes an approach to training ...   \n",
       "3    Noise, corruptions and variations in face im...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Title:Calculating Valid Domains for BDD-Based ...   \n",
       "1  Title:A study of structural properties on prof...   \n",
       "2  Title:Bayesian approach to rough set\\nAbstract...   \n",
       "3  Title:Comparing Robustness of Pairwise and Mul...   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [0.035932083, 0.018666347, 0.061117645, -0.023...   \n",
       "1  [0.007371153, 0.014932285, -0.0031393894, -0.0...   \n",
       "2  [-0.002857872, -0.0032559216, 0.047268394, 0.0...   \n",
       "3  [-0.0033912328, -0.014125003, 0.036404647, -0....   \n",
       "\n",
       "                           chunk_id  \n",
       "0  e53b8c4c0077270bbcd70953a75d2628  \n",
       "1  95632f153798757abe735d5882c657b3  \n",
       "2  d6dea0edd048c82504e98cd7e019d9de  \n",
       "3  8119b6693be1f30dd152dac35b8eaa73  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.to_pandas().head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2c4b7f4-5fd3-4c42-8199-2943655ddf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "cs.AI      100\n",
       "cs.IR      100\n",
       "stat.ML    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.to_pandas()['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e0a97f6-4f11-4d56-9513-244811e52c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category='cs.AI'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "from typing import Literal\n",
    "import openai\n",
    "import instructor\n",
    "\n",
    "client  = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "category_description = \"\"\"\n",
    "This represents a categorization of the user's query\n",
    "\n",
    "- stat.ML : Covers machine learning papers (supervised, unsupervised, semi-supervised learning, graphical models, reinforcement learning, bandits, high dimensional inference, etc.) with a statistical or theoretical grounding\n",
    "- cs.AI : Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.\n",
    "- cs.IR : Covers indexing, dictionaries, retrieval, content and analysis. Roughly includes material in ACM Subject Classes H.3.0, H.3.1, H.3.2, H.3.3, and H.3.4.\n",
    "\"\"\".strip()\n",
    "\n",
    "class Category(BaseModel):\n",
    "    category: Literal['cs.AI','cs.IR','stat.ML'] = Field(...,description=category_description)\n",
    "\n",
    "query = \"What potential application does Quantum Computation (QC) have to Artificial Intelligence (AI) according to the text?\"\n",
    "\n",
    "def categorize_item(query):\n",
    "    return client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert topic classifier, your job is to classify the following title into the categories provided in the response object. Make sure to classify it into one of the individual categories provided\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"The title is {query}\"\n",
    "            }\n",
    "        ],\n",
    "        model = \"gpt-4o\",\n",
    "        response_model = Category\n",
    "    )\n",
    "\n",
    "q = categorize_item(query)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fa8181f-1748-44f4-857d-6a4c810d2b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'category': 'cs.AI', 'score': 44.642051696777344},\n",
       " {'category': 'cs.AI', 'score': 16.717119216918945},\n",
       " {'category': 'cs.AI', 'score': 15.190481185913086},\n",
       " {'category': 'cs.AI', 'score': 11.489946365356445},\n",
       " {'category': 'cs.AI', 'score': 10.238595008850098},\n",
       " {'category': 'cs.AI', 'score': 9.757229804992676},\n",
       " {'category': 'cs.AI', 'score': 9.154081344604492},\n",
       " {'category': 'cs.AI', 'score': 8.645560264587402},\n",
       " {'category': 'cs.IR', 'score': 8.546699523925781},\n",
       " {'category': 'stat.ML', 'score': 8.312337875366211},\n",
       " {'category': 'cs.AI', 'score': 8.290803909301758},\n",
       " {'category': 'cs.AI', 'score': 8.105945587158203},\n",
       " {'category': 'cs.IR', 'score': 7.876897811889648},\n",
       " {'category': 'cs.IR', 'score': 7.408782005310059},\n",
       " {'category': 'cs.AI', 'score': 7.327653884887695},\n",
       " {'category': 'cs.AI', 'score': 7.243310928344727},\n",
       " {'category': 'cs.AI', 'score': 7.066513538360596},\n",
       " {'category': 'cs.IR', 'score': 6.783278465270996},\n",
       " {'category': 'cs.AI', 'score': 6.648003578186035},\n",
       " {'category': 'cs.AI', 'score': 6.570124626159668},\n",
       " {'category': 'cs.AI', 'score': 6.534304618835449},\n",
       " {'category': 'cs.AI', 'score': 6.524085521697998},\n",
       " {'category': 'stat.ML', 'score': 6.064927101135254},\n",
       " {'category': 'cs.IR', 'score': 6.0605316162109375},\n",
       " {'category': 'cs.IR', 'score': 6.039505958557129}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.search(query,query_type=\"fts\").select(['category']).limit(25).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4b63fc2-65d4-4092-ae34-46e3fed88882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'category': 'cs.AI', 'score': 44.642051696777344},\n",
       " {'category': 'cs.AI', 'score': 16.717119216918945},\n",
       " {'category': 'cs.AI', 'score': 15.190481185913086},\n",
       " {'category': 'cs.AI', 'score': 11.489946365356445},\n",
       " {'category': 'cs.AI', 'score': 10.238595008850098},\n",
       " {'category': 'cs.AI', 'score': 9.757229804992676},\n",
       " {'category': 'cs.AI', 'score': 9.154081344604492},\n",
       " {'category': 'cs.AI', 'score': 8.645560264587402},\n",
       " {'category': 'cs.AI', 'score': 8.290803909301758},\n",
       " {'category': 'cs.AI', 'score': 8.105945587158203},\n",
       " {'category': 'cs.AI', 'score': 7.327653884887695},\n",
       " {'category': 'cs.AI', 'score': 7.243310928344727},\n",
       " {'category': 'cs.AI', 'score': 7.066513538360596},\n",
       " {'category': 'cs.AI', 'score': 6.648003578186035},\n",
       " {'category': 'cs.AI', 'score': 6.570124626159668},\n",
       " {'category': 'cs.AI', 'score': 6.534304618835449},\n",
       " {'category': 'cs.AI', 'score': 6.524085521697998}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.search(query,query_type=\"fts\").where(\"category = 'cs.AI'\", prefilter=True).select(['category']).limit(25).to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
