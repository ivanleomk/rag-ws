{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd660c72-4d10-49ac-8167-625884b3f8f1",
   "metadata": {},
   "source": [
    "Goals\n",
    "\n",
    "1. How to setup lancedb locally\n",
    "2. Looking at some metrics like recall and MRR and manually calculating some examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d7bf9-2384-4fc5-a54f-597d691ec74d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Before starting this part, make sure that you have ran the `setup.py` file so that we have a lancedb db that is populated with the first 1000 entries of the ms-marco dataset. Depending on the internet, this might take a while so do make sure that you have completed this step before the workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41a1ead-536e-4ef0-a44b-b0ec2335064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28fa391b-c7db-4d12-9d40-c215ac36e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a db by using the connect function\n",
    "\n",
    "db = lancedb.connect(\"./lance-db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd7c8b1-61dd-460d-9917-573b79a7804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then create tables which can be based off a simple pydantic schema\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "\n",
    "class Entry(LanceModel):\n",
    "    vector: Vector(func.ndims()) = func.VectorField()\n",
    "    text: str = func.SourceField()\n",
    "\n",
    "table = db.create_table(\"sample_table\",schema=Entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be546caa-a3f3-4af8-8202-fb8d2c4906ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    \"The Capital of France is Paris\",\n",
    "    \"How long do you need for sydney and surrounding areas\",\n",
    "    \"Twitter is a popular web application\"\n",
    "]\n",
    "table.add([{\"text\":item} for item in sample_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1c9a254-aa75-4161-9ec7-0ae718d42fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: The Capital of France is Paris, vector: [0.026081105694174767, 0.020630236715078354], distance: 509.652\n",
      "\n",
      "text: Twitter is a popular web application, vector: [0.004896957892924547, -0.04718351364135742], distance: 510.056\n",
      "\n",
      "text: How long do you need for sydney and surrounding areas, vector: [0.00892411544919014, 0.022230438888072968], distance: 512.427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = table.search(np.random.random((1536))) \\\n",
    "    .limit(10) \\\n",
    "    .to_list()\n",
    "\n",
    "for result in results:\n",
    "    print(f\"text: {result['text']}, vector: {result['vector'][:2]}, distance: {round(result['_distance'],3)}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da7cbc06-25f9-4c83-900e-947386365161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: The Capital of France is Paris, vector: [0.026081105694174767, 0.020630236715078354], distance: 0.94\n",
      "\n",
      "text: How long do you need for sydney and surrounding areas, vector: [0.00892411544919014, 0.022230438888072968], distance: 1.611\n",
      "\n",
      "text: Twitter is a popular web application, vector: [0.004896957892924547, -0.04718351364135742], distance: 1.719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = table.search(\"Paris is a nice city to visit\") \\\n",
    "    .limit(10) \\\n",
    "    .to_list()\n",
    "\n",
    "for result in results:\n",
    "    print(f\"text: {result['text']}, vector: {result['vector'][:2]}, distance: {round(result['_distance'],3)}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "165e1b8a-117c-4fb1-992d-5b48e927f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"./lance-db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a453b7-ee4e-404c-9beb-ae8120e41851",
   "metadata": {},
   "source": [
    "**Summary** : LanceDB provides an easy way to have FTS ( as a simple baseline ) and embedding search. It also handles batching and provides other functionality such as integrations with duckdb, arrow table and filtering out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da960f25-4d35-4a4e-bf97-fdc7a6b93f7d",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Now that we've figured out how our vector db works, let's look at some metrics we want to consider. Important to note that we always take these metrics at a value of `k`.\n",
    "\n",
    "This is relevant to the specific use case (Eg. if we have a menu item that we want to show things for then we need to focus on the top 5 items )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca76b5-ad7d-4397-8e15-93719263075c",
   "metadata": {},
   "source": [
    "## Reciprocal Rank\n",
    "\n",
    "Highlights the importance of quickly surfacing at least one relevant document, with an emphasis on the efficiency of relevance delivery. Matters a lot when there are only a few items we can show to the user at any given time.\n",
    "\n",
    "Formula is $$\\frac{1}{\\text{First Relevant Item}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47b6588a-aa4b-4ca4-a4ef-4a54f0ad911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reciprocal_rank(predictions, labels):\n",
    "    for index, prediction in enumerate(predictions):\n",
    "        if prediction in labels:\n",
    "            return 1 / (index + 1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a6e6edb-b1f7-4001-99b0-f21e63f9c507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [1,2,3,4,5]\n",
    "labels = [2,4]\n",
    "\n",
    "calculate_reciprocal_rank(predictions,labels) # 1/2 = 0.5 since earliest relevant item is at index=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4ddc2a5-cdea-48bf-bc73-21517cc9764d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [1,2,3,4,5]\n",
    "labels = [10,20]\n",
    "\n",
    "calculate_reciprocal_rank(predictions,labels) # No Relevant Items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae2fe5-f0c4-409e-a338-99084b10a30c",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "Recall measures the system's capability to retrieve all relevant documents within the top K results, emphasizing the breadth of relevant information captured.\n",
    "\n",
    "Formula is $$\\frac{\\text{Number of Retrieved Relevant Items}}{\\text{Total Number of Relevant Items}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f15d227-d361-4e34-9c0f-7b4500123f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(predictions, labels):\n",
    "    correct_predictions = sum(1 for prediction in predictions if prediction in labels)\n",
    "    if labels:\n",
    "        return correct_predictions / len(labels)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d26afb77-8a47-4817-ac47-46e7612026f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [1,2,3,4,5]\n",
    "labels = [2,4]\n",
    "\n",
    "calculate_recall(predictions,labels) # 2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99fb7223-800e-4ee1-8ede-7670dc813442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [1,2,3,4,5]\n",
    "labels = [200,20]\n",
    "\n",
    "calculate_recall(predictions,labels) # 0/2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f8055b5-fb91-4b5d-b880-01e954768388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [1,2,3,4,5]\n",
    "labels = [2,10]\n",
    "\n",
    "calculate_recall(predictions,labels) # 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eae666-ef65-4c3a-a671-4210ea98d630",
   "metadata": {},
   "source": [
    "These metrics allow us to be able to see the performance of our system and quantify the performance improvements of incremental improvements over time. There are more metrics that you can track ( see our article [here](https://jxnl.co/writing/2024/02/05/when-to-lgtm-at-k/) )\n",
    "\n",
    "In short, think of the two metrics as follows\n",
    "\n",
    "- Recall: How many relevant items did we surface?\n",
    "- Reciprocal Rank: Where was the first relevant item that we care about?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c76a2-b2c6-4677-af12-9835be4f1be3",
   "metadata": {},
   "source": [
    "# Evaluating Our Data\n",
    "\n",
    "We've generated a .jsonl file with the queries from the [MS-Marco](https://huggingface.co/datasets/ms_marco) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26543905-9a81-44cf-9c2a-1dd52610f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line.strip())\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "data = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag Workshop",
   "language": "python",
   "name": "rag_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
