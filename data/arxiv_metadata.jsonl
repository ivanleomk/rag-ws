{"title":"Calculating Valid Domains for BDD-Based Interactive Configuration","authors":"Tarik Hadzic, Rune Moller Jensen, Henrik Reif Andersen","category":"cs.AI","abstract":"  In these notes we formally describe the functionality of Calculating Valid\nDomains from the BDD representing the solution space of valid configurations.\nThe formalization is largely based on the CLab configuration framework.\n","text":"Title:Calculating Valid Domains for BDD-Based Interactive Configuration\nAbstract:  In these notes we formally describe the functionality of Calculating Valid\nDomains from the BDD representing the solution space of valid configurations.\nThe formalization is largely based on the CLab configuration framework.\n","vector":null,"chunk_id":"e53b8c4c0077270bbcd70953a75d2628"}
{"title":"A study of structural properties on profiles HMMs","authors":"Juliana S Bernardes, Alberto Davila, Vitor Santos Costa, Gerson\n  Zaverucha","category":"cs.AI","abstract":"  Motivation: Profile hidden Markov Models (pHMMs) are a popular and very\nuseful tool in the detection of the remote homologue protein families.\nUnfortunately, their performance is not always satisfactory when proteins are\nin the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm\nand tool that tries to improve pHMM performance by using structural information\nwhile training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.\nEach pHMM is constructed by weighting each residue in an aligned protein\naccording to a specific structural property of the residue. Properties used\nwere primary, secondary and tertiary structures, accessibility and packing.\nHMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP\ndatabase to perform our experiments. Throughout, we apply leave-one-family-out\ncross-validation over protein superfamilies. First, we used the MAMMOTH-mult\nstructural aligner to align the training set proteins. Then, we performed two\nsets of experiments. In a first experiment, we compared structure weighted\nmodels against standard pHMMs and against each other. In a second experiment,\nwe compared the voting model against individual pHMMs. We compare method\nperformance through ROC curves and through Precision/Recall curves, and assess\nsignificance through the paired two tailed t-test. Our results show significant\nperformance improvements of all structurally weighted models over default\nHMMER, and a significant improvement in sensitivity of the combined models over\nboth the original model and the structurally weighted models.\n","text":"Title:A study of structural properties on profiles HMMs\nAbstract:  Motivation: Profile hidden Markov Models (pHMMs) are a popular and very\nuseful tool in the detection of the remote homologue protein families.\nUnfortunately, their performance is not always satisfactory when proteins are\nin the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm\nand tool that tries to improve pHMM performance by using structural information\nwhile training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.\nEach pHMM is constructed by weighting each residue in an aligned protein\naccording to a specific structural property of the residue. Properties used\nwere primary, secondary and tertiary structures, accessibility and packing.\nHMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP\ndatabase to perform our experiments. Throughout, we apply leave-one-family-out\ncross-validation over protein superfamilies. First, we used the MAMMOTH-mult\nstructural aligner to align the training set proteins. Then, we performed two\nsets of experiments. In a first experiment, we compared structure weighted\nmodels against standard pHMMs and against each other. In a second experiment,\nwe compared the voting model against individual pHMMs. We compare method\nperformance through ROC curves and through Precision/Recall curves, and assess\nsignificance through the paired two tailed t-test. Our results show significant\nperformance improvements of all structurally weighted models over default\nHMMER, and a significant improvement in sensitivity of the combined models over\nboth the original model and the structurally weighted models.\n","vector":null,"chunk_id":"95632f153798757abe735d5882c657b3"}
{"title":"Bayesian approach to rough set","authors":"Tshilidzi Marwala and Bodie Crossingham","category":"cs.AI","abstract":"  This paper proposes an approach to training rough set models using Bayesian\nframework trained using Markov Chain Monte Carlo (MCMC) method. The prior\nprobabilities are constructed from the prior knowledge that good rough set\nmodels have fewer rules. Markov Chain Monte Carlo sampling is conducted through\nsampling in the rough set granule space and Metropolis algorithm is used as an\nacceptance criteria. The proposed method is tested to estimate the risk of HIV\ngiven demographic data. The results obtained shows that the proposed approach\nis able to achieve an average accuracy of 58% with the accuracy varying up to\n66%. In addition the Bayesian rough set give the probabilities of the estimated\nHIV status as well as the linguistic rules describing how the demographic\nparameters drive the risk of HIV.\n","text":"Title:Bayesian approach to rough set\nAbstract:  This paper proposes an approach to training rough set models using Bayesian\nframework trained using Markov Chain Monte Carlo (MCMC) method. The prior\nprobabilities are constructed from the prior knowledge that good rough set\nmodels have fewer rules. Markov Chain Monte Carlo sampling is conducted through\nsampling in the rough set granule space and Metropolis algorithm is used as an\nacceptance criteria. The proposed method is tested to estimate the risk of HIV\ngiven demographic data. The results obtained shows that the proposed approach\nis able to achieve an average accuracy of 58% with the accuracy varying up to\n66%. In addition the Bayesian rough set give the probabilities of the estimated\nHIV status as well as the linguistic rules describing how the demographic\nparameters drive the risk of HIV.\n","vector":null,"chunk_id":"d6dea0edd048c82504e98cd7e019d9de"}
{"title":"Comparing Robustness of Pairwise and Multiclass Neural-Network Systems\n  for Face Recognition","authors":"J. Uglov, V. Schetinin, C. Maple","category":"cs.AI","abstract":"  Noise, corruptions and variations in face images can seriously hurt the\nperformance of face recognition systems. To make such systems robust,\nmulticlass neuralnetwork classifiers capable of learning from noisy data have\nbeen suggested. However on large face data sets such systems cannot provide the\nrobustness at a high level. In this paper we explore a pairwise neural-network\nsystem as an alternative approach to improving the robustness of face\nrecognition. In our experiments this approach is shown to outperform the\nmulticlass neural-network system in terms of the predictive accuracy on the\nface images corrupted by noise.\n","text":"Title:Comparing Robustness of Pairwise and Multiclass Neural-Network Systems\n  for Face Recognition\nAbstract:  Noise, corruptions and variations in face images can seriously hurt the\nperformance of face recognition systems. To make such systems robust,\nmulticlass neuralnetwork classifiers capable of learning from noisy data have\nbeen suggested. However on large face data sets such systems cannot provide the\nrobustness at a high level. In this paper we explore a pairwise neural-network\nsystem as an alternative approach to improving the robustness of face\nrecognition. In our experiments this approach is shown to outperform the\nmulticlass neural-network system in terms of the predictive accuracy on the\nface images corrupted by noise.\n","vector":null,"chunk_id":"8119b6693be1f30dd152dac35b8eaa73"}
{"title":"Ensemble Learning for Free with Evolutionary Algorithms ?","authors":"Christian Gagn\\'e (INFORMATIQUE WGZ INC.), Mich\\`ele Sebag (INRIA\n  Futurs), Marc Schoenauer (INRIA Futurs), Marco Tomassini (ISI)","category":"cs.AI","abstract":"  Evolutionary Learning proceeds by evolving a population of classifiers, from\nwhich it generally returns (with some notable exceptions) the single\nbest-of-run classifier as final result. In the meanwhile, Ensemble Learning,\none of the most efficient approaches in supervised Machine Learning for the\nlast decade, proceeds by building a population of diverse classifiers. Ensemble\nLearning with Evolutionary Computation thus receives increasing attention. The\nEvolutionary Ensemble Learning (EEL) approach presented in this paper features\ntwo contributions. First, a new fitness function, inspired by co-evolution and\nenforcing the classifier diversity, is presented. Further, a new selection\ncriterion based on the classification margin is proposed. This criterion is\nused to extract the classifier ensemble from the final population only\n(Off-line) or incrementally along evolution (On-line). Experiments on a set of\nbenchmark problems show that Off-line outperforms single-hypothesis\nevolutionary learning and state-of-art Boosting and generates smaller\nclassifier ensembles.\n","text":"Title:Ensemble Learning for Free with Evolutionary Algorithms ?\nAbstract:  Evolutionary Learning proceeds by evolving a population of classifiers, from\nwhich it generally returns (with some notable exceptions) the single\nbest-of-run classifier as final result. In the meanwhile, Ensemble Learning,\none of the most efficient approaches in supervised Machine Learning for the\nlast decade, proceeds by building a population of diverse classifiers. Ensemble\nLearning with Evolutionary Computation thus receives increasing attention. The\nEvolutionary Ensemble Learning (EEL) approach presented in this paper features\ntwo contributions. First, a new fitness function, inspired by co-evolution and\nenforcing the classifier diversity, is presented. Further, a new selection\ncriterion based on the classification margin is proposed. This criterion is\nused to extract the classifier ensemble from the final population only\n(Off-line) or incrementally along evolution (On-line). Experiments on a set of\nbenchmark problems show that Off-line outperforms single-hypothesis\nevolutionary learning and state-of-art Boosting and generates smaller\nclassifier ensembles.\n","vector":null,"chunk_id":"fd97d7930fb8b027877a81a5f7f58483"}
{"title":"Fault Classification in Cylinders Using Multilayer Perceptrons, Support\n  Vector Machines and Guassian Mixture Models","authors":"Tshilidzi Marwala, Unathi Mahola and Snehashish Chakraverty","category":"cs.AI","abstract":"  Gaussian mixture models (GMM) and support vector machines (SVM) are\nintroduced to classify faults in a population of cylindrical shells. The\nproposed procedures are tested on a population of 20 cylindrical shells and\ntheir performance is compared to the procedure, which uses multi-layer\nperceptrons (MLP). The modal properties extracted from vibration data are used\nto train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM\nproduces 94% classification accuracy while the MLP produces 88% classification\nrates.\n","text":"Title:Fault Classification in Cylinders Using Multilayer Perceptrons, Support\n  Vector Machines and Guassian Mixture Models\nAbstract:  Gaussian mixture models (GMM) and support vector machines (SVM) are\nintroduced to classify faults in a population of cylindrical shells. The\nproposed procedures are tested on a population of 20 cylindrical shells and\ntheir performance is compared to the procedure, which uses multi-layer\nperceptrons (MLP). The modal properties extracted from vibration data are used\nto train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM\nproduces 94% classification accuracy while the MLP produces 88% classification\nrates.\n","vector":null,"chunk_id":"d6e5b8a140bbd6572e22ae26b25db7ad"}
{"title":"Learning to Bluff","authors":"Evan Hurwitz and Tshilidzi Marwala","category":"cs.AI","abstract":"  The act of bluffing confounds game designers to this day. The very nature of\nbluffing is even open for debate, adding further complication to the process of\ncreating intelligent virtual players that can bluff, and hence play,\nrealistically. Through the use of intelligent, learning agents, and carefully\ndesigned agent outlooks, an agent can in fact learn to predict its opponents\nreactions based not only on its own cards, but on the actions of those around\nit. With this wider scope of understanding, an agent can in learn to bluff its\nopponents, with the action representing not an illogical action, as bluffing is\noften viewed, but rather as an act of maximising returns through an effective\nstatistical optimisation. By using a tee dee lambda learning algorithm to\ncontinuously adapt neural network agent intelligence, agents have been shown to\nbe able to learn to bluff without outside prompting, and even to learn to call\neach others bluffs in free, competitive play.\n","text":"Title:Learning to Bluff\nAbstract:  The act of bluffing confounds game designers to this day. The very nature of\nbluffing is even open for debate, adding further complication to the process of\ncreating intelligent virtual players that can bluff, and hence play,\nrealistically. Through the use of intelligent, learning agents, and carefully\ndesigned agent outlooks, an agent can in fact learn to predict its opponents\nreactions based not only on its own cards, but on the actions of those around\nit. With this wider scope of understanding, an agent can in learn to bluff its\nopponents, with the action representing not an illogical action, as bluffing is\noften viewed, but rather as an act of maximising returns through an effective\nstatistical optimisation. By using a tee dee lambda learning algorithm to\ncontinuously adapt neural network agent intelligence, agents have been shown to\nbe able to learn to bluff without outside prompting, and even to learn to call\neach others bluffs in free, competitive play.\n","vector":null,"chunk_id":"edea3a0b54ccddc481f4a76516cce4b4"}
{"title":"Soft constraint abstraction based on semiring homomorphism","authors":"Sanjiang Li and Mingsheng Ying","category":"cs.AI","abstract":"  The semiring-based constraint satisfaction problems (semiring CSPs), proposed\nby Bistarelli, Montanari and Rossi \\cite{BMR97}, is a very general framework of\nsoft constraints. In this paper we propose an abstraction scheme for soft\nconstraints that uses semiring homomorphism. To find optimal solutions of the\nconcrete problem, the idea is, first working in the abstract problem and\nfinding its optimal solutions, then using them to solve the concrete problem.\n  In particular, we show that a mapping preserves optimal solutions if and only\nif it is an order-reflecting semiring homomorphism. Moreover, for a semiring\nhomomorphism $\\alpha$ and a problem $P$ over $S$, if $t$ is optimal in\n$\\alpha(P)$, then there is an optimal solution $\\bar{t}$ of $P$ such that\n$\\bar{t}$ has the same value as $t$ in $\\alpha(P)$.\n","text":"Title:Soft constraint abstraction based on semiring homomorphism\nAbstract:  The semiring-based constraint satisfaction problems (semiring CSPs), proposed\nby Bistarelli, Montanari and Rossi \\cite{BMR97}, is a very general framework of\nsoft constraints. In this paper we propose an abstraction scheme for soft\nconstraints that uses semiring homomorphism. To find optimal solutions of the\nconcrete problem, the idea is, first working in the abstract problem and\nfinding its optimal solutions, then using them to solve the concrete problem.\n  In particular, we show that a mapping preserves optimal solutions if and only\nif it is an order-reflecting semiring homomorphism. Moreover, for a semiring\nhomomorphism $\\alpha$ and a problem $P$ over $S$, if $t$ is optimal in\n$\\alpha(P)$, then there is an optimal solution $\\bar{t}$ of $P$ such that\n$\\bar{t}$ has the same value as $t$ in $\\alpha(P)$.\n","vector":null,"chunk_id":"fab3d2e3b3edeb814d7fd2515eccecac"}
{"title":"Bayesian Approach to Neuro-Rough Models","authors":"Tshilidzi Marwala and Bodie Crossingham","category":"cs.AI","abstract":"  This paper proposes a neuro-rough model based on multi-layered perceptron and\nrough set. The neuro-rough model is then tested on modelling the risk of HIV\nfrom demographic data. The model is formulated using Bayesian framework and\ntrained using Monte Carlo method and Metropolis criterion. When the model was\ntested to estimate the risk of HIV infection given the demographic data it was\nfound to give the accuracy of 62%. The proposed model is able to combine the\naccuracy of the Bayesian MLP model and the transparency of Bayesian rough set\nmodel.\n","text":"Title:Bayesian Approach to Neuro-Rough Models\nAbstract:  This paper proposes a neuro-rough model based on multi-layered perceptron and\nrough set. The neuro-rough model is then tested on modelling the risk of HIV\nfrom demographic data. The model is formulated using Bayesian framework and\ntrained using Monte Carlo method and Metropolis criterion. When the model was\ntested to estimate the risk of HIV infection given the demographic data it was\nfound to give the accuracy of 62%. The proposed model is able to combine the\naccuracy of the Bayesian MLP model and the transparency of Bayesian rough set\nmodel.\n","vector":null,"chunk_id":"bfbe8cb86ae78309d9cc3af572cc453b"}
{"title":"Artificial Neural Networks and Support Vector Machines for Water Demand\n  Time Series Forecasting","authors":"Ishmael S. Msiza, Fulufhelo V. Nelwamondo and Tshilidzi Marwala","category":"cs.AI","abstract":"  Water plays a pivotal role in many physical processes, and most importantly\nin sustaining human life, animal life and plant life. Water supply entities\ntherefore have the responsibility to supply clean and safe water at the rate\nrequired by the consumer. It is therefore necessary to implement mechanisms and\nsystems that can be employed to predict both short-term and long-term water\ndemands. The increasingly growing field of computational intelligence\ntechniques has been proposed as an efficient tool in the modelling of dynamic\nphenomena. The primary objective of this paper is to compare the efficiency of\ntwo computational intelligence techniques in water demand forecasting. The\ntechniques under comparison are the Artificial Neural Networks (ANNs) and the\nSupport Vector Machines (SVMs). In this study it was observed that the ANNs\nperform better than the SVMs. This performance is measured against the\ngeneralisation ability of the two.\n","text":"Title:Artificial Neural Networks and Support Vector Machines for Water Demand\n  Time Series Forecasting\nAbstract:  Water plays a pivotal role in many physical processes, and most importantly\nin sustaining human life, animal life and plant life. Water supply entities\ntherefore have the responsibility to supply clean and safe water at the rate\nrequired by the consumer. It is therefore necessary to implement mechanisms and\nsystems that can be employed to predict both short-term and long-term water\ndemands. The increasingly growing field of computational intelligence\ntechniques has been proposed as an efficient tool in the modelling of dynamic\nphenomena. The primary objective of this paper is to compare the efficiency of\ntwo computational intelligence techniques in water demand forecasting. The\ntechniques under comparison are the Artificial Neural Networks (ANNs) and the\nSupport Vector Machines (SVMs). In this study it was observed that the ANNs\nperform better than the SVMs. This performance is measured against the\ngeneralisation ability of the two.\n","vector":null,"chunk_id":"77e6ff441e7345f62a058dd8fdb373a7"}
{"title":"Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs\n  with Missing Values","authors":"F.V. Nelwamondo and T. Marwala","category":"cs.AI","abstract":"  An ensemble based approach for dealing with missing data, without predicting\nor imputing the missing values is proposed. This technique is suitable for\nonline operations of neural networks and as a result, is used for online\ncondition monitoring. The proposed technique is tested in both classification\nand regression problems. An ensemble of Fuzzy-ARTMAPs is used for\nclassification whereas an ensemble of multi-layer perceptrons is used for the\nregression problem. Results obtained using this ensemble-based technique are\ncompared to those obtained using a combination of auto-associative neural\nnetworks and genetic algorithms and findings show that this method can perform\nup to 9% better in regression problems. Another advantage of the proposed\ntechnique is that it eliminates the need for finding the best estimate of the\ndata, and hence, saves time.\n","text":"Title:Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs\n  with Missing Values\nAbstract:  An ensemble based approach for dealing with missing data, without predicting\nor imputing the missing values is proposed. This technique is suitable for\nonline operations of neural networks and as a result, is used for online\ncondition monitoring. The proposed technique is tested in both classification\nand regression problems. An ensemble of Fuzzy-ARTMAPs is used for\nclassification whereas an ensemble of multi-layer perceptrons is used for the\nregression problem. Results obtained using this ensemble-based technique are\ncompared to those obtained using a combination of auto-associative neural\nnetworks and genetic algorithms and findings show that this method can perform\nup to 9% better in regression problems. Another advantage of the proposed\ntechnique is that it eliminates the need for finding the best estimate of the\ndata, and hence, saves time.\n","vector":null,"chunk_id":"760f28285c15530d27b13e59dc3ed9f1"}
{"title":"Artificial Intelligence for Conflict Management","authors":"E. Habtemariam, T. Marwala and M. Lagazio","category":"cs.AI","abstract":"  Militarised conflict is one of the risks that have a significant impact on\nsociety. Militarised Interstate Dispute (MID) is defined as an outcome of\ninterstate interactions, which result on either peace or conflict. Effective\nprediction of the possibility of conflict between states is an important\ndecision support tool for policy makers. In a previous research, neural\nnetworks (NNs) have been implemented to predict the MID. Support Vector\nMachines (SVMs) have proven to be very good prediction techniques and are\nintroduced for the prediction of MIDs in this study and compared to neural\nnetworks. The results show that SVMs predict MID better than NNs while NNs give\nmore consistent and easy to interpret sensitivity analysis than SVMs.\n","text":"Title:Artificial Intelligence for Conflict Management\nAbstract:  Militarised conflict is one of the risks that have a significant impact on\nsociety. Militarised Interstate Dispute (MID) is defined as an outcome of\ninterstate interactions, which result on either peace or conflict. Effective\nprediction of the possibility of conflict between states is an important\ndecision support tool for policy makers. In a previous research, neural\nnetworks (NNs) have been implemented to predict the MID. Support Vector\nMachines (SVMs) have proven to be very good prediction techniques and are\nintroduced for the prediction of MIDs in this study and compared to neural\nnetworks. The results show that SVMs predict MID better than NNs while NNs give\nmore consistent and easy to interpret sensitivity analysis than SVMs.\n","vector":null,"chunk_id":"c1ee13236858118fe895f4e50611a90b"}
{"title":"Evolving Symbolic Controllers","authors":"Nicolas Godzik (INRIA Futurs, INRIA Rocquencourt), Marc Schoenauer\n  (INRIA Futurs, INRIA Rocquencourt), Mich\\`ele Sebag (INRIA Futurs, LRI)","category":"cs.AI","abstract":"  The idea of symbolic controllers tries to bridge the gap between the top-down\nmanual design of the controller architecture, as advocated in Brooks'\nsubsumption architecture, and the bottom-up designer-free approach that is now\nstandard within the Evolutionary Robotics community. The designer provides a\nset of elementary behavior, and evolution is given the goal of assembling them\nto solve complex tasks. Two experiments are presented, demonstrating the\nefficiency and showing the recursiveness of this approach. In particular, the\nsensitivity with respect to the proposed elementary behaviors, and the\nrobustness w.r.t. generalization of the resulting controllers are studied in\ndetail.\n","text":"Title:Evolving Symbolic Controllers\nAbstract:  The idea of symbolic controllers tries to bridge the gap between the top-down\nmanual design of the controller architecture, as advocated in Brooks'\nsubsumption architecture, and the bottom-up designer-free approach that is now\nstandard within the Evolutionary Robotics community. The designer provides a\nset of elementary behavior, and evolution is given the goal of assembling them\nto solve complex tasks. Two experiments are presented, demonstrating the\nefficiency and showing the recursiveness of this approach. In particular, the\nsensitivity with respect to the proposed elementary behaviors, and the\nrobustness w.r.t. generalization of the resulting controllers are studied in\ndetail.\n","vector":null,"chunk_id":"15f8ae95e5afc909d1915115ed59ea1a"}
{"title":"Robust Multi-Cellular Developmental Design","authors":"Alexandre Devert (INRIA Futurs), Nicolas Bred\\`eche (INRIA Futurs),\n  Marc Schoenauer (INRIA Futurs)","category":"cs.AI","abstract":"  This paper introduces a continuous model for Multi-cellular Developmental\nDesign. The cells are fixed on a 2D grid and exchange \"chemicals\" with their\nneighbors during the growth process. The quantity of chemicals that a cell\nproduces, as well as the differentiation value of the cell in the phenotype,\nare controlled by a Neural Network (the genotype) that takes as inputs the\nchemicals produced by the neighboring cells at the previous time step. In the\nproposed model, the number of iterations of the growth process is not\npre-determined, but emerges during evolution: only organisms for which the\ngrowth process stabilizes give a phenotype (the stable state), others are\ndeclared nonviable. The optimization of the controller is done using the NEAT\nalgorithm, that optimizes both the topology and the weights of the Neural\nNetworks. Though each cell only receives local information from its neighbors,\nthe experimental results of the proposed approach on the 'flags' problems (the\nphenotype must match a given 2D pattern) are almost as good as those of a\ndirect regression approach using the same model with global information.\nMoreover, the resulting multi-cellular organisms exhibit almost perfect\nself-healing characteristics.\n","text":"Title:Robust Multi-Cellular Developmental Design\nAbstract:  This paper introduces a continuous model for Multi-cellular Developmental\nDesign. The cells are fixed on a 2D grid and exchange \"chemicals\" with their\nneighbors during the growth process. The quantity of chemicals that a cell\nproduces, as well as the differentiation value of the cell in the phenotype,\nare controlled by a Neural Network (the genotype) that takes as inputs the\nchemicals produced by the neighboring cells at the previous time step. In the\nproposed model, the number of iterations of the growth process is not\npre-determined, but emerges during evolution: only organisms for which the\ngrowth process stabilizes give a phenotype (the stable state), others are\ndeclared nonviable. The optimization of the controller is done using the NEAT\nalgorithm, that optimizes both the topology and the weights of the Neural\nNetworks. Though each cell only receives local information from its neighbors,\nthe experimental results of the proposed approach on the 'flags' problems (the\nphenotype must match a given 2D pattern) are almost as good as those of a\ndirect regression approach using the same model with global information.\nMoreover, the resulting multi-cellular organisms exhibit almost perfect\nself-healing characteristics.\n","vector":null,"chunk_id":"25c6426567ea46e4f7f5ea548dbceac8"}
{"title":"Ontology-Supported and Ontology-Driven Conceptual Navigation on the\n  World Wide Web","authors":"Michel Crampes (LGI2P), Sylvie Ranwez (LGI2P)","category":"cs.IR","abstract":"  This paper presents the principles of ontology-supported and ontology-driven\nconceptual navigation. Conceptual navigation realizes the independence between\nresources and links to facilitate interoperability and reusability. An engine\nbuilds dynamic links, assembles resources under an argumentative scheme and\nallows optimization with a possible constraint, such as the user's available\ntime. Among several strategies, two are discussed in detail with examples of\napplications. On the one hand, conceptual specifications for linking and\nassembling are embedded in the resource meta-description with the support of\nthe ontology of the domain to facilitate meta-communication. Resources are like\nagents looking for conceptual acquaintances with intention. On the other hand,\nthe domain ontology and an argumentative ontology drive the linking and\nassembling strategies.\n","text":"Title:Ontology-Supported and Ontology-Driven Conceptual Navigation on the\n  World Wide Web\nAbstract:  This paper presents the principles of ontology-supported and ontology-driven\nconceptual navigation. Conceptual navigation realizes the independence between\nresources and links to facilitate interoperability and reusability. An engine\nbuilds dynamic links, assembles resources under an argumentative scheme and\nallows optimization with a possible constraint, such as the user's available\ntime. Among several strategies, two are discussed in detail with examples of\napplications. On the one hand, conceptual specifications for linking and\nassembling are embedded in the resource meta-description with the support of\nthe ontology of the domain to facilitate meta-communication. Resources are like\nagents looking for conceptual acquaintances with intention. On the other hand,\nthe domain ontology and an argumentative ontology drive the linking and\nassembling strategies.\n","vector":null,"chunk_id":"897e1035fa07ae4b11f961f1d16e35f4"}
{"title":"Response Prediction of Structural System Subject to Earthquake Motions\n  using Artificial Neural Network","authors":"S. Chakraverty, T. Marwala, Pallavi Gupta and Thando Tettey","category":"cs.AI","abstract":"  This paper uses Artificial Neural Network (ANN) models to compute response of\nstructural system subject to Indian earthquakes at Chamoli and Uttarkashi\nground motion data. The system is first trained for a single real earthquake\ndata. The trained ANN architecture is then used to simulate earthquakes with\nvarious intensities and it was found that the predicted responses given by ANN\nmodel are accurate for practical purposes. When the ANN is trained by a part of\nthe ground motion data, it can also identify the responses of the structural\nsystem well. In this way the safeness of the structural systems may be\npredicted in case of future earthquakes without waiting for the earthquake to\noccur for the lessons. Time period and the corresponding maximum response of\nthe building for an earthquake has been evaluated, which is again trained to\npredict the maximum response of the building at different time periods. The\ntrained time period versus maximum response ANN model is also tested for real\nearthquake data of other place, which was not used in the training and was\nfound to be in good agreement.\n","text":"Title:Response Prediction of Structural System Subject to Earthquake Motions\n  using Artificial Neural Network\nAbstract:  This paper uses Artificial Neural Network (ANN) models to compute response of\nstructural system subject to Indian earthquakes at Chamoli and Uttarkashi\nground motion data. The system is first trained for a single real earthquake\ndata. The trained ANN architecture is then used to simulate earthquakes with\nvarious intensities and it was found that the predicted responses given by ANN\nmodel are accurate for practical purposes. When the ANN is trained by a part of\nthe ground motion data, it can also identify the responses of the structural\nsystem well. In this way the safeness of the structural systems may be\npredicted in case of future earthquakes without waiting for the earthquake to\noccur for the lessons. Time period and the corresponding maximum response of\nthe building for an earthquake has been evaluated, which is again trained to\npredict the maximum response of the building at different time periods. The\ntrained time period versus maximum response ANN model is also tested for real\nearthquake data of other place, which was not used in the training and was\nfound to be in good agreement.\n","vector":null,"chunk_id":"bed1071920ba7d44d7ed47d7090ca8f5"}
{"title":"Fault Classification using Pseudomodal Energies and Neuro-fuzzy\n  modelling","authors":"Tshilidzi Marwala, Thando Tettey and Snehashish Chakraverty","category":"cs.AI","abstract":"  This paper presents a fault classification method which makes use of a\nTakagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the\nvibration signals of cylindrical shells. The calculation of Pseudomodal\nEnergies, for the purposes of condition monitoring, has previously been found\nto be an accurate method of extracting features from vibration signals. This\ncalculation is therefore used to extract features from vibration signals\nobtained from a diverse population of cylindrical shells. Some of the cylinders\nin the population have faults in different substructures. The pseudomodal\nenergies calculated from the vibration signals are then used as inputs to a\nneuro-fuzzy model. A leave-one-out cross-validation process is used to test the\nperformance of the model. It is found that the neuro-fuzzy model is able to\nclassify faults with an accuracy of 91.62%, which is higher than the previously\nused multilayer perceptron.\n","text":"Title:Fault Classification using Pseudomodal Energies and Neuro-fuzzy\n  modelling\nAbstract:  This paper presents a fault classification method which makes use of a\nTakagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the\nvibration signals of cylindrical shells. The calculation of Pseudomodal\nEnergies, for the purposes of condition monitoring, has previously been found\nto be an accurate method of extracting features from vibration signals. This\ncalculation is therefore used to extract features from vibration signals\nobtained from a diverse population of cylindrical shells. Some of the cylinders\nin the population have faults in different substructures. The pseudomodal\nenergies calculated from the vibration signals are then used as inputs to a\nneuro-fuzzy model. A leave-one-out cross-validation process is used to test the\nperformance of the model. It is found that the neuro-fuzzy model is able to\nclassify faults with an accuracy of 91.62%, which is higher than the previously\nused multilayer perceptron.\n","vector":null,"chunk_id":"3d20d7a54fa40d01072b73882769df4a"}
{"title":"On-Line Condition Monitoring using Computational Intelligence","authors":"C.B. Vilakazi, T. Marwala, P. Mautla and E. Moloto","category":"cs.AI","abstract":"  This paper presents bushing condition monitoring frameworks that use\nmulti-layer perceptrons (MLP), radial basis functions (RBF) and support vector\nmachines (SVM) classifiers. The first level of the framework determines if the\nbushing is faulty or not while the second level determines the type of fault.\nThe diagnostic gases in the bushings are analyzed using the dissolve gas\nanalysis. MLP gives superior performance in terms of accuracy and training time\nthan SVM and RBF. In addition, an on-line bushing condition monitoring\napproach, which is able to adapt to newly acquired data are introduced. This\napproach is able to accommodate new classes that are introduced by incoming\ndata and is implemented using an incremental learning algorithm that uses MLP.\nThe testing results improved from 67.5% to 95.8% as new data were introduced\nand the testing results improved from 60% to 95.3% as new conditions were\nintroduced. On average the confidence value of the framework on its decision\nwas 0.92.\n","text":"Title:On-Line Condition Monitoring using Computational Intelligence\nAbstract:  This paper presents bushing condition monitoring frameworks that use\nmulti-layer perceptrons (MLP), radial basis functions (RBF) and support vector\nmachines (SVM) classifiers. The first level of the framework determines if the\nbushing is faulty or not while the second level determines the type of fault.\nThe diagnostic gases in the bushings are analyzed using the dissolve gas\nanalysis. MLP gives superior performance in terms of accuracy and training time\nthan SVM and RBF. In addition, an on-line bushing condition monitoring\napproach, which is able to adapt to newly acquired data are introduced. This\napproach is able to accommodate new classes that are introduced by incoming\ndata and is implemented using an incremental learning algorithm that uses MLP.\nThe testing results improved from 67.5% to 95.8% as new data were introduced\nand the testing results improved from 60% to 95.3% as new conditions were\nintroduced. On average the confidence value of the framework on its decision\nwas 0.92.\n","vector":null,"chunk_id":"43193580459970563e2e4b8f061595a4"}
{"title":"Lasso type classifiers with a reject option","authors":"Marten Wegkamp","category":"stat.ML","abstract":"  We consider the problem of binary classification where one can, for a\nparticular cost, choose not to classify an observation. We present a simple\nproof for the oracle inequality for the excess risk of structural risk\nminimizers using a lasso type penalty.\n","text":"Title:Lasso type classifiers with a reject option\nAbstract:  We consider the problem of binary classification where one can, for a\nparticular cost, choose not to classify an observation. We present a simple\nproof for the oracle inequality for the excess risk of structural risk\nminimizers using a lasso type penalty.\n","vector":null,"chunk_id":"bf00998bd67deb85f560fc78e6d7bec0"}
{"title":"The Road to Quantum Artificial Intelligence","authors":"Kyriakos N. Sgarbas","category":"cs.AI","abstract":"  This paper overviews the basic principles and recent advances in the emerging\nfield of Quantum Computation (QC), highlighting its potential application to\nArtificial Intelligence (AI). The paper provides a very brief introduction to\nbasic QC issues like quantum registers, quantum gates and quantum algorithms\nand then it presents references, ideas and research guidelines on how QC can be\nused to deal with some basic AI problems, such as search and pattern matching,\nas soon as quantum computers become widely available.\n","text":"Title:The Road to Quantum Artificial Intelligence\nAbstract:  This paper overviews the basic principles and recent advances in the emerging\nfield of Quantum Computation (QC), highlighting its potential application to\nArtificial Intelligence (AI). The paper provides a very brief introduction to\nbasic QC issues like quantum registers, quantum gates and quantum algorithms\nand then it presents references, ideas and research guidelines on how QC can be\nused to deal with some basic AI problems, such as search and pattern matching,\nas soon as quantum computers become widely available.\n","vector":null,"chunk_id":"3f1466365bd090bfafcbfc64cfffb144"}
{"title":"Truecluster matching","authors":"Jens Oehlschl\\\"agel","category":"cs.AI","abstract":"  Cluster matching by permuting cluster labels is important in many clustering\ncontexts such as cluster validation and cluster ensemble techniques. The\nclassic approach is to minimize the euclidean distance between two cluster\nsolutions which induces inappropriate stability in certain settings. Therefore,\nwe present the truematch algorithm that introduces two improvements best\nexplained in the crisp case. First, instead of maximizing the trace of the\ncluster crosstable, we propose to maximize a chi-square transformation of this\ncrosstable. Thus, the trace will not be dominated by the cells with the largest\ncounts but by the cells with the most non-random observations, taking into\naccount the marginals. Second, we suggest a probabilistic component in order to\nbreak ties and to make the matching algorithm truly random on random data. The\ntruematch algorithm is designed as a building block of the truecluster\nframework and scales in polynomial time. First simulation results confirm that\nthe truematch algorithm gives more consistent truecluster results for unequal\ncluster sizes. Free R software is available.\n","text":"Title:Truecluster matching\nAbstract:  Cluster matching by permuting cluster labels is important in many clustering\ncontexts such as cluster validation and cluster ensemble techniques. The\nclassic approach is to minimize the euclidean distance between two cluster\nsolutions which induces inappropriate stability in certain settings. Therefore,\nwe present the truematch algorithm that introduces two improvements best\nexplained in the crisp case. First, instead of maximizing the trace of the\ncluster crosstable, we propose to maximize a chi-square transformation of this\ncrosstable. Thus, the trace will not be dominated by the cells with the largest\ncounts but by the cells with the most non-random observations, taking into\naccount the marginals. Second, we suggest a probabilistic component in order to\nbreak ties and to make the matching algorithm truly random on random data. The\ntruematch algorithm is designed as a building block of the truecluster\nframework and scales in polynomial time. First simulation results confirm that\nthe truematch algorithm gives more consistent truecluster results for unequal\ncluster sizes. Free R software is available.\n","vector":null,"chunk_id":"4d3dd7755f62092a2fad89f931383f37"}
{"title":"Modeling Computations in a Semantic Network","authors":"Marko A. Rodriguez and Johan Bollen","category":"cs.AI","abstract":"  Semantic network research has seen a resurgence from its early history in the\ncognitive sciences with the inception of the Semantic Web initiative. The\nSemantic Web effort has brought forth an array of technologies that support the\nencoding, storage, and querying of the semantic network data structure at the\nworld stage. Currently, the popular conception of the Semantic Web is that of a\ndata modeling medium where real and conceptual entities are related in\nsemantically meaningful ways. However, new models have emerged that explicitly\nencode procedural information within the semantic network substrate. With these\nnew technologies, the Semantic Web has evolved from a data modeling medium to a\ncomputational medium. This article provides a classification of existing\ncomputational modeling efforts and the requirements of supporting technologies\nthat will aid in the further growth of this burgeoning domain.\n","text":"Title:Modeling Computations in a Semantic Network\nAbstract:  Semantic network research has seen a resurgence from its early history in the\ncognitive sciences with the inception of the Semantic Web initiative. The\nSemantic Web effort has brought forth an array of technologies that support the\nencoding, storage, and querying of the semantic network data structure at the\nworld stage. Currently, the popular conception of the Semantic Web is that of a\ndata modeling medium where real and conceptual entities are related in\nsemantically meaningful ways. However, new models have emerged that explicitly\nencode procedural information within the semantic network substrate. With these\nnew technologies, the Semantic Web has evolved from a data modeling medium to a\ncomputational medium. This article provides a classification of existing\ncomputational modeling efforts and the requirements of supporting technologies\nthat will aid in the further growth of this burgeoning domain.\n","vector":null,"chunk_id":"2afbed61c70d083976c0c5b07aad5fa9"}
{"title":"Automatically Restructuring Practice Guidelines using the GEM DTD","authors":"Amanda Bouffier (LIPN), Thierry Poibeau (LIPN)","category":"cs.AI","abstract":"  This paper describes a system capable of semi-automatically filling an XML\ntemplate from free texts in the clinical domain (practice guidelines). The XML\ntemplate includes semantic information not explicitly encoded in the text\n(pairs of conditions and actions/recommendations). Therefore, there is a need\nto compute the exact scope of conditions over text sequences expressing the\nrequired actions. We present a system developed for this task. We show that it\nyields good performance when applied to the analysis of French practice\nguidelines.\n","text":"Title:Automatically Restructuring Practice Guidelines using the GEM DTD\nAbstract:  This paper describes a system capable of semi-automatically filling an XML\ntemplate from free texts in the clinical domain (practice guidelines). The XML\ntemplate includes semantic information not explicitly encoded in the text\n(pairs of conditions and actions/recommendations). Therefore, there is a need\nto compute the exact scope of conditions over text sequences expressing the\nrequired actions. We present a system developed for this task. We show that it\nyields good performance when applied to the analysis of French practice\nguidelines.\n","vector":null,"chunk_id":"23035e48ab52075b40a32c78f89eb2e0"}
{"title":"Temporal Reasoning without Transitive Tables","authors":"Sylviane R. Schwer (LIPN)","category":"cs.AI","abstract":"  Representing and reasoning about qualitative temporal information is an\nessential part of many artificial intelligence tasks. Lots of models have been\nproposed in the litterature for representing such temporal information. All\nderive from a point-based or an interval-based framework. One fundamental\nreasoning task that arises in applications of these frameworks is given by the\nfollowing scheme: given possibly indefinite and incomplete knowledge of the\nbinary relationships between some temporal objects, find the consistent\nscenarii between all these objects. All these models require transitive tables\n-- or similarly inference rules-- for solving such tasks. We have defined an\nalternative model, S-languages - to represent qualitative temporal information,\nbased on the only two relations of \\emph{precedence} and \\emph{simultaneity}.\nIn this paper, we show how this model enables to avoid transitive tables or\ninference rules to handle this kind of problem.\n","text":"Title:Temporal Reasoning without Transitive Tables\nAbstract:  Representing and reasoning about qualitative temporal information is an\nessential part of many artificial intelligence tasks. Lots of models have been\nproposed in the litterature for representing such temporal information. All\nderive from a point-based or an interval-based framework. One fundamental\nreasoning task that arises in applications of these frameworks is given by the\nfollowing scheme: given possibly indefinite and incomplete knowledge of the\nbinary relationships between some temporal objects, find the consistent\nscenarii between all these objects. All these models require transitive tables\n-- or similarly inference rules-- for solving such tasks. We have defined an\nalternative model, S-languages - to represent qualitative temporal information,\nbased on the only two relations of \\emph{precedence} and \\emph{simultaneity}.\nIn this paper, we show how this model enables to avoid transitive tables or\ninference rules to handle this kind of problem.\n","vector":null,"chunk_id":"34117f80153097f391c0ab7df3b8b7d7"}
{"title":"Extraction d'entit\\'es dans des collections \\'evolutives","authors":"Thierry Despeyroux (INRIA Rocquencourt / INRIA Sophia Antipolis),\n  Eduardo Fraschini (INRIA Rocquencourt / INRIA Sophia Antipolis), Anne-Marie\n  Vercoustre (INRIA Rocquencourt / INRIA Sophia Antipolis)","category":"cs.IR","abstract":"  The goal of our work is to use a set of reports and extract named entities,\nin our case the names of Industrial or Academic partners. Starting with an\ninitial list of entities, we use a first set of documents to identify syntactic\npatterns that are then validated in a supervised learning phase on a set of\nannotated documents. The complete collection is then explored. This approach is\nsimilar to the ones used in data extraction from semi-structured documents\n(wrappers) and do not need any linguistic resources neither a large set for\ntraining. As our collection of documents would evolve over years, we hope that\nthe performance of the extraction would improve with the increased size of the\ntraining set.\n","text":"Title:Extraction d'entit\\'es dans des collections \\'evolutives\nAbstract:  The goal of our work is to use a set of reports and extract named entities,\nin our case the names of Industrial or Academic partners. Starting with an\ninitial list of entities, we use a first set of documents to identify syntactic\npatterns that are then validated in a supervised learning phase on a set of\nannotated documents. The complete collection is then explored. This approach is\nsimilar to the ones used in data extraction from semi-structured documents\n(wrappers) and do not need any linguistic resources neither a large set for\ntraining. As our collection of documents would evolve over years, we hope that\nthe performance of the extraction would improve with the increased size of the\ntraining set.\n","vector":null,"chunk_id":"7452dcfc36551416d724ce22ce2c536c"}
{"title":"Metric Embedding for Nearest Neighbor Classification","authors":"Bharath K. Sriperumbudur and Gert R. G. Lanckriet","category":"stat.ML","abstract":"  The distance metric plays an important role in nearest neighbor (NN)\nclassification. Usually the Euclidean distance metric is assumed or a\nMahalanobis distance metric is optimized to improve the NN performance. In this\npaper, we study the problem of embedding arbitrary metric spaces into a\nEuclidean space with the goal to improve the accuracy of the NN classifier. We\npropose a solution by appealing to the framework of regularization in a\nreproducing kernel Hilbert space and prove a representer-like theorem for NN\nclassification. The embedding function is then determined by solving a\nsemidefinite program which has an interesting connection to the soft-margin\nlinear binary support vector machine classifier. Although the main focus of\nthis paper is to present a general, theoretical framework for metric embedding\nin a NN setting, we demonstrate the performance of the proposed method on some\nbenchmark datasets and show that it performs better than the Mahalanobis metric\nlearning algorithm in terms of leave-one-out and generalization errors.\n","text":"Title:Metric Embedding for Nearest Neighbor Classification\nAbstract:  The distance metric plays an important role in nearest neighbor (NN)\nclassification. Usually the Euclidean distance metric is assumed or a\nMahalanobis distance metric is optimized to improve the NN performance. In this\npaper, we study the problem of embedding arbitrary metric spaces into a\nEuclidean space with the goal to improve the accuracy of the NN classifier. We\npropose a solution by appealing to the framework of regularization in a\nreproducing kernel Hilbert space and prove a representer-like theorem for NN\nclassification. The embedding function is then determined by solving a\nsemidefinite program which has an interesting connection to the soft-margin\nlinear binary support vector machine classifier. Although the main focus of\nthis paper is to present a general, theoretical framework for metric embedding\nin a NN setting, we demonstrate the performance of the proposed method on some\nbenchmark datasets and show that it performs better than the Mahalanobis metric\nlearning algorithm in terms of leave-one-out and generalization errors.\n","vector":null,"chunk_id":"570f84a285af4f73ad455c6c4d92cf08"}
{"title":"A Collection of Definitions of Intelligence","authors":"Shane Legg and Marcus Hutter","category":"cs.AI","abstract":"  This paper is a survey of a large number of informal definitions of\n``intelligence'' that the authors have collected over the years. Naturally,\ncompiling a complete list would be impossible as many definitions of\nintelligence are buried deep inside articles and books. Nevertheless, the\n70-odd definitions presented here are, to the authors' knowledge, the largest\nand most well referenced collection there is.\n","text":"Title:A Collection of Definitions of Intelligence\nAbstract:  This paper is a survey of a large number of informal definitions of\n``intelligence'' that the authors have collected over the years. Naturally,\ncompiling a complete list would be impossible as many definitions of\nintelligence are buried deep inside articles and books. Nevertheless, the\n70-odd definitions presented here are, to the authors' knowledge, the largest\nand most well referenced collection there is.\n","vector":null,"chunk_id":"ca200a22155b86fbfe2473b87e65b38d"}
{"title":"A Robust Linguistic Platform for Efficient and Domain specific Web\n  Content Analysis","authors":"Thierry Hamon (LIPN), Adeline Nazarenko (LIPN), Thierry Poibeau\n  (LIPN), Sophie Aubin (LIPN), Julien Derivi\\`ere (LIPN)","category":"cs.AI","abstract":"  Web semantic access in specific domains calls for specialized search engines\nwith enhanced semantic querying and indexing capacities, which pertain both to\ninformation retrieval (IR) and to information extraction (IE). A rich\nlinguistic analysis is required either to identify the relevant semantic units\nto index and weight them according to linguistic specific statistical\ndistribution, or as the basis of an information extraction process. Recent\ndevelopments make Natural Language Processing (NLP) techniques reliable enough\nto process large collections of documents and to enrich them with semantic\nannotations. This paper focuses on the design and the development of a text\nprocessing platform, Ogmios, which has been developed in the ALVIS project. The\nOgmios platform exploits existing NLP modules and resources, which may be tuned\nto specific domains and produces linguistically annotated documents. We show\nhow the three constraints of genericity, domain semantic awareness and\nperformance can be handled all together.\n","text":"Title:A Robust Linguistic Platform for Efficient and Domain specific Web\n  Content Analysis\nAbstract:  Web semantic access in specific domains calls for specialized search engines\nwith enhanced semantic querying and indexing capacities, which pertain both to\ninformation retrieval (IR) and to information extraction (IE). A rich\nlinguistic analysis is required either to identify the relevant semantic units\nto index and weight them according to linguistic specific statistical\ndistribution, or as the basis of an information extraction process. Recent\ndevelopments make Natural Language Processing (NLP) techniques reliable enough\nto process large collections of documents and to enrich them with semantic\nannotations. This paper focuses on the design and the development of a text\nprocessing platform, Ogmios, which has been developed in the ALVIS project. The\nOgmios platform exploits existing NLP modules and resources, which may be tuned\nto specific domains and produces linguistically annotated documents. We show\nhow the three constraints of genericity, domain semantic awareness and\nperformance can be handled all together.\n","vector":null,"chunk_id":"addee692a552e2307cdc610f9cae02ca"}
{"title":"Mixed Integer Linear Programming For Exact Finite-Horizon Planning In\n  Decentralized Pomdps","authors":"Raghav Aras (INRIA Lorraine - LORIA), Alain Dutech (INRIA Lorraine -\n  LORIA), Fran\\c{c}ois Charpillet (INRIA Lorraine - LORIA)","category":"cs.AI","abstract":"  We consider the problem of finding an n-agent joint-policy for the optimal\nfinite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem\nof very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new\nmathematical programming approach for the problem. Our approach is based on two\nideas: First, we represent each agent's policy in the sequence-form and not in\nthe tree-form, thereby obtaining a very compact representation of the set of\njoint-policies. Second, using this compact representation, we solve this\nproblem as an instance of combinatorial optimization for which we formulate a\nmixed integer linear program (MILP). The optimal solution of the MILP directly\nyields an optimal joint-policy for the Dec-Pomdp. Computational experience\nshows that formulating and solving the MILP requires significantly less time to\nsolve benchmark Dec-Pomdp problems than existing algorithms. For example, the\nmulti-agent tiger problem for horizon 4 is solved in 72 secs with the MILP\nwhereas existing algorithms require several hours to solve it.\n","text":"Title:Mixed Integer Linear Programming For Exact Finite-Horizon Planning In\n  Decentralized Pomdps\nAbstract:  We consider the problem of finding an n-agent joint-policy for the optimal\nfinite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem\nof very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new\nmathematical programming approach for the problem. Our approach is based on two\nideas: First, we represent each agent's policy in the sequence-form and not in\nthe tree-form, thereby obtaining a very compact representation of the set of\njoint-policies. Second, using this compact representation, we solve this\nproblem as an instance of combinatorial optimization for which we formulate a\nmixed integer linear program (MILP). The optimal solution of the MILP directly\nyields an optimal joint-policy for the Dec-Pomdp. Computational experience\nshows that formulating and solving the MILP requires significantly less time to\nsolve benchmark Dec-Pomdp problems than existing algorithms. For example, the\nmulti-agent tiger problem for horizon 4 is solved in 72 secs with the MILP\nwhereas existing algorithms require several hours to solve it.\n","vector":null,"chunk_id":"1a89c66800d3bcd1054a5d2fbcafd664"}
{"title":"Degenerating families of dendrograms","authors":"Patrick Erik Bradley","category":"stat.ML","abstract":"  Dendrograms used in data analysis are ultrametric spaces, hence objects of\nnonarchimedean geometry. It is known that there exist $p$-adic representation\nof dendrograms. Completed by a point at infinity, they can be viewed as\nsubtrees of the Bruhat-Tits tree associated to the $p$-adic projective line.\nThe implications are that certain moduli spaces known in algebraic geometry are\n$p$-adic parameter spaces of (families of) dendrograms, and stochastic\nclassification can also be handled within this framework. At the end, we\ncalculate the topology of the hidden part of a dendrogram.\n","text":"Title:Degenerating families of dendrograms\nAbstract:  Dendrograms used in data analysis are ultrametric spaces, hence objects of\nnonarchimedean geometry. It is known that there exist $p$-adic representation\nof dendrograms. Completed by a point at infinity, they can be viewed as\nsubtrees of the Bruhat-Tits tree associated to the $p$-adic projective line.\nThe implications are that certain moduli spaces known in algebraic geometry are\n$p$-adic parameter spaces of (families of) dendrograms, and stochastic\nclassification can also be handled within this framework. At the end, we\ncalculate the topology of the hidden part of a dendrogram.\n","vector":null,"chunk_id":"35b71ffd49faf04c61a3ea2f31fd91f7"}
{"title":"Families of dendrograms","authors":"Patrick Erik Bradley","category":"stat.ML","abstract":"  A conceptual framework for cluster analysis from the viewpoint of p-adic\ngeometry is introduced by describing the space of all dendrograms for n\ndatapoints and relating it to the moduli space of p-adic Riemannian spheres\nwith punctures using a method recently applied by Murtagh (2004b). This method\nembeds a dendrogram as a subtree into the Bruhat-Tits tree associated to the\np-adic numbers, and goes back to Cornelissen et al. (2001) in p-adic geometry.\nAfter explaining the definitions, the concept of classifiers is discussed in\nthe context of moduli spaces, and upper bounds for the number of hidden\nvertices in dendrograms are given.\n","text":"Title:Families of dendrograms\nAbstract:  A conceptual framework for cluster analysis from the viewpoint of p-adic\ngeometry is introduced by describing the space of all dendrograms for n\ndatapoints and relating it to the moduli space of p-adic Riemannian spheres\nwith punctures using a method recently applied by Murtagh (2004b). This method\nembeds a dendrogram as a subtree into the Bruhat-Tits tree associated to the\np-adic numbers, and goes back to Cornelissen et al. (2001) in p-adic geometry.\nAfter explaining the definitions, the concept of classifiers is discussed in\nthe context of moduli spaces, and upper bounds for the number of hidden\nvertices in dendrograms are given.\n","vector":null,"chunk_id":"4b2504953407a93d38bd3434dc687514"}
{"title":"A Leaf Recognition Algorithm for Plant Classification Using\n  Probabilistic Neural Network","authors":"Stephen Gang Wu, Forrest Sheng Bao, Eric You Xu, Yu-Xuan Wang, Yi-Fan\n  Chang and Qiao-Liang Xiang","category":"cs.AI","abstract":"  In this paper, we employ Probabilistic Neural Network (PNN) with image and\ndata processing techniques to implement a general purpose automated leaf\nrecognition algorithm. 12 leaf features are extracted and orthogonalized into 5\nprincipal variables which consist the input vector of the PNN. The PNN is\ntrained by 1800 leaves to classify 32 kinds of plants with an accuracy greater\nthan 90%. Compared with other approaches, our algorithm is an accurate\nartificial intelligence approach which is fast in execution and easy in\nimplementation.\n","text":"Title:A Leaf Recognition Algorithm for Plant Classification Using\n  Probabilistic Neural Network\nAbstract:  In this paper, we employ Probabilistic Neural Network (PNN) with image and\ndata processing techniques to implement a general purpose automated leaf\nrecognition algorithm. 12 leaf features are extracted and orthogonalized into 5\nprincipal variables which consist the input vector of the PNN. The PNN is\ntrained by 1800 leaves to classify 32 kinds of plants with an accuracy greater\nthan 90%. Compared with other approaches, our algorithm is an accurate\nartificial intelligence approach which is fast in execution and easy in\nimplementation.\n","vector":null,"chunk_id":"17e2ad1e3ee543844d83a7f19a559fc9"}
{"title":"Characterising Web Site Link Structure","authors":"Shi Zhou, Ingemar Cox and Vaclav Petricek","category":"cs.IR","abstract":"  The topological structures of the Internet and the Web have received\nconsiderable attention. However, there has been little research on the\ntopological properties of individual web sites. In this paper, we consider\nwhether web sites (as opposed to the entire Web) exhibit structural\nsimilarities. To do so, we exhaustively crawled 18 web sites as diverse as\ngovernmental departments, commercial companies and university departments in\ndifferent countries. These web sites consisted of as little as a few thousand\npages to millions of pages. Statistical analysis of these 18 sites revealed\nthat the internal link structure of the web sites are significantly different\nwhen measured with first and second-order topological properties, i.e.\nproperties based on the connectivity of an individual or a pairs of nodes.\nHowever, examination of a third-order topological property that consider the\nconnectivity between three nodes that form a triangle, revealed a strong\ncorrespondence across web sites, suggestive of an invariant. Comparison with\nthe Web, the AS Internet, and a citation network, showed that this third-order\nproperty is not shared across other types of networks. Nor is the property\nexhibited in generative network models such as that of Barabasi and Albert.\n","text":"Title:Characterising Web Site Link Structure\nAbstract:  The topological structures of the Internet and the Web have received\nconsiderable attention. However, there has been little research on the\ntopological properties of individual web sites. In this paper, we consider\nwhether web sites (as opposed to the entire Web) exhibit structural\nsimilarities. To do so, we exhaustively crawled 18 web sites as diverse as\ngovernmental departments, commercial companies and university departments in\ndifferent countries. These web sites consisted of as little as a few thousand\npages to millions of pages. Statistical analysis of these 18 sites revealed\nthat the internal link structure of the web sites are significantly different\nwhen measured with first and second-order topological properties, i.e.\nproperties based on the connectivity of an individual or a pairs of nodes.\nHowever, examination of a third-order topological property that consider the\nconnectivity between three nodes that form a triangle, revealed a strong\ncorrespondence across web sites, suggestive of an invariant. Comparison with\nthe Web, the AS Internet, and a citation network, showed that this third-order\nproperty is not shared across other types of networks. Nor is the property\nexhibited in generative network models such as that of Barabasi and Albert.\n","vector":null,"chunk_id":"741089f581d6859208313de5f5e24697"}
{"title":"Online Learning in Discrete Hidden Markov Models","authors":"Roberto C. Alamino, Nestor Caticha","category":"stat.ML","abstract":"  We present and analyse three online algorithms for learning in discrete\nHidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm.\nUsing the Kullback-Leibler divergence as a measure of generalisation error we\ndraw learning curves in simplified situations. The performance for learning\ndrifting concepts of one of the presented algorithms is analysed and compared\nwith the Baldi-Chauvin algorithm in the same situations. A brief discussion\nabout learning and symmetry breaking based on our results is also presented.\n","text":"Title:Online Learning in Discrete Hidden Markov Models\nAbstract:  We present and analyse three online algorithms for learning in discrete\nHidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm.\nUsing the Kullback-Leibler divergence as a measure of generalisation error we\ndraw learning curves in simplified situations. The performance for learning\ndrifting concepts of one of the presented algorithms is analysed and compared\nwith the Baldi-Chauvin algorithm in the same situations. A brief discussion\nabout learning and symmetry breaking based on our results is also presented.\n","vector":null,"chunk_id":"def145ace3c1b2ad10e78bb149950082"}
{"title":"Integrating users' needs into multimedia information retrieval system","authors":"Han\\`ene Maghrebi (LORIA), Amos David (LORIA)","category":"cs.IR","abstract":"  The exponential growth of multimedia information and the development of\nvarious communication media generated new problems at various levels including\nthe rate of flow of information, problems of storage and management. The\ndifficulty which arises is no longer the existence of information but rather\nthe access to this information. When designing multimedia information retrieval\nsystem, it is appropriate to bear in mind the potential users and their\ninformation needs. We assumed that multimedia information representation which\ntakes into account explicitly the users' needs and the cases of use could\ncontribute to the adaptation potentials of the system for the end-users. We\nbelieve also that responses of multimedia information system would be more\nrelevant to the users' needs if the types of results to be used from the system\nwere identified before the design and development of the system. We propose the\nintegration of the users' information needs. More precisely integrating usage\ncontexts of resulting information in an information system (during creation and\nfeedback) should enhance more pertinent users' need. The first section of this\nstudy is dedicated to traditional multimedia information systems and\nspecifically the approaches of representing multimedia information. Taking into\naccount the dynamism of users, these approaches do not permit the explicit\nintegration of the users' information needs. In this paper, we will present our\nproposals based on economic intelligence approach. This approach emphasizes the\nimportance of starting any process of information retrieval witch the user\ninformation need.\n","text":"Title:Integrating users' needs into multimedia information retrieval system\nAbstract:  The exponential growth of multimedia information and the development of\nvarious communication media generated new problems at various levels including\nthe rate of flow of information, problems of storage and management. The\ndifficulty which arises is no longer the existence of information but rather\nthe access to this information. When designing multimedia information retrieval\nsystem, it is appropriate to bear in mind the potential users and their\ninformation needs. We assumed that multimedia information representation which\ntakes into account explicitly the users' needs and the cases of use could\ncontribute to the adaptation potentials of the system for the end-users. We\nbelieve also that responses of multimedia information system would be more\nrelevant to the users' needs if the types of results to be used from the system\nwere identified before the design and development of the system. We propose the\nintegration of the users' information needs. More precisely integrating usage\ncontexts of resulting information in an information system (during creation and\nfeedback) should enhance more pertinent users' need. The first section of this\nstudy is dedicated to traditional multimedia information systems and\nspecifically the approaches of representing multimedia information. Taking into\naccount the dynamism of users, these approaches do not permit the explicit\nintegration of the users' information needs. In this paper, we will present our\nproposals based on economic intelligence approach. This approach emphasizes the\nimportance of starting any process of information retrieval witch the user\ninformation need.\n","vector":null,"chunk_id":"7e44d40e26795b30666968edfbfa452b"}
{"title":"2006: Celebrating 75 years of AI - History and Outlook: the Next 25\n  Years","authors":"Juergen Schmidhuber","category":"cs.AI","abstract":"  When Kurt Goedel layed the foundations of theoretical computer science in\n1931, he also introduced essential concepts of the theory of Artificial\nIntelligence (AI). Although much of subsequent AI research has focused on\nheuristics, which still play a major role in many practical AI applications, in\nthe new millennium AI theory has finally become a full-fledged formal science,\nwith important optimality results for embodied agents living in unknown\nenvironments, obtained through a combination of theory a la Goedel and\nprobability theory. Here we look back at important milestones of AI history,\nmention essential recent results, and speculate about what we may expect from\nthe next 25 years, emphasizing the significance of the ongoing dramatic\nhardware speedups, and discussing Goedel-inspired, self-referential,\nself-improving universal problem solvers.\n","text":"Title:2006: Celebrating 75 years of AI - History and Outlook: the Next 25\n  Years\nAbstract:  When Kurt Goedel layed the foundations of theoretical computer science in\n1931, he also introduced essential concepts of the theory of Artificial\nIntelligence (AI). Although much of subsequent AI research has focused on\nheuristics, which still play a major role in many practical AI applications, in\nthe new millennium AI theory has finally become a full-fledged formal science,\nwith important optimality results for embodied agents living in unknown\nenvironments, obtained through a combination of theory a la Goedel and\nprobability theory. Here we look back at important milestones of AI history,\nmention essential recent results, and speculate about what we may expect from\nthe next 25 years, emphasizing the significance of the ongoing dramatic\nhardware speedups, and discussing Goedel-inspired, self-referential,\nself-improving universal problem solvers.\n","vector":null,"chunk_id":"6d5df6ecad3ba3eb4dcd0aacf087da57"}
{"title":"Qualitative Belief Conditioning Rules (QBCR)","authors":"Florentin Smarandache, Jean Dezert","category":"cs.AI","abstract":"  In this paper we extend the new family of (quantitative) Belief Conditioning\nRules (BCR) recently developed in the Dezert-Smarandache Theory (DSmT) to their\nqualitative counterpart for belief revision. Since the revision of quantitative\nas well as qualitative belief assignment given the occurrence of a new event\n(the conditioning constraint) can be done in many possible ways, we present\nhere only what we consider as the most appealing Qualitative Belief\nConditioning Rules (QBCR) which allow to revise the belief directly with words\nand linguistic labels and thus avoids the introduction of ad-hoc translations\nof quantitative beliefs into quantitative ones for solving the problem.\n","text":"Title:Qualitative Belief Conditioning Rules (QBCR)\nAbstract:  In this paper we extend the new family of (quantitative) Belief Conditioning\nRules (BCR) recently developed in the Dezert-Smarandache Theory (DSmT) to their\nqualitative counterpart for belief revision. Since the revision of quantitative\nas well as qualitative belief assignment given the occurrence of a new event\n(the conditioning constraint) can be done in many possible ways, we present\nhere only what we consider as the most appealing Qualitative Belief\nConditioning Rules (QBCR) which allow to revise the belief directly with words\nand linguistic labels and thus avoids the introduction of ad-hoc translations\nof quantitative beliefs into quantitative ones for solving the problem.\n","vector":null,"chunk_id":"a322062cd28bdba5859487194e2c477e"}
{"title":"Using RDF to Model the Structure and Process of Systems","authors":"Marko A. Rodriguez, Jennifer H. Watkins, Johan Bollen, Carlos\n  Gershenson","category":"cs.AI","abstract":"  Many systems can be described in terms of networks of discrete elements and\ntheir various relationships to one another. A semantic network, or\nmulti-relational network, is a directed labeled graph consisting of a\nheterogeneous set of entities connected by a heterogeneous set of\nrelationships. Semantic networks serve as a promising general-purpose modeling\nsubstrate for complex systems. Various standardized formats and tools are now\navailable to support practical, large-scale semantic network models. First, the\nResource Description Framework (RDF) offers a standardized semantic network\ndata model that can be further formalized by ontology modeling languages such\nas RDF Schema (RDFS) and the Web Ontology Language (OWL). Second, the recent\nintroduction of highly performant triple-stores (i.e. semantic network\ndatabases) allows semantic network models on the order of $10^9$ edges to be\nefficiently stored and manipulated. RDF and its related technologies are\ncurrently used extensively in the domains of computer science, digital library\nscience, and the biological sciences. This article will provide an introduction\nto RDF/RDFS/OWL and an examination of its suitability to model discrete element\ncomplex systems.\n","text":"Title:Using RDF to Model the Structure and Process of Systems\nAbstract:  Many systems can be described in terms of networks of discrete elements and\ntheir various relationships to one another. A semantic network, or\nmulti-relational network, is a directed labeled graph consisting of a\nheterogeneous set of entities connected by a heterogeneous set of\nrelationships. Semantic networks serve as a promising general-purpose modeling\nsubstrate for complex systems. Various standardized formats and tools are now\navailable to support practical, large-scale semantic network models. First, the\nResource Description Framework (RDF) offers a standardized semantic network\ndata model that can be further formalized by ontology modeling languages such\nas RDF Schema (RDFS) and the Web Ontology Language (OWL). Second, the recent\nintroduction of highly performant triple-stores (i.e. semantic network\ndatabases) allows semantic network models on the order of $10^9$ edges to be\nefficiently stored and manipulated. RDF and its related technologies are\ncurrently used extensively in the domains of computer science, digital library\nscience, and the biological sciences. This article will provide an introduction\nto RDF/RDFS/OWL and an examination of its suitability to model discrete element\ncomplex systems.\n","vector":null,"chunk_id":"82f4a2fa250f44562ad1c64013a1662b"}
{"title":"Enrichment of Qualitative Beliefs for Reasoning under Uncertainty","authors":"Xinde Li (ICRL), Xinhan Huang (ICRL), Florentin Smarandache (UNM),\n  Jean Dezert (ONERA)","category":"cs.AI","abstract":"  This paper deals with enriched qualitative belief functions for reasoning\nunder uncertainty and for combining information expressed in natural language\nthrough linguistic labels. In this work, two possible enrichments (quantitative\nand/or qualitative) of linguistic labels are considered and operators\n(addition, multiplication, division, etc) for dealing with them are proposed\nand explained. We denote them $qe$-operators, $qe$ standing for\n\"qualitative-enriched\" operators. These operators can be seen as a direct\nextension of the classical qualitative operators ($q$-operators) proposed\nrecently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning\n(DSmT). $q$-operators are also justified in details in this paper. The\nquantitative enrichment of linguistic label is a numerical supporting degree in\n$[0,\\infty)$, while the qualitative enrichment takes its values in a finite\nordered set of linguistic values. Quantitative enrichment is less precise than\nqualitative enrichment, but it is expected more close with what human experts\ncan easily provide when expressing linguistic labels with supporting degrees.\nTwo simple examples are given to show how the fusion of qualitative-enriched\nbelief assignments can be done.\n","text":"Title:Enrichment of Qualitative Beliefs for Reasoning under Uncertainty\nAbstract:  This paper deals with enriched qualitative belief functions for reasoning\nunder uncertainty and for combining information expressed in natural language\nthrough linguistic labels. In this work, two possible enrichments (quantitative\nand/or qualitative) of linguistic labels are considered and operators\n(addition, multiplication, division, etc) for dealing with them are proposed\nand explained. We denote them $qe$-operators, $qe$ standing for\n\"qualitative-enriched\" operators. These operators can be seen as a direct\nextension of the classical qualitative operators ($q$-operators) proposed\nrecently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning\n(DSmT). $q$-operators are also justified in details in this paper. The\nquantitative enrichment of linguistic label is a numerical supporting degree in\n$[0,\\infty)$, while the qualitative enrichment takes its values in a finite\nordered set of linguistic values. Quantitative enrichment is less precise than\nqualitative enrichment, but it is expected more close with what human experts\ncan easily provide when expressing linguistic labels with supporting degrees.\nTwo simple examples are given to show how the fusion of qualitative-enriched\nbelief assignments can be done.\n","vector":null,"chunk_id":"03f65f3522fc71f7088598a09cfe8396"}
{"title":"Toward Psycho-robots","authors":"Andrei Khrennikov","category":"cs.AI","abstract":"  We try to perform geometrization of psychology by representing mental states,\n<<ideas>>, by points of a metric space, <<mental space>>. Evolution of ideas is\ndescribed by dynamical systems in metric mental space. We apply the mental\nspace approach for modeling of flows of unconscious and conscious information\nin the human brain. In a series of models, Models 1-4, we consider cognitive\nsystems with increasing complexity of psychological behavior determined by\nstructure of flows of ideas. Since our models are in fact models of the\nAI-type, one immediately recognizes that they can be used for creation of\nAI-systems, which we call psycho-robots, exhibiting important elements of human\npsyche. Creation of such psycho-robots may be useful improvement of domestic\nrobots. At the moment domestic robots are merely simple working devices (e.g.\nvacuum cleaners or lawn mowers) . However, in future one can expect demand in\nsystems which be able not only perform simple work tasks, but would have\nelements of human self-developing psyche. Such AI-psyche could play an\nimportant role both in relations between psycho-robots and their owners as well\nas between psycho-robots. Since the presence of a huge numbers of\npsycho-complexes is an essential characteristic of human psychology, it would\nbe interesting to model them in the AI-framework.\n","text":"Title:Toward Psycho-robots\nAbstract:  We try to perform geometrization of psychology by representing mental states,\n<<ideas>>, by points of a metric space, <<mental space>>. Evolution of ideas is\ndescribed by dynamical systems in metric mental space. We apply the mental\nspace approach for modeling of flows of unconscious and conscious information\nin the human brain. In a series of models, Models 1-4, we consider cognitive\nsystems with increasing complexity of psychological behavior determined by\nstructure of flows of ideas. Since our models are in fact models of the\nAI-type, one immediately recognizes that they can be used for creation of\nAI-systems, which we call psycho-robots, exhibiting important elements of human\npsyche. Creation of such psycho-robots may be useful improvement of domestic\nrobots. At the moment domestic robots are merely simple working devices (e.g.\nvacuum cleaners or lawn mowers) . However, in future one can expect demand in\nsystems which be able not only perform simple work tasks, but would have\nelements of human self-developing psyche. Such AI-psyche could play an\nimportant role both in relations between psycho-robots and their owners as well\nas between psycho-robots. Since the presence of a huge numbers of\npsycho-complexes is an essential characteristic of human psychology, it would\nbe interesting to model them in the AI-framework.\n","vector":null,"chunk_id":"4a4a4c163e2e9cf1e3cc4fcef4452ece"}
{"title":"Supervised Machine Learning with a Novel Kernel Density Estimator","authors":"Yen-Jen Oyang, Darby Tien-Hao Chang, Yu-Yen Ou, Hao-Geng Hung,\n  Chih-Peng Wu and Chien-Yu Chen","category":"stat.ML","abstract":"  In recent years, kernel density estimation has been exploited by computer\nscientists to model machine learning problems. The kernel density estimation\nbased approaches are of interest due to the low time complexity of either O(n)\nor O(n*log(n)) for constructing a classifier, where n is the number of sampling\ninstances. Concerning design of kernel density estimators, one essential issue\nis how fast the pointwise mean square error (MSE) and/or the integrated mean\nsquare error (IMSE) diminish as the number of sampling instances increases. In\nthis article, it is shown that with the proposed kernel function it is feasible\nto make the pointwise MSE of the density estimator converge at O(n^-2/3)\nregardless of the dimension of the vector space, provided that the probability\ndensity function at the point of interest meets certain conditions.\n","text":"Title:Supervised Machine Learning with a Novel Kernel Density Estimator\nAbstract:  In recent years, kernel density estimation has been exploited by computer\nscientists to model machine learning problems. The kernel density estimation\nbased approaches are of interest due to the low time complexity of either O(n)\nor O(n*log(n)) for constructing a classifier, where n is the number of sampling\ninstances. Concerning design of kernel density estimators, one essential issue\nis how fast the pointwise mean square error (MSE) and/or the integrated mean\nsquare error (IMSE) diminish as the number of sampling instances increases. In\nthis article, it is shown that with the proposed kernel function it is feasible\nto make the pointwise MSE of the density estimator converge at O(n^-2/3)\nregardless of the dimension of the vector space, provided that the probability\ndensity function at the point of interest meets certain conditions.\n","vector":null,"chunk_id":"a2dcaf2cbce0d7ccc1572dd3a93f20f9"}
{"title":"Bayesian Classification and Regression with High Dimensional Features","authors":"Longhai Li","category":"stat.ML","abstract":"  This thesis responds to the challenges of using a large number, such as\nthousands, of features in regression and classification problems.\n  There are two situations where such high dimensional features arise. One is\nwhen high dimensional measurements are available, for example, gene expression\ndata produced by microarray techniques. For computational or other reasons,\npeople may select only a small subset of features when modelling such data, by\nlooking at how relevant the features are to predicting the response, based on\nsome measure such as correlation with the response in the training data.\nAlthough it is used very commonly, this procedure will make the response appear\nmore predictable than it actually is. In Chapter 2, we propose a Bayesian\nmethod to avoid this selection bias, with application to naive Bayes models and\nmixture models.\n  High dimensional features also arise when we consider high-order\ninteractions. The number of parameters will increase exponentially with the\norder considered. In Chapter 3, we propose a method for compressing a group of\nparameters into a single one, by exploiting the fact that many predictor\nvariables derived from high-order interactions have the same values for all the\ntraining cases. The number of compressed parameters may have converged before\nconsidering the highest possible order. We apply this compression method to\nlogistic sequence prediction models and logistic classification models.\n  We use both simulated data and real data to test our methods in both\nchapters.\n","text":"Title:Bayesian Classification and Regression with High Dimensional Features\nAbstract:  This thesis responds to the challenges of using a large number, such as\nthousands, of features in regression and classification problems.\n  There are two situations where such high dimensional features arise. One is\nwhen high dimensional measurements are available, for example, gene expression\ndata produced by microarray techniques. For computational or other reasons,\npeople may select only a small subset of features when modelling such data, by\nlooking at how relevant the features are to predicting the response, based on\nsome measure such as correlation with the response in the training data.\nAlthough it is used very commonly, this procedure will make the response appear\nmore predictable than it actually is. In Chapter 2, we propose a Bayesian\nmethod to avoid this selection bias, with application to naive Bayes models and\nmixture models.\n  High dimensional features also arise when we consider high-order\ninteractions. The number of parameters will increase exponentially with the\norder considered. In Chapter 3, we propose a method for compressing a group of\nparameters into a single one, by exploiting the fact that many predictor\nvariables derived from high-order interactions have the same values for all the\ntraining cases. The number of compressed parameters may have converged before\nconsidering the highest possible order. We apply this compression method to\nlogistic sequence prediction models and logistic classification models.\n  We use both simulated data and real data to test our methods in both\nchapters.\n","vector":null,"chunk_id":"6133becd6fc933491f77fc78dfd88456"}
{"title":"Simulated Annealing: Rigorous finite-time guarantees for optimization on\n  continuous domains","authors":"A. Lecchini-Visintini, J. Lygeros, J. Maciejowski","category":"stat.ML","abstract":"  Simulated annealing is a popular method for approaching the solution of a\nglobal optimization problem. Existing results on its performance apply to\ndiscrete combinatorial optimization where the optimization variables can assume\nonly a finite set of possible values. We introduce a new general formulation of\nsimulated annealing which allows one to guarantee finite-time performance in\nthe optimization of functions of continuous variables. The results hold\nuniversally for any optimization problem on a bounded domain and establish a\nconnection between simulated annealing and up-to-date theory of convergence of\nMarkov chain Monte Carlo methods on continuous domains. This work is inspired\nby the concept of finite-time learning with known accuracy and confidence\ndeveloped in statistical learning theory.\n","text":"Title:Simulated Annealing: Rigorous finite-time guarantees for optimization on\n  continuous domains\nAbstract:  Simulated annealing is a popular method for approaching the solution of a\nglobal optimization problem. Existing results on its performance apply to\ndiscrete combinatorial optimization where the optimization variables can assume\nonly a finite set of possible values. We introduce a new general formulation of\nsimulated annealing which allows one to guarantee finite-time performance in\nthe optimization of functions of continuous variables. The results hold\nuniversally for any optimization problem on a bounded domain and establish a\nconnection between simulated annealing and up-to-date theory of convergence of\nMarkov chain Monte Carlo methods on continuous domains. This work is inspired\nby the concept of finite-time learning with known accuracy and confidence\ndeveloped in statistical learning theory.\n","vector":null,"chunk_id":"8903b17268c38257bae9101ee4fd2a90"}
{"title":"Fitness landscape of the cellular automata majority problem: View from\n  the Olympus","authors":"S\\'ebastien Verel (I3S), Philippe Collard (I3S), Marco Tomassini\n  (ISI), Leonardo Vanneschi (DISCO)","category":"cs.AI","abstract":"  In this paper we study cellular automata (CAs) that perform the computational\nMajority task. This task is a good example of what the phenomenon of emergence\nin complex systems is. We take an interest in the reasons that make this\nparticular fitness landscape a difficult one. The first goal is to study the\nlandscape as such, and thus it is ideally independent from the actual\nheuristics used to search the space. However, a second goal is to understand\nthe features a good search technique for this particular problem space should\npossess. We statistically quantify in various ways the degree of difficulty of\nsearching this landscape. Due to neutrality, investigations based on sampling\ntechniques on the whole landscape are difficult to conduct. So, we go exploring\nthe landscape from the top. Although it has been proved that no CA can perform\nthe task perfectly, several efficient CAs for this task have been found.\nExploiting similarities between these CAs and symmetries in the landscape, we\ndefine the Olympus landscape which is regarded as the ''heavenly home'' of the\nbest local optima known (blok). Then we measure several properties of this\nsubspace. Although it is easier to find relevant CAs in this subspace than in\nthe overall landscape, there are structural reasons that prevent a searcher\nfrom finding overfitted CAs in the Olympus. Finally, we study dynamics and\nperformance of genetic algorithms on the Olympus in order to confirm our\nanalysis and to find efficient CAs for the Majority problem with low\ncomputational cost.\n","text":"Title:Fitness landscape of the cellular automata majority problem: View from\n  the Olympus\nAbstract:  In this paper we study cellular automata (CAs) that perform the computational\nMajority task. This task is a good example of what the phenomenon of emergence\nin complex systems is. We take an interest in the reasons that make this\nparticular fitness landscape a difficult one. The first goal is to study the\nlandscape as such, and thus it is ideally independent from the actual\nheuristics used to search the space. However, a second goal is to understand\nthe features a good search technique for this particular problem space should\npossess. We statistically quantify in various ways the degree of difficulty of\nsearching this landscape. Due to neutrality, investigations based on sampling\ntechniques on the whole landscape are difficult to conduct. So, we go exploring\nthe landscape from the top. Although it has been proved that no CA can perform\nthe task perfectly, several efficient CAs for this task have been found.\nExploiting similarities between these CAs and symmetries in the landscape, we\ndefine the Olympus landscape which is regarded as the ''heavenly home'' of the\nbest local optima known (blok). Then we measure several properties of this\nsubspace. Although it is easier to find relevant CAs in this subspace than in\nthe overall landscape, there are structural reasons that prevent a searcher\nfrom finding overfitted CAs in the Olympus. Finally, we study dynamics and\nperformance of genetic algorithms on the Olympus in order to confirm our\nanalysis and to find efficient CAs for the Majority problem with low\ncomputational cost.\n","vector":null,"chunk_id":"7a9f3e0ec9feb74cfc48608bae44c9e5"}
{"title":"Local search heuristics: Fitness Cloud versus Fitness Landscape","authors":"Philippe Collard (I3S), S\\'ebastien Verel (I3S), Manuel Clergue (I3S)","category":"cs.AI","abstract":"  This paper introduces the concept of fitness cloud as an alternative way to\nvisualize and analyze search spaces than given by the geographic notion of\nfitness landscape. It is argued that the fitness cloud concept overcomes\nseveral deficiencies of the landscape representation. Our analysis is based on\nthe correlation between fitness of solutions and fitnesses of nearest solutions\naccording to some neighboring. We focus on the behavior of local search\nheuristics, such as hill climber, on the well-known NK fitness landscape. In\nboth cases the fitness vs. fitness correlation is shown to be related to the\nepistatic parameter K.\n","text":"Title:Local search heuristics: Fitness Cloud versus Fitness Landscape\nAbstract:  This paper introduces the concept of fitness cloud as an alternative way to\nvisualize and analyze search spaces than given by the geographic notion of\nfitness landscape. It is argued that the fitness cloud concept overcomes\nseveral deficiencies of the landscape representation. Our analysis is based on\nthe correlation between fitness of solutions and fitnesses of nearest solutions\naccording to some neighboring. We focus on the behavior of local search\nheuristics, such as hill climber, on the well-known NK fitness landscape. In\nboth cases the fitness vs. fitness correlation is shown to be related to the\nepistatic parameter K.\n","vector":null,"chunk_id":"b5fa98023f479ce32730fb241804fd90"}
{"title":"Measuring the Evolvability Landscape to study Neutrality","authors":"S\\'ebastien Verel (I3S), Philippe Collard (I3S), Manuel Clergue (I3S)","category":"cs.AI","abstract":"  This theoretical work defines the measure of autocorrelation of evolvability\nin the context of neutral fitness landscape. This measure has been studied on\nthe classical MAX-SAT problem. This work highlight a new characteristic of\nneutral fitness landscapes which allows to design new adapted metaheuristic.\n","text":"Title:Measuring the Evolvability Landscape to study Neutrality\nAbstract:  This theoretical work defines the measure of autocorrelation of evolvability\nin the context of neutral fitness landscape. This measure has been studied on\nthe classical MAX-SAT problem. This work highlight a new characteristic of\nneutral fitness landscapes which allows to design new adapted metaheuristic.\n","vector":null,"chunk_id":"f988449267fcc4abfd8a11e6f535a586"}
{"title":"From Texts to Structured Documents: The Case of Health Practice\n  Guidelines","authors":"Amanda Bouffier (LIPN)","category":"cs.AI","abstract":"  This paper describes a system capable of semi-automatically filling an XML\ntemplate from free texts in the clinical domain (practice guidelines). The XML\ntemplate includes semantic information not explicitly encoded in the text\n(pairs of conditions and actions/recommendations). Therefore, there is a need\nto compute the exact scope of conditions over text sequences expressing the\nrequired actions. We present in this paper the rules developed for this task.\nWe show that the system yields good performance when applied to the analysis of\nFrench practice guidelines.\n","text":"Title:From Texts to Structured Documents: The Case of Health Practice\n  Guidelines\nAbstract:  This paper describes a system capable of semi-automatically filling an XML\ntemplate from free texts in the clinical domain (practice guidelines). The XML\ntemplate includes semantic information not explicitly encoded in the text\n(pairs of conditions and actions/recommendations). Therefore, there is a need\nto compute the exact scope of conditions over text sequences expressing the\nrequired actions. We present in this paper the rules developed for this task.\nWe show that the system yields good performance when applied to the analysis of\nFrench practice guidelines.\n","vector":null,"chunk_id":"79e4aa19ebb269de4bfc4896bab7b443"}
{"title":"The Extended Edit Distance Metric","authors":"Muhammad Marwan Muhammad Fuad (VALORIA), Pierre-Fran\\c{c}ois Marteau\n  (VALORIA)","category":"cs.IR","abstract":"  Similarity search is an important problem in information retrieval. This\nsimilarity is based on a distance. Symbolic representation of time series has\nattracted many researchers recently, since it reduces the dimensionality of\nthese high dimensional data objects. We propose a new distance metric that is\napplied to symbolic data objects and we test it on time series data bases in a\nclassification task. We compare it to other distances that are well known in\nthe literature for symbolic data objects. We also prove, mathematically, that\nour distance is metric.\n","text":"Title:The Extended Edit Distance Metric\nAbstract:  Similarity search is an important problem in information retrieval. This\nsimilarity is based on a distance. Symbolic representation of time series has\nattracted many researchers recently, since it reduces the dimensionality of\nthese high dimensional data objects. We propose a new distance metric that is\napplied to symbolic data objects and we test it on time series data bases in a\nclassification task. We compare it to other distances that are well known in\nthe literature for symbolic data objects. We also prove, mathematically, that\nour distance is metric.\n","vector":null,"chunk_id":"b5f45637e6c0b388a12dd84ca580b5e4"}
{"title":"Lagrangian Relaxation for MAP Estimation in Graphical Models","authors":"Jason K. Johnson, Dmitry M. Malioutov, Alan S. Willsky","category":"cs.AI","abstract":"  We develop a general framework for MAP estimation in discrete and Gaussian\ngraphical models using Lagrangian relaxation techniques. The key idea is to\nreformulate an intractable estimation problem as one defined on a more\ntractable graph, but subject to additional constraints. Relaxing these\nconstraints gives a tractable dual problem, one defined by a thin graph, which\nis then optimized by an iterative procedure. When this iterative optimization\nleads to a consistent estimate, one which also satisfies the constraints, then\nit corresponds to an optimal MAP estimate of the original model. Otherwise\nthere is a ``duality gap'', and we obtain a bound on the optimal solution.\nThus, our approach combines convex optimization with dynamic programming\ntechniques applicable for thin graphs. The popular tree-reweighted max-product\n(TRMP) method may be seen as solving a particular class of such relaxations,\nwhere the intractable graph is relaxed to a set of spanning trees. We also\nconsider relaxations to a set of small induced subgraphs, thin subgraphs (e.g.\nloops), and a connected tree obtained by ``unwinding'' cycles. In addition, we\npropose a new class of multiscale relaxations that introduce ``summary''\nvariables. The potential benefits of such generalizations include: reducing or\neliminating the ``duality gap'' in hard problems, reducing the number or\nLagrange multipliers in the dual problem, and accelerating convergence of the\niterative optimization procedure.\n","text":"Title:Lagrangian Relaxation for MAP Estimation in Graphical Models\nAbstract:  We develop a general framework for MAP estimation in discrete and Gaussian\ngraphical models using Lagrangian relaxation techniques. The key idea is to\nreformulate an intractable estimation problem as one defined on a more\ntractable graph, but subject to additional constraints. Relaxing these\nconstraints gives a tractable dual problem, one defined by a thin graph, which\nis then optimized by an iterative procedure. When this iterative optimization\nleads to a consistent estimate, one which also satisfies the constraints, then\nit corresponds to an optimal MAP estimate of the original model. Otherwise\nthere is a ``duality gap'', and we obtain a bound on the optimal solution.\nThus, our approach combines convex optimization with dynamic programming\ntechniques applicable for thin graphs. The popular tree-reweighted max-product\n(TRMP) method may be seen as solving a particular class of such relaxations,\nwhere the intractable graph is relaxed to a set of spanning trees. We also\nconsider relaxations to a set of small induced subgraphs, thin subgraphs (e.g.\nloops), and a connected tree obtained by ``unwinding'' cycles. In addition, we\npropose a new class of multiscale relaxations that introduce ``summary''\nvariables. The potential benefits of such generalizations include: reducing or\neliminating the ``duality gap'' in hard problems, reducing the number or\nLagrange multipliers in the dual problem, and accelerating convergence of the\niterative optimization procedure.\n","vector":null,"chunk_id":"8cd018ecc2139cdea7bd52b26a9dfb50"}
{"title":"The nested Chinese restaurant process and Bayesian nonparametric\n  inference of topic hierarchies","authors":"David M. Blei, Thomas L. Griffiths, Michael I. Jordan","category":"stat.ML","abstract":"  We present the nested Chinese restaurant process (nCRP), a stochastic process\nwhich assigns probability distributions to infinitely-deep,\ninfinitely-branching trees. We show how this stochastic process can be used as\na prior distribution in a Bayesian nonparametric model of document collections.\nSpecifically, we present an application to information retrieval in which\ndocuments are modeled as paths down a random tree, and the preferential\nattachment dynamics of the nCRP leads to clustering of documents according to\nsharing of topics at multiple levels of abstraction. Given a corpus of\ndocuments, a posterior inference algorithm finds an approximation to a\nposterior distribution over trees, topics and allocations of words to levels of\nthe tree. We demonstrate this algorithm on collections of scientific abstracts\nfrom several journals. This model exemplifies a recent trend in statistical\nmachine learning--the use of Bayesian nonparametric methods to infer\ndistributions on flexible data structures.\n","text":"Title:The nested Chinese restaurant process and Bayesian nonparametric\n  inference of topic hierarchies\nAbstract:  We present the nested Chinese restaurant process (nCRP), a stochastic process\nwhich assigns probability distributions to infinitely-deep,\ninfinitely-branching trees. We show how this stochastic process can be used as\na prior distribution in a Bayesian nonparametric model of document collections.\nSpecifically, we present an application to information retrieval in which\ndocuments are modeled as paths down a random tree, and the preferential\nattachment dynamics of the nCRP leads to clustering of documents according to\nsharing of topics at multiple levels of abstraction. Given a corpus of\ndocuments, a posterior inference algorithm finds an approximation to a\nposterior distribution over trees, topics and allocations of words to levels of\nthe tree. We demonstrate this algorithm on collections of scientific abstracts\nfrom several journals. This model exemplifies a recent trend in statistical\nmachine learning--the use of Bayesian nonparametric methods to infer\ndistributions on flexible data structures.\n","vector":null,"chunk_id":"8bc48f9264c3f5db47d660d468c8c59b"}
{"title":"Stanford Matrix Considered Harmful","authors":"Sebastiano Vigna","category":"cs.IR","abstract":"  This note argues about the validity of web-graph data used in the literature.\n","text":"Title:Stanford Matrix Considered Harmful\nAbstract:  This note argues about the validity of web-graph data used in the literature.\n","vector":null,"chunk_id":"b395ee81c1fead1b86145e813fd192a6"}
{"title":"Probabilistic coherence and proper scoring rules","authors":"Joel Predd, Robert Seiringer, Elliott H. Lieb, Daniel Osherson,\n  Vincent Poor, Sanjeev Kulkarni","category":"stat.ML","abstract":"  We provide self-contained proof of a theorem relating probabilistic coherence\nof forecasts to their non-domination by rival forecasts with respect to any\nproper scoring rule. The theorem appears to be new but is closely related to\nresults achieved by other investigators.\n","text":"Title:Probabilistic coherence and proper scoring rules\nAbstract:  We provide self-contained proof of a theorem relating probabilistic coherence\nof forecasts to their non-domination by rival forecasts with respect to any\nproper scoring rule. The theorem appears to be new but is closely related to\nresults achieved by other investigators.\n","vector":null,"chunk_id":"03899903659636ccf9e5e6d960752dc4"}
{"title":"Bayesian Online Changepoint Detection","authors":"Ryan Prescott Adams, David J.C. MacKay","category":"stat.ML","abstract":"  Changepoints are abrupt variations in the generative parameters of a data\nsequence. Online detection of changepoints is useful in modelling and\nprediction of time series in application areas such as finance, biometrics, and\nrobotics. While frequentist methods have yielded online filtering and\nprediction techniques, most Bayesian papers have focused on the retrospective\nsegmentation problem. Here we examine the case where the model parameters\nbefore and after the changepoint are independent and we derive an online\nalgorithm for exact inference of the most recent changepoint. We compute the\nprobability distribution of the length of the current ``run,'' or time since\nthe last changepoint, using a simple message-passing algorithm. Our\nimplementation is highly modular so that the algorithm may be applied to a\nvariety of types of data. We illustrate this modularity by demonstrating the\nalgorithm on three different real-world data sets.\n","text":"Title:Bayesian Online Changepoint Detection\nAbstract:  Changepoints are abrupt variations in the generative parameters of a data\nsequence. Online detection of changepoints is useful in modelling and\nprediction of time series in application areas such as finance, biometrics, and\nrobotics. While frequentist methods have yielded online filtering and\nprediction techniques, most Bayesian papers have focused on the retrospective\nsegmentation problem. Here we examine the case where the model parameters\nbefore and after the changepoint are independent and we derive an online\nalgorithm for exact inference of the most recent changepoint. We compute the\nprobability distribution of the length of the current ``run,'' or time since\nthe last changepoint, using a simple message-passing algorithm. Our\nimplementation is highly modular so that the algorithm may be applied to a\nvariety of types of data. We illustrate this modularity by demonstrating the\nalgorithm on three different real-world data sets.\n","vector":null,"chunk_id":"982ef7b360ebaa5fcf33f08223f9f52a"}
{"title":"Analyzing covert social network foundation behind terrorism disaster","authors":"Yoshiharu Maeno, and Yukio Ohsawa","category":"cs.AI","abstract":"  This paper addresses a method to analyze the covert social network foundation\nhidden behind the terrorism disaster. It is to solve a node discovery problem,\nwhich means to discover a node, which functions relevantly in a social network,\nbut escaped from monitoring on the presence and mutual relationship of nodes.\nThe method aims at integrating the expert investigator's prior understanding,\ninsight on the terrorists' social network nature derived from the complex graph\ntheory, and computational data processing. The social network responsible for\nthe 9/11 attack in 2001 is used to execute simulation experiment to evaluate\nthe performance of the method.\n","text":"Title:Analyzing covert social network foundation behind terrorism disaster\nAbstract:  This paper addresses a method to analyze the covert social network foundation\nhidden behind the terrorism disaster. It is to solve a node discovery problem,\nwhich means to discover a node, which functions relevantly in a social network,\nbut escaped from monitoring on the presence and mutual relationship of nodes.\nThe method aims at integrating the expert investigator's prior understanding,\ninsight on the terrorists' social network nature derived from the complex graph\ntheory, and computational data processing. The social network responsible for\nthe 9/11 attack in 2001 is used to execute simulation experiment to evaluate\nthe performance of the method.\n","vector":null,"chunk_id":"de7e9e4b4d2fe18a876419ddf1fd65ea"}
{"title":"Node discovery problem for a social network","authors":"Yoshiharu Maeno","category":"cs.AI","abstract":"  Methods to solve a node discovery problem for a social network are presented.\nCovert nodes refer to the nodes which are not observable directly. They\ntransmit the influence and affect the resulting collaborative activities among\nthe persons in a social network, but do not appear in the surveillance logs\nwhich record the participants of the collaborative activities. Discovering the\ncovert nodes is identifying the suspicious logs where the covert nodes would\nappear if the covert nodes became overt. The performance of the methods is\ndemonstrated with a test dataset generated from computationally synthesized\nnetworks and a real organization.\n","text":"Title:Node discovery problem for a social network\nAbstract:  Methods to solve a node discovery problem for a social network are presented.\nCovert nodes refer to the nodes which are not observable directly. They\ntransmit the influence and affect the resulting collaborative activities among\nthe persons in a social network, but do not appear in the surveillance logs\nwhich record the participants of the collaborative activities. Discovering the\ncovert nodes is identifying the suspicious logs where the covert nodes would\nappear if the covert nodes became overt. The performance of the methods is\ndemonstrated with a test dataset generated from computationally synthesized\nnetworks and a real organization.\n","vector":null,"chunk_id":"2f9847403c232fb44c973a19620352ac"}
{"title":"Predicting relevant empty spots in social interaction","authors":"Yoshiharu Maeno and Yukio Ohsawa","category":"cs.AI","abstract":"  An empty spot refers to an empty hard-to-fill space which can be found in the\nrecords of the social interaction, and is the clue to the persons in the\nunderlying social network who do not appear in the records. This contribution\naddresses a problem to predict relevant empty spots in social interaction.\nHomogeneous and inhomogeneous networks are studied as a model underlying the\nsocial interaction. A heuristic predictor function approach is presented as a\nnew method to address the problem. Simulation experiment is demonstrated over a\nhomogeneous network. A test data in the form of baskets is generated from the\nsimulated communication. Precision to predict the empty spots is calculated to\ndemonstrate the performance of the presented approach.\n","text":"Title:Predicting relevant empty spots in social interaction\nAbstract:  An empty spot refers to an empty hard-to-fill space which can be found in the\nrecords of the social interaction, and is the clue to the persons in the\nunderlying social network who do not appear in the records. This contribution\naddresses a problem to predict relevant empty spots in social interaction.\nHomogeneous and inhomogeneous networks are studied as a model underlying the\nsocial interaction. A heuristic predictor function approach is presented as a\nnew method to address the problem. Simulation experiment is demonstrated over a\nhomogeneous network. A test data in the form of baskets is generated from the\nsimulated communication. Precision to predict the empty spots is calculated to\ndemonstrate the performance of the presented approach.\n","vector":null,"chunk_id":"85fd5e273f403459863c03a70665e634"}
{"title":"Variable importance in binary regression trees and forests","authors":"Hemant Ishwaran","category":"stat.ML","abstract":"  We characterize and study variable importance (VIMP) and pairwise variable\nassociations in binary regression trees. A key component involves the node mean\nsquared error for a quantity we refer to as a maximal subtree. The theory\nnaturally extends from single trees to ensembles of trees and applies to\nmethods like random forests. This is useful because while importance values\nfrom random forests are used to screen variables, for example they are used to\nfilter high throughput genomic data in Bioinformatics, very little theory\nexists about their properties.\n","text":"Title:Variable importance in binary regression trees and forests\nAbstract:  We characterize and study variable importance (VIMP) and pairwise variable\nassociations in binary regression trees. A key component involves the node mean\nsquared error for a quantity we refer to as a maximal subtree. The theory\nnaturally extends from single trees to ensembles of trees and applies to\nmethods like random forests. This is useful because while importance values\nfrom random forests are used to screen variables, for example they are used to\nfilter high throughput genomic data in Bioinformatics, very little theory\nexists about their properties.\n","vector":null,"chunk_id":"e632ed41553aecc832762c6a069ff2cf"}
{"title":"Premi\\`ere \\'etape vers une navigation r\\'ef\\'erentielle par l'image\n  pour l'assistance \\`a la conception des ambiances lumineuses","authors":"Salma Chaabouni (MAP / Crai), Jc Bignon (MAP / Crai), Gilles Halin\n  (MAP / Crai)","category":"cs.IR","abstract":"  In the first design stage, image reference plays a double role of means of\nformulation and resolution of problems. In our approach, we consider image\nreference as a support of creation activity to generate ideas and we propose a\ntool for navigation in references by image in order to assist daylight ambience\ndesign. Within this paper, we present, in a first part, the semantic indexation\nmethod to be used for the indexation of our image database. In a second part we\npropose a synthetic analysis of various modes of referential navigation in\norder to propose a tool implementing all or a part of these modes.\n","text":"Title:Premi\\`ere \\'etape vers une navigation r\\'ef\\'erentielle par l'image\n  pour l'assistance \\`a la conception des ambiances lumineuses\nAbstract:  In the first design stage, image reference plays a double role of means of\nformulation and resolution of problems. In our approach, we consider image\nreference as a support of creation activity to generate ideas and we propose a\ntool for navigation in references by image in order to assist daylight ambience\ndesign. Within this paper, we present, in a first part, the semantic indexation\nmethod to be used for the indexation of our image database. In a second part we\npropose a synthetic analysis of various modes of referential navigation in\norder to propose a tool implementing all or a part of these modes.\n","vector":null,"chunk_id":"701a78be9b060e060abce6286941f051"}
{"title":"Use of Wikipedia Categories in Entity Ranking","authors":"James A. Thom (RMIT), Jovan Pehcevski (INRIA Rocquencourt / INRIA\n  Sophia Antipolis), Anne-Marie Vercoustre (INRIA Rocquencourt / INRIA Sophia\n  Antipolis)","category":"cs.IR","abstract":"  Wikipedia is a useful source of knowledge that has many applications in\nlanguage processing and knowledge representation. The Wikipedia category graph\ncan be compared with the class hierarchy in an ontology; it has some\ncharacteristics in common as well as some differences. In this paper, we\npresent our approach for answering entity ranking queries from the Wikipedia.\nIn particular, we explore how to make use of Wikipedia categories to improve\nentity ranking effectiveness. Our experiments show that using categories of\nexample entities works significantly better than using loosely defined target\ncategories.\n","text":"Title:Use of Wikipedia Categories in Entity Ranking\nAbstract:  Wikipedia is a useful source of knowledge that has many applications in\nlanguage processing and knowledge representation. The Wikipedia category graph\ncan be compared with the class hierarchy in an ontology; it has some\ncharacteristics in common as well as some differences. In this paper, we\npresent our approach for answering entity ranking queries from the Wikipedia.\nIn particular, we explore how to make use of Wikipedia categories to improve\nentity ranking effectiveness. Our experiments show that using categories of\nexample entities works significantly better than using loosely defined target\ncategories.\n","vector":null,"chunk_id":"b123b1832fe32aa4477034fe6feb8928"}
{"title":"Entity Ranking in Wikipedia","authors":"Anne-Marie Vercoustre (INRIA Rocquencourt / INRIA Sophia Antipolis),\n  James A. Thom (RMIT), Jovan Pehcevski (INRIA Rocquencourt / INRIA Sophia\n  Antipolis)","category":"cs.IR","abstract":"  The traditional entity extraction problem lies in the ability of extracting\nnamed entities from plain text using natural language processing techniques and\nintensive training from large document collections. Examples of named entities\ninclude organisations, people, locations, or dates. There are many research\nactivities involving named entities; we are interested in entity ranking in the\nfield of information retrieval. In this paper, we describe our approach to\nidentifying and ranking entities from the INEX Wikipedia document collection.\nWikipedia offers a number of interesting features for entity identification and\nranking that we first introduce. We then describe the principles and the\narchitecture of our entity ranking system, and introduce our methodology for\nevaluation. Our preliminary results show that the use of categories and the\nlink structure of Wikipedia, together with entity examples, can significantly\nimprove retrieval effectiveness.\n","text":"Title:Entity Ranking in Wikipedia\nAbstract:  The traditional entity extraction problem lies in the ability of extracting\nnamed entities from plain text using natural language processing techniques and\nintensive training from large document collections. Examples of named entities\ninclude organisations, people, locations, or dates. There are many research\nactivities involving named entities; we are interested in entity ranking in the\nfield of information retrieval. In this paper, we describe our approach to\nidentifying and ranking entities from the INEX Wikipedia document collection.\nWikipedia offers a number of interesting features for entity identification and\nranking that we first introduce. We then describe the principles and the\narchitecture of our entity ranking system, and introduce our methodology for\nevaluation. Our preliminary results show that the use of categories and the\nlink structure of Wikipedia, together with entity examples, can significantly\nimprove retrieval effectiveness.\n","vector":null,"chunk_id":"2636c1246360509107b1d75cce5b3d5b"}
{"title":"Translating OWL and Semantic Web Rules into Prolog: Moving Toward\n  Description Logic Programs","authors":"Ken Samuel, Leo Obrst, Suzette Stoutenberg, Karen Fox, Paul Franklin,\n  Adrian Johnson, Ken Laskey, Deborah Nichols, Steve Lopez, Jason Peterson","category":"cs.AI","abstract":"  To appear in Theory and Practice of Logic Programming (TPLP), 2008.\n  We are researching the interaction between the rule and the ontology layers\nof the Semantic Web, by comparing two options: 1) using OWL and its rule\nextension SWRL to develop an integrated ontology/rule language, and 2) layering\nrules on top of an ontology with RuleML and OWL. Toward this end, we are\ndeveloping the SWORIER system, which enables efficient automated reasoning on\nontologies and rules, by translating all of them into Prolog and adding a set\nof general rules that properly capture the semantics of OWL. We have also\nenabled the user to make dynamic changes on the fly, at run time. This work\naddresses several of the concerns expressed in previous work, such as negation,\ncomplementary classes, disjunctive heads, and cardinality, and it discusses\nalternative approaches for dealing with inconsistencies in the knowledge base.\nIn addition, for efficiency, we implemented techniques called\nextensionalization, avoiding reanalysis, and code minimization.\n","text":"Title:Translating OWL and Semantic Web Rules into Prolog: Moving Toward\n  Description Logic Programs\nAbstract:  To appear in Theory and Practice of Logic Programming (TPLP), 2008.\n  We are researching the interaction between the rule and the ontology layers\nof the Semantic Web, by comparing two options: 1) using OWL and its rule\nextension SWRL to develop an integrated ontology/rule language, and 2) layering\nrules on top of an ontology with RuleML and OWL. Toward this end, we are\ndeveloping the SWORIER system, which enables efficient automated reasoning on\nontologies and rules, by translating all of them into Prolog and adding a set\nof general rules that properly capture the semantics of OWL. We have also\nenabled the user to make dynamic changes on the fly, at run time. This work\naddresses several of the concerns expressed in previous work, such as negation,\ncomplementary classes, disjunctive heads, and cardinality, and it discusses\nalternative approaches for dealing with inconsistencies in the knowledge base.\nIn addition, for efficiency, we implemented techniques called\nextensionalization, avoiding reanalysis, and code minimization.\n","vector":null,"chunk_id":"8cc5356adcef391755cd38e3bca6c892"}
{"title":"Iterative Filtering for a Dynamical Reputation System","authors":"Cristobald de Kerchove, Paul Van Dooren","category":"cs.IR","abstract":"  The paper introduces a novel iterative method that assigns a reputation to n\n+ m items: n raters and m objects. Each rater evaluates a subset of objects\nleading to a n x m rating matrix with a certain sparsity pattern. From this\nrating matrix we give a nonlinear formula to define the reputation of raters\nand objects. We also provide an iterative algorithm that superlinearly\nconverges to the unique vector of reputations and this for any rating matrix.\nIn contrast to classical outliers detection, no evaluation is discarded in this\nmethod but each one is taken into account with different weights for the\nreputation of the objects. The complexity of one iteration step is linear in\nthe number of evaluations, making our algorithm efficient for large data set.\nExperiments show good robustness of the reputation of the objects against\ncheaters and spammers and good detection properties of cheaters and spammers.\n","text":"Title:Iterative Filtering for a Dynamical Reputation System\nAbstract:  The paper introduces a novel iterative method that assigns a reputation to n\n+ m items: n raters and m objects. Each rater evaluates a subset of objects\nleading to a n x m rating matrix with a certain sparsity pattern. From this\nrating matrix we give a nonlinear formula to define the reputation of raters\nand objects. We also provide an iterative algorithm that superlinearly\nconverges to the unique vector of reputations and this for any rating matrix.\nIn contrast to classical outliers detection, no evaluation is discarded in this\nmethod but each one is taken into account with different weights for the\nreputation of the objects. The complexity of one iteration step is linear in\nthe number of evaluations, making our algorithm efficient for large data set.\nExperiments show good robustness of the reputation of the objects against\ncheaters and spammers and good detection properties of cheaters and spammers.\n","vector":null,"chunk_id":"fb768534284563376e72b89e6deec583"}
{"title":"Pac-Bayesian Supervised Classification: The Thermodynamics of\n  Statistical Learning","authors":"Olivier Catoni","category":"stat.ML","abstract":"  This monograph deals with adaptive supervised classification, using tools\nborrowed from statistical mechanics and information theory, stemming from the\nPACBayesian approach pioneered by David McAllester and applied to a conception\nof statistical learning theory forged by Vladimir Vapnik. Using convex analysis\non the set of posterior probability measures, we show how to get local measures\nof the complexity of the classification model involving the relative entropy of\nposterior distributions with respect to Gibbs posterior measures. We then\ndiscuss relative bounds, comparing the generalization error of two\nclassification rules, showing how the margin assumption of Mammen and Tsybakov\ncan be replaced with some empirical measure of the covariance structure of the\nclassification model.We show how to associate to any posterior distribution an\neffective temperature relating it to the Gibbs prior distribution with the same\nlevel of expected error rate, and how to estimate this effective temperature\nfrom data, resulting in an estimator whose expected error rate converges\naccording to the best possible power of the sample size adaptively under any\nmargin and parametric complexity assumptions. We describe and study an\nalternative selection scheme based on relative bounds between estimators, and\npresent a two step localization technique which can handle the selection of a\nparametric model from a family of those. We show how to extend systematically\nall the results obtained in the inductive setting to transductive learning, and\nuse this to improve Vapnik's generalization bounds, extending them to the case\nwhen the sample is made of independent non-identically distributed pairs of\npatterns and labels. Finally we review briefly the construction of Support\nVector Machines and show how to derive generalization bounds for them,\nmeasuring the complexity either through the number of support vectors or\nthrough the value of the transductive or inductive margin.\n","text":"Title:Pac-Bayesian Supervised Classification: The Thermodynamics of\n  Statistical Learning\nAbstract:  This monograph deals with adaptive supervised classification, using tools\nborrowed from statistical mechanics and information theory, stemming from the\nPACBayesian approach pioneered by David McAllester and applied to a conception\nof statistical learning theory forged by Vladimir Vapnik. Using convex analysis\non the set of posterior probability measures, we show how to get local measures\nof the complexity of the classification model involving the relative entropy of\nposterior distributions with respect to Gibbs posterior measures. We then\ndiscuss relative bounds, comparing the generalization error of two\nclassification rules, showing how the margin assumption of Mammen and Tsybakov\ncan be replaced with some empirical measure of the covariance structure of the\nclassification model.We show how to associate to any posterior distribution an\neffective temperature relating it to the Gibbs prior distribution with the same\nlevel of expected error rate, and how to estimate this effective temperature\nfrom data, resulting in an estimator whose expected error rate converges\naccording to the best possible power of the sample size adaptively under any\nmargin and parametric complexity assumptions. We describe and study an\nalternative selection scheme based on relative bounds between estimators, and\npresent a two step localization technique which can handle the selection of a\nparametric model from a family of those. We show how to extend systematically\nall the results obtained in the inductive setting to transductive learning, and\nuse this to improve Vapnik's generalization bounds, extending them to the case\nwhen the sample is made of independent non-identically distributed pairs of\npatterns and labels. Finally we review briefly the construction of Support\nVector Machines and show how to derive generalization bounds for them,\nmeasuring the complexity either through the number of support vectors or\nthrough the value of the transductive or inductive margin.\n","vector":null,"chunk_id":"8755333bffbdd75fbfeed0960b9d257a"}
{"title":"Evolving localizations in reaction-diffusion cellular automata","authors":"Andrew Adamatzky, Larry Bull, Pierre Collet, Emmanuel Sapin","category":"cs.AI","abstract":"  We consider hexagonal cellular automata with immediate cell neighbourhood and\nthree cell-states. Every cell calculates its next state depending on the\nintegral representation of states in its neighbourhood, i.e. how many\nneighbours are in each one state. We employ evolutionary algorithms to breed\nlocal transition functions that support mobile localizations (gliders), and\ncharacterize sets of the functions selected in terms of quasi-chemical systems.\nAnalysis of the set of functions evolved allows to speculate that mobile\nlocalizations are likely to emerge in the quasi-chemical systems with limited\ndiffusion of one reagent, a small number of molecules is required for\namplification of travelling localizations, and reactions leading to stationary\nlocalizations involve relatively equal amount of quasi-chemical species.\nTechniques developed can be applied in cascading signals in nature-inspired\nspatially extended computing devices, and phenomenological studies and\nclassification of non-linear discrete systems.\n","text":"Title:Evolving localizations in reaction-diffusion cellular automata\nAbstract:  We consider hexagonal cellular automata with immediate cell neighbourhood and\nthree cell-states. Every cell calculates its next state depending on the\nintegral representation of states in its neighbourhood, i.e. how many\nneighbours are in each one state. We employ evolutionary algorithms to breed\nlocal transition functions that support mobile localizations (gliders), and\ncharacterize sets of the functions selected in terms of quasi-chemical systems.\nAnalysis of the set of functions evolved allows to speculate that mobile\nlocalizations are likely to emerge in the quasi-chemical systems with limited\ndiffusion of one reagent, a small number of molecules is required for\namplification of travelling localizations, and reactions leading to stationary\nlocalizations involve relatively equal amount of quasi-chemical species.\nTechniques developed can be applied in cascading signals in nature-inspired\nspatially extended computing devices, and phenomenological studies and\nclassification of non-linear discrete systems.\n","vector":null,"chunk_id":"d88f50c15b49b97d2cb56417d975156a"}
{"title":"An axiomatic approach to intrinsic dimension of a dataset","authors":"Vladimir Pestov","category":"cs.IR","abstract":"  We perform a deeper analysis of an axiomatic approach to the concept of\nintrinsic dimension of a dataset proposed by us in the IJCNN'07 paper\n(arXiv:cs/0703125). The main features of our approach are that a high intrinsic\ndimension of a dataset reflects the presence of the curse of dimensionality (in\na certain mathematically precise sense), and that dimension of a discrete\ni.i.d. sample of a low-dimensional manifold is, with high probability, close to\nthat of the manifold. At the same time, the intrinsic dimension of a sample is\neasily corrupted by moderate high-dimensional noise (of the same amplitude as\nthe size of the manifold) and suffers from prohibitevely high computational\ncomplexity (computing it is an $NP$-complete problem). We outline a possible\nway to overcome these difficulties.\n","text":"Title:An axiomatic approach to intrinsic dimension of a dataset\nAbstract:  We perform a deeper analysis of an axiomatic approach to the concept of\nintrinsic dimension of a dataset proposed by us in the IJCNN'07 paper\n(arXiv:cs/0703125). The main features of our approach are that a high intrinsic\ndimension of a dataset reflects the presence of the curse of dimensionality (in\na certain mathematically precise sense), and that dimension of a discrete\ni.i.d. sample of a low-dimensional manifold is, with high probability, close to\nthat of the manifold. At the same time, the intrinsic dimension of a sample is\neasily corrupted by moderate high-dimensional noise (of the same amplitude as\nthe size of the manifold) and suffers from prohibitevely high computational\ncomplexity (computing it is an $NP$-complete problem). We outline a possible\nway to overcome these difficulties.\n","vector":null,"chunk_id":"acf47d05bba386fa18e083e226267442"}
{"title":"Decomposition During Search for Propagation-Based Constraint Solvers","authors":"Martin Mann and Guido Tack and Sebastian Will","category":"cs.AI","abstract":"  We describe decomposition during search (DDS), an integration of And/Or tree\nsearch into propagation-based constraint solvers. The presented search\nalgorithm dynamically decomposes sub-problems of a constraint satisfaction\nproblem into independent partial problems, avoiding redundant work.\n  The paper discusses how DDS interacts with key features that make\npropagation-based solvers successful: constraint propagation, especially for\nglobal constraints, and dynamic search heuristics.\n  We have implemented DDS for the Gecode constraint programming library. Two\napplications, solution counting in graph coloring and protein structure\nprediction, exemplify the benefits of DDS in practice.\n","text":"Title:Decomposition During Search for Propagation-Based Constraint Solvers\nAbstract:  We describe decomposition during search (DDS), an integration of And/Or tree\nsearch into propagation-based constraint solvers. The presented search\nalgorithm dynamically decomposes sub-problems of a constraint satisfaction\nproblem into independent partial problems, avoiding redundant work.\n  The paper discusses how DDS interacts with key features that make\npropagation-based solvers successful: constraint propagation, especially for\nglobal constraints, and dynamic search heuristics.\n  We have implemented DDS for the Gecode constraint programming library. Two\napplications, solution counting in graph coloring and protein structure\nprediction, exemplify the benefits of DDS in practice.\n","vector":null,"chunk_id":"cff7454007b4094676eaf448d9dbe50c"}
{"title":"Universal Intelligence: A Definition of Machine Intelligence","authors":"Shane Legg and Marcus Hutter","category":"cs.AI","abstract":"  A fundamental problem in artificial intelligence is that nobody really knows\nwhat intelligence is. The problem is especially acute when we need to consider\nartificial systems which are significantly different to humans. In this paper\nwe approach this problem in the following way: We take a number of well known\ninformal definitions of human intelligence that have been given by experts, and\nextract their essential features. These are then mathematically formalised to\nproduce a general measure of intelligence for arbitrary machines. We believe\nthat this equation formally captures the concept of machine intelligence in the\nbroadest reasonable sense. We then show how this formal definition is related\nto the theory of universal optimal learning agents. Finally, we survey the many\nother tests and definitions of intelligence that have been proposed for\nmachines.\n","text":"Title:Universal Intelligence: A Definition of Machine Intelligence\nAbstract:  A fundamental problem in artificial intelligence is that nobody really knows\nwhat intelligence is. The problem is especially acute when we need to consider\nartificial systems which are significantly different to humans. In this paper\nwe approach this problem in the following way: We take a number of well known\ninformal definitions of human intelligence that have been given by experts, and\nextract their essential features. These are then mathematically formalised to\nproduce a general measure of intelligence for arbitrary machines. We believe\nthat this equation formally captures the concept of machine intelligence in the\nbroadest reasonable sense. We then show how this formal definition is related\nto the theory of universal optimal learning agents. Finally, we survey the many\nother tests and definitions of intelligence that have been proposed for\nmachines.\n","vector":null,"chunk_id":"e7541f3752a55da0df78e43808030bc2"}
{"title":"Tests of Machine Intelligence","authors":"Shane Legg and Marcus Hutter","category":"cs.AI","abstract":"  Although the definition and measurement of intelligence is clearly of\nfundamental importance to the field of artificial intelligence, no general\nsurvey of definitions and tests of machine intelligence exists. Indeed few\nresearchers are even aware of alternatives to the Turing test and its many\nderivatives. In this paper we fill this gap by providing a short survey of the\nmany tests of machine intelligence that have been proposed.\n","text":"Title:Tests of Machine Intelligence\nAbstract:  Although the definition and measurement of intelligence is clearly of\nfundamental importance to the field of artificial intelligence, no general\nsurvey of definitions and tests of machine intelligence exists. Indeed few\nresearchers are even aware of alternatives to the Turing test and its many\nderivatives. In this paper we fill this gap by providing a short survey of the\nmany tests of machine intelligence that have been proposed.\n","vector":null,"chunk_id":"bfa6cd5e759ce19e81c83367a69455b0"}
{"title":"Convergence of Expected Utilities with Algorithmic Probability\n  Distributions","authors":"Peter de Blanc","category":"cs.AI","abstract":"  We consider an agent interacting with an unknown environment. The environment\nis a function which maps natural numbers to natural numbers; the agent's set of\nhypotheses about the environment contains all such functions which are\ncomputable and compatible with a finite set of known input-output pairs, and\nthe agent assigns a positive probability to each such hypothesis. We do not\nrequire that this probability distribution be computable, but it must be\nbounded below by a positive computable function. The agent has a utility\nfunction on outputs from the environment. We show that if this utility function\nis bounded below in absolute value by an unbounded computable function, then\nthe expected utility of any input is undefined. This implies that a computable\nutility function will have convergent expected utilities iff that function is\nbounded.\n","text":"Title:Convergence of Expected Utilities with Algorithmic Probability\n  Distributions\nAbstract:  We consider an agent interacting with an unknown environment. The environment\nis a function which maps natural numbers to natural numbers; the agent's set of\nhypotheses about the environment contains all such functions which are\ncomputable and compatible with a finite set of known input-output pairs, and\nthe agent assigns a positive probability to each such hypothesis. We do not\nrequire that this probability distribution be computable, but it must be\nbounded below by a positive computable function. The agent has a utility\nfunction on outputs from the environment. We show that if this utility function\nis bounded below in absolute value by an unbounded computable function, then\nthe expected utility of any input is undefined. This implies that a computable\nutility function will have convergent expected utilities iff that function is\nbounded.\n","vector":null,"chunk_id":"6747b04d05b5d361ebd611c308ed1d24"}
{"title":"Le terme et le concept : fondements d'une ontoterminologie","authors":"Christophe Roche (LISTIC)","category":"cs.AI","abstract":"  Most definitions of ontology, viewed as a \"specification of a\nconceptualization\", agree on the fact that if an ontology can take different\nforms, it necessarily includes a vocabulary of terms and some specification of\ntheir meaning in relation to the domain's conceptualization. And as domain\nknowledge is mainly conveyed through scientific and technical texts, we can\nhope to extract some useful information from them for building ontology. But is\nit as simple as this? In this article we shall see that the lexical structure,\ni.e. the network of words linked by linguistic relationships, does not\nnecessarily match the domain conceptualization. We have to bear in mind that\nwriting documents is the concern of textual linguistics, of which one of the\nprinciples is the incompleteness of text, whereas building ontology - viewed as\ntask-independent knowledge - is concerned with conceptualization based on\nformal and not natural languages. Nevertheless, the famous Sapir and Whorf\nhypothesis, concerning the interdependence of thought and language, is also\napplicable to formal languages. This means that the way an ontology is built\nand a concept is defined depends directly on the formal language which is used;\nand the results will not be the same. The introduction of the notion of\nontoterminology allows to take into account epistemological principles for\nformal ontology building.\n","text":"Title:Le terme et le concept : fondements d'une ontoterminologie\nAbstract:  Most definitions of ontology, viewed as a \"specification of a\nconceptualization\", agree on the fact that if an ontology can take different\nforms, it necessarily includes a vocabulary of terms and some specification of\ntheir meaning in relation to the domain's conceptualization. And as domain\nknowledge is mainly conveyed through scientific and technical texts, we can\nhope to extract some useful information from them for building ontology. But is\nit as simple as this? In this article we shall see that the lexical structure,\ni.e. the network of words linked by linguistic relationships, does not\nnecessarily match the domain conceptualization. We have to bear in mind that\nwriting documents is the concern of textual linguistics, of which one of the\nprinciples is the incompleteness of text, whereas building ontology - viewed as\ntask-independent knowledge - is concerned with conceptualization based on\nformal and not natural languages. Nevertheless, the famous Sapir and Whorf\nhypothesis, concerning the interdependence of thought and language, is also\napplicable to formal languages. This means that the way an ontology is built\nand a concept is defined depends directly on the formal language which is used;\nand the results will not be the same. The introduction of the notion of\nontoterminology allows to take into account epistemological principles for\nformal ontology building.\n","vector":null,"chunk_id":"915f190653031cab642265599bd99fed"}
{"title":"Stream Computing","authors":"Subhash Kak","category":"cs.AI","abstract":"  Stream computing is the use of multiple autonomic and parallel modules\ntogether with integrative processors at a higher level of abstraction to embody\n\"intelligent\" processing. The biological basis of this computing is sketched\nand the matter of learning is examined.\n","text":"Title:Stream Computing\nAbstract:  Stream computing is the use of multiple autonomic and parallel modules\ntogether with integrative processors at a higher level of abstraction to embody\n\"intelligent\" processing. The biological basis of this computing is sketched\nand the matter of learning is examined.\n","vector":null,"chunk_id":"7fc4838a5a2dcad9014d5673da0e478f"}
{"title":"Encoding changing country codes for the Semantic Web with ISO 3166 and\n  SKOS","authors":"Jakob Voss","category":"cs.IR","abstract":"  This paper shows how authority files can be encoded for the Semantic Web with\nthe Simple Knowledge Organisation System (SKOS). In particular the application\nof SKOS for encoding the structure, management, and utilization of country\ncodes as defined in ISO 3166 is demonstrated. The proposed encoding gives a use\ncase for SKOS that includes features that have only been discussed little so\nfar, such as multiple notations, nested concept schemes, changes by versioning.\n","text":"Title:Encoding changing country codes for the Semantic Web with ISO 3166 and\n  SKOS\nAbstract:  This paper shows how authority files can be encoded for the Semantic Web with\nthe Simple Knowledge Organisation System (SKOS). In particular the application\nof SKOS for encoding the structure, management, and utilization of country\ncodes as defined in ISO 3166 is demonstrated. The proposed encoding gives a use\ncase for SKOS that includes features that have only been discussed little so\nfar, such as multiple notations, nested concept schemes, changes by versioning.\n","vector":null,"chunk_id":"ebaf2874163fac820c9a5f86d1e8e9f2"}
{"title":"Anisotropic selection in cellular genetic algorithms","authors":"David Simoncini (I3S), S\\'ebastien Verel (I3S), Philippe Collard\n  (I3S), Manuel Clergue (I3S)","category":"cs.AI","abstract":"  In this paper we introduce a new selection scheme in cellular genetic\nalgorithms (cGAs). Anisotropic Selection (AS) promotes diversity and allows\naccurate control of the selective pressure. First we compare this new scheme\nwith the classical rectangular grid shapes solution according to the selective\npressure: we can obtain the same takeover time with the two techniques although\nthe spreading of the best individual is different. We then give experimental\nresults that show to what extent AS promotes the emergence of niches that\nsupport low coupling and high cohesion. Finally, using a cGA with anisotropic\nselection on a Quadratic Assignment Problem we show the existence of an\nanisotropic optimal value for which the best average performance is observed.\nFurther work will focus on the selective pressure self-adjustment ability\nprovided by this new selection scheme.\n","text":"Title:Anisotropic selection in cellular genetic algorithms\nAbstract:  In this paper we introduce a new selection scheme in cellular genetic\nalgorithms (cGAs). Anisotropic Selection (AS) promotes diversity and allows\naccurate control of the selective pressure. First we compare this new scheme\nwith the classical rectangular grid shapes solution according to the selective\npressure: we can obtain the same takeover time with the two techniques although\nthe spreading of the best individual is different. We then give experimental\nresults that show to what extent AS promotes the emergence of niches that\nsupport low coupling and high cohesion. Finally, using a cGA with anisotropic\nselection on a Quadratic Assignment Problem we show the existence of an\nanisotropic optimal value for which the best average performance is observed.\nFurther work will focus on the selective pressure self-adjustment ability\nprovided by this new selection scheme.\n","vector":null,"chunk_id":"ea96945b31393b8bd60c8c0fefd4e492"}
{"title":"Classification Constrained Dimensionality Reduction","authors":"Raviv Raich, Jose A. Costa, Steven B. Damelin, and Alfred O. Hero III","category":"stat.ML","abstract":"  Dimensionality reduction is a topic of recent interest. In this paper, we\npresent the classification constrained dimensionality reduction (CCDR)\nalgorithm to account for label information. The algorithm can account for\nmultiple classes as well as the semi-supervised setting. We present an\nout-of-sample expressions for both labeled and unlabeled data. For unlabeled\ndata, we introduce a method of embedding a new point as preprocessing to a\nclassifier. For labeled data, we introduce a method that improves the embedding\nduring the training phase using the out-of-sample extension. We investigate\nclassification performance using the CCDR algorithm on hyper-spectral satellite\nimagery data. We demonstrate the performance gain for both local and global\nclassifiers and demonstrate a 10% improvement of the $k$-nearest neighbors\nalgorithm performance. We present a connection between intrinsic dimension\nestimation and the optimal embedding dimension obtained using the CCDR\nalgorithm.\n","text":"Title:Classification Constrained Dimensionality Reduction\nAbstract:  Dimensionality reduction is a topic of recent interest. In this paper, we\npresent the classification constrained dimensionality reduction (CCDR)\nalgorithm to account for label information. The algorithm can account for\nmultiple classes as well as the semi-supervised setting. We present an\nout-of-sample expressions for both labeled and unlabeled data. For unlabeled\ndata, we introduce a method of embedding a new point as preprocessing to a\nclassifier. For labeled data, we introduce a method that improves the embedding\nduring the training phase using the out-of-sample extension. We investigate\nclassification performance using the CCDR algorithm on hyper-spectral satellite\nimagery data. We demonstrate the performance gain for both local and global\nclassifiers and demonstrate a 10% improvement of the $k$-nearest neighbors\nalgorithm performance. We present a connection between intrinsic dimension\nestimation and the optimal embedding dimension obtained using the CCDR\nalgorithm.\n","vector":null,"chunk_id":"1a4b6979627b807e9a1a6beb0811a6c4"}
{"title":"Time Warp Edit Distance","authors":"Pierre-Fran\\c{c}ois Marteau (VALORIA)","category":"cs.IR","abstract":"  This technical report details a family of time warp distances on the set of\ndiscrete time series. This family is constructed as an editing distance whose\nelementary operations apply on linear segments. A specific parameter allows\ncontrolling the stiffness of the elastic matching. It is well suited for the\nprocessing of event data for which each data sample is associated with a\ntimestamp, not necessarily obtained according to a constant sampling rate. Some\nproperties verified by these distances are proposed and proved in this report.\n","text":"Title:Time Warp Edit Distance\nAbstract:  This technical report details a family of time warp distances on the set of\ndiscrete time series. This family is constructed as an editing distance whose\nelementary operations apply on linear segments. A specific parameter allows\ncontrolling the stiffness of the elastic matching. It is well suited for the\nprocessing of event data for which each data sample is associated with a\ntimestamp, not necessarily obtained according to a constant sampling rate. Some\nproperties verified by these distances are proposed and proved in this report.\n","vector":null,"chunk_id":"8d11d625092608e31c1547363b00c3ed"}
{"title":"Website Optimization through Mining User Navigational Pattern","authors":"Biswajit Biswal","category":"cs.IR","abstract":"  With the World Wide Web's ubiquity increase and the rapid development of\nvarious online businesses, the complexity of web sites grow. The analysis of\nweb user's navigational pattern within a web site can provide useful\ninformation for server performance enhancements, restructuring a website and\ndirect marketing in e-commerce etc. In this paper, an algorithm is proposed for\nmining such navigation patterns. The key insight is that users access\ninformation of interest and follow a certain path while navigating a web site.\nIf they don't find it, they would backtrack and choose among the alternate\npaths till they reach the destination. The point they backtrack is the\nIntermediate Reference Location. Identifying such Intermediate locations and\ndestinations out of the pattern will be the main endeavor in the rest of this\nreport.\n","text":"Title:Website Optimization through Mining User Navigational Pattern\nAbstract:  With the World Wide Web's ubiquity increase and the rapid development of\nvarious online businesses, the complexity of web sites grow. The analysis of\nweb user's navigational pattern within a web site can provide useful\ninformation for server performance enhancements, restructuring a website and\ndirect marketing in e-commerce etc. In this paper, an algorithm is proposed for\nmining such navigation patterns. The key insight is that users access\ninformation of interest and follow a certain path while navigating a web site.\nIf they don't find it, they would backtrack and choose among the alternate\npaths till they reach the destination. The point they backtrack is the\nIntermediate Reference Location. Identifying such Intermediate locations and\ndestinations out of the pattern will be the main endeavor in the rest of this\nreport.\n","vector":null,"chunk_id":"c149fc075e942c1d85949f137ed86b08"}
{"title":"The Future of Scientific Simulations: from Artificial Life to Artificial\n  Cosmogenesis","authors":"Clement Vidal","category":"cs.AI","abstract":"  This philosophical paper explores the relation between modern scientific\nsimulations and the future of the universe. We argue that a simulation of an\nentire universe will result from future scientific activity. This requires us\nto tackle the challenge of simulating open-ended evolution at all levels in a\nsingle simulation. The simulation should encompass not only biological\nevolution, but also physical evolution (a level below) and cultural evolution\n(a level above). The simulation would allow us to probe what would happen if we\nwould \"replay the tape of the universe\" with the same or different laws and\ninitial conditions. We also distinguish between real-world and artificial-world\nmodelling. Assuming that intelligent life could indeed simulate an entire\nuniverse, this leads to two tentative hypotheses. Some authors have argued that\nwe may already be in a simulation run by an intelligent entity. Or, if such a\nsimulation could be made real, this would lead to the production of a new\nuniverse. This last direction is argued with a careful speculative\nphilosophical approach, emphasizing the imperative to find a solution to the\nheat death problem in cosmology. The reader is invited to consult Annex 1 for\nan overview of the logical structure of this paper. -- Keywords: far future,\nfuture of science, ALife, simulation, realization, cosmology, heat death,\nfine-tuning, physical eschatology, cosmological natural selection, cosmological\nartificial selection, artificial cosmogenesis, selfish biocosm hypothesis,\nmeduso-anthropic principle, developmental singularity hypothesis, role of\nintelligent life.\n","text":"Title:The Future of Scientific Simulations: from Artificial Life to Artificial\n  Cosmogenesis\nAbstract:  This philosophical paper explores the relation between modern scientific\nsimulations and the future of the universe. We argue that a simulation of an\nentire universe will result from future scientific activity. This requires us\nto tackle the challenge of simulating open-ended evolution at all levels in a\nsingle simulation. The simulation should encompass not only biological\nevolution, but also physical evolution (a level below) and cultural evolution\n(a level above). The simulation would allow us to probe what would happen if we\nwould \"replay the tape of the universe\" with the same or different laws and\ninitial conditions. We also distinguish between real-world and artificial-world\nmodelling. Assuming that intelligent life could indeed simulate an entire\nuniverse, this leads to two tentative hypotheses. Some authors have argued that\nwe may already be in a simulation run by an intelligent entity. Or, if such a\nsimulation could be made real, this would lead to the production of a new\nuniverse. This last direction is argued with a careful speculative\nphilosophical approach, emphasizing the imperative to find a solution to the\nheat death problem in cosmology. The reader is invited to consult Annex 1 for\nan overview of the logical structure of this paper. -- Keywords: far future,\nfuture of science, ALife, simulation, realization, cosmology, heat death,\nfine-tuning, physical eschatology, cosmological natural selection, cosmological\nartificial selection, artificial cosmogenesis, selfish biocosm hypothesis,\nmeduso-anthropic principle, developmental singularity hypothesis, role of\nintelligent life.\n","vector":null,"chunk_id":"b0e19906bb4dc532e2e6e2e99d4450b2"}
{"title":"Serious Flaws in Korf et al.'s Analysis on Time Complexity of A*","authors":"Hang Dinh","category":"cs.AI","abstract":"  This paper has been withdrawn.\n","text":"Title:Serious Flaws in Korf et al.'s Analysis on Time Complexity of A*\nAbstract:  This paper has been withdrawn.\n","vector":null,"chunk_id":"9809790aea89d73100e8b666441eb2f7"}
{"title":"Component models for large networks","authors":"Janne Sinkkonen, Janne Aukia and Samuel Kaski","category":"stat.ML","abstract":"  Being among the easiest ways to find meaningful structure from discrete data,\nLatent Dirichlet Allocation (LDA) and related component models have been\napplied widely. They are simple, computationally fast and scalable,\ninterpretable, and admit nonparametric priors. In the currently popular field\nof network modeling, relatively little work has taken uncertainty of data\nseriously in the Bayesian sense, and component models have been introduced to\nthe field only recently, by treating each node as a bag of out-going links. We\nintroduce an alternative, interaction component model for communities (ICMc),\nwhere the whole network is a bag of links, stemming from different components.\nThe former finds both disassortative and assortative structure, while the\nalternative assumes assortativity and finds community-like structures like the\nearlier methods motivated by physics. With Dirichlet Process priors and an\nefficient implementation the models are highly scalable, as demonstrated with a\nsocial network from the Last.fm web site, with 670,000 nodes and 1.89 million\nlinks.\n","text":"Title:Component models for large networks\nAbstract:  Being among the easiest ways to find meaningful structure from discrete data,\nLatent Dirichlet Allocation (LDA) and related component models have been\napplied widely. They are simple, computationally fast and scalable,\ninterpretable, and admit nonparametric priors. In the currently popular field\nof network modeling, relatively little work has taken uncertainty of data\nseriously in the Bayesian sense, and component models have been introduced to\nthe field only recently, by treating each node as a bag of out-going links. We\nintroduce an alternative, interaction component model for communities (ICMc),\nwhere the whole network is a bag of links, stemming from different components.\nThe former finds both disassortative and assortative structure, while the\nalternative assumes assortativity and finds community-like structures like the\nearlier methods motivated by physics. With Dirichlet Process priors and an\nefficient implementation the models are highly scalable, as demonstrated with a\nsocial network from the Last.fm web site, with 670,000 nodes and 1.89 million\nlinks.\n","vector":null,"chunk_id":"167a4e62b44ed3c99016e444f5a27634"}
{"title":"The Anatomy of Mitos Web Search Engine","authors":"Panagiotis Papadakos, Giorgos Vasiliadis, Yannis Theoharis, Nikos\n  Armenatzoglou, Stella Kopidaki, Yannis Marketakis, Manos Daskalakis, Kostas\n  Karamaroudis, Giorgos Linardakis, Giannis Makrydakis, Vangelis Papathanasiou,\n  Lefteris Sardis, Petros Tsialiamanis, Georgia Troullinou, Kostas Vandikas,\n  Dimitris Velegrakis and Yannis Tzitzikas","category":"cs.IR","abstract":"  Engineering a Web search engine offering effective and efficient information\nretrieval is a challenging task. This document presents our experiences from\ndesigning and developing a Web search engine offering a wide spectrum of\nfunctionalities and we report some interesting experimental results. A rather\npeculiar design choice of the engine is that its index is based on a DBMS,\nwhile some of the distinctive functionalities that are offered include advanced\nGreek language stemming, real time result clustering, and advanced link\nanalysis techniques (also for spam page detection).\n","text":"Title:The Anatomy of Mitos Web Search Engine\nAbstract:  Engineering a Web search engine offering effective and efficient information\nretrieval is a challenging task. This document presents our experiences from\ndesigning and developing a Web search engine offering a wide spectrum of\nfunctionalities and we report some interesting experimental results. A rather\npeculiar design choice of the engine is that its index is based on a DBMS,\nwhile some of the distinctive functionalities that are offered include advanced\nGreek language stemming, real time result clustering, and advanced link\nanalysis techniques (also for spam page detection).\n","vector":null,"chunk_id":"109ec69fe1daa7e3a04e15a1066e1de5"}
{"title":"Eye-Tracking Evolutionary Algorithm to minimize user's fatigue in IEC\n  applied to Interactive One-Max problem","authors":"Denis Pallez (LIRIS), Philippe Collard (I3S), Thierry Baccino (LPEQ),\n  Laurent Dumercy (LPEQ)","category":"cs.AI","abstract":"  In this paper, we describe a new algorithm that consists in combining an\neye-tracker for minimizing the fatigue of a user during the evaluation process\nof Interactive Evolutionary Computation. The approach is then applied to the\nInteractive One-Max optimization problem.\n","text":"Title:Eye-Tracking Evolutionary Algorithm to minimize user's fatigue in IEC\n  applied to Interactive One-Max problem\nAbstract:  In this paper, we describe a new algorithm that consists in combining an\neye-tracker for minimizing the fatigue of a user during the evaluation process\nof Interactive Evolutionary Computation. The approach is then applied to the\nInteractive One-Max optimization problem.\n","vector":null,"chunk_id":"81cda2db97287649313c6f3b330f25e0"}
{"title":"Node discovery in a networked organization","authors":"Yoshiharu Maeno","category":"cs.AI","abstract":"  In this paper, I present a method to solve a node discovery problem in a\nnetworked organization. Covert nodes refer to the nodes which are not\nobservable directly. They affect social interactions, but do not appear in the\nsurveillance logs which record the participants of the social interactions.\nDiscovering the covert nodes is defined as identifying the suspicious logs\nwhere the covert nodes would appear if the covert nodes became overt. A\nmathematical model is developed for the maximal likelihood estimation of the\nnetwork behind the social interactions and for the identification of the\nsuspicious logs. Precision, recall, and F measure characteristics are\ndemonstrated with the dataset generated from a real organization and the\ncomputationally synthesized datasets. The performance is close to the\ntheoretical limit for any covert nodes in the networks of any topologies and\nsizes if the ratio of the number of observation to the number of possible\ncommunication patterns is large.\n","text":"Title:Node discovery in a networked organization\nAbstract:  In this paper, I present a method to solve a node discovery problem in a\nnetworked organization. Covert nodes refer to the nodes which are not\nobservable directly. They affect social interactions, but do not appear in the\nsurveillance logs which record the participants of the social interactions.\nDiscovering the covert nodes is defined as identifying the suspicious logs\nwhere the covert nodes would appear if the covert nodes became overt. A\nmathematical model is developed for the maximal likelihood estimation of the\nnetwork behind the social interactions and for the identification of the\nsuspicious logs. Precision, recall, and F measure characteristics are\ndemonstrated with the dataset generated from a real organization and the\ncomputationally synthesized datasets. The performance is close to the\ntheoretical limit for any covert nodes in the networks of any topologies and\nsizes if the ratio of the number of observation to the number of possible\ncommunication patterns is large.\n","vector":null,"chunk_id":"5964ce0733c358388b078ec533259cae"}
{"title":"Multiagent Approach for the Representation of Information in a Decision\n  Support System","authors":"Fahem Kebair (LITIS), Fr\\'ed\\'eric Serin (LITIS)","category":"cs.AI","abstract":"  In an emergency situation, the actors need an assistance allowing them to\nreact swiftly and efficiently. In this prospect, we present in this paper a\ndecision support system that aims to prepare actors in a crisis situation\nthanks to a decision-making support. The global architecture of this system is\npresented in the first part. Then we focus on a part of this system which is\ndesigned to represent the information of the current situation. This part is\ncomposed of a multiagent system that is made of factual agents. Each agent\ncarries a semantic feature and aims to represent a partial part of a situation.\nThe agents develop thanks to their interactions by comparing their semantic\nfeatures using proximity measures and according to specific ontologies.\n","text":"Title:Multiagent Approach for the Representation of Information in a Decision\n  Support System\nAbstract:  In an emergency situation, the actors need an assistance allowing them to\nreact swiftly and efficiently. In this prospect, we present in this paper a\ndecision support system that aims to prepare actors in a crisis situation\nthanks to a decision-making support. The global architecture of this system is\npresented in the first part. Then we focus on a part of this system which is\ndesigned to represent the information of the current situation. This part is\ncomposed of a multiagent system that is made of factual agents. Each agent\ncarries a semantic feature and aims to represent a partial part of a situation.\nThe agents develop thanks to their interactions by comparing their semantic\nfeatures using proximity measures and according to specific ontologies.\n","vector":null,"chunk_id":"01b5b71c12c8ecca632c4e209cababa1"}
{"title":"Reflective visualization and verbalization of unconscious preference","authors":"Yoshiharu Maeno and Yukio Ohsawa","category":"cs.AI","abstract":"  A new method is presented, that can help a person become aware of his or her\nunconscious preferences, and convey them to others in the form of verbal\nexplanation. The method combines the concepts of reflection, visualization, and\nverbalization. The method was tested in an experiment where the unconscious\npreferences of the subjects for various artworks were investigated. In the\nexperiment, two lessons were learned. The first is that it helps the subjects\nbecome aware of their unconscious preferences to verbalize weak preferences as\ncompared with strong preferences through discussion over preference diagrams.\nThe second is that it is effective to introduce an adjustable factor into\nvisualization to adapt to the differences in the subjects and to foster their\nmutual understanding.\n","text":"Title:Reflective visualization and verbalization of unconscious preference\nAbstract:  A new method is presented, that can help a person become aware of his or her\nunconscious preferences, and convey them to others in the form of verbal\nexplanation. The method combines the concepts of reflection, visualization, and\nverbalization. The method was tested in an experiment where the unconscious\npreferences of the subjects for various artworks were investigated. In the\nexperiment, two lessons were learned. The first is that it helps the subjects\nbecome aware of their unconscious preferences to verbalize weak preferences as\ncompared with strong preferences through discussion over preference diagrams.\nThe second is that it is effective to introduce an adjustable factor into\nvisualization to adapt to the differences in the subjects and to foster their\nmutual understanding.\n","vector":null,"chunk_id":"e8e7cd523476d8af9ac8bbc6b3c2edf0"}
{"title":"Application of Rough Set Theory to Analysis of Hydrocyclone Operation","authors":"H.Owladeghaffari, M.Ejtemaei, M.Irannajad","category":"cs.AI","abstract":"  This paper describes application of rough set theory, on the analysis of\nhydrocyclone operation. In this manner, using Self Organizing Map (SOM) as\npreprocessing step, best crisp granules of data are obtained. Then, using a\ncombining of SOM and rough set theory (RST)-called SORST-, the dominant rules\non the information table, obtained from laboratory tests, are extracted. Based\non these rules, an approximate estimation on decision attribute is fulfilled.\nFinally, a brief comparison of this method with the SOM-NFIS system (briefly\nSONFIS) is highlighted.\n","text":"Title:Application of Rough Set Theory to Analysis of Hydrocyclone Operation\nAbstract:  This paper describes application of rough set theory, on the analysis of\nhydrocyclone operation. In this manner, using Self Organizing Map (SOM) as\npreprocessing step, best crisp granules of data are obtained. Then, using a\ncombining of SOM and rough set theory (RST)-called SORST-, the dominant rules\non the information table, obtained from laboratory tests, are extracted. Based\non these rules, an approximate estimation on decision attribute is fulfilled.\nFinally, a brief comparison of this method with the SOM-NFIS system (briefly\nSONFIS) is highlighted.\n","vector":null,"chunk_id":"f65e2d4f687f7c3250739af5625dfc41"}
{"title":"Agent-Based Perception of an Environment in an Emergency Situation","authors":"Fahem Kebair (LITIS), Fr\\'ed\\'eric Serin (LITIS), Cyrille Bertelle\n  (LITIS)","category":"cs.AI","abstract":"  We are interested in the problem of multiagent systems development for risk\ndetecting and emergency response in an uncertain and partially perceived\nenvironment. The evaluation of the current situation passes by three stages\ninside the multiagent system. In a first time, the situation is represented in\na dynamic way. The second step, consists to characterise the situation and\nfinally, it is compared with other similar known situations. In this paper, we\npresent an information modelling of an observed environment, that we have\napplied on the RoboCupRescue Simulation System. Information coming from the\nenvironment are formatted according to a taxonomy and using semantic features.\nThe latter are defined thanks to a fine ontology of the domain and are managed\nby factual agents that aim to represent dynamically the current situation.\n","text":"Title:Agent-Based Perception of an Environment in an Emergency Situation\nAbstract:  We are interested in the problem of multiagent systems development for risk\ndetecting and emergency response in an uncertain and partially perceived\nenvironment. The evaluation of the current situation passes by three stages\ninside the multiagent system. In a first time, the situation is represented in\na dynamic way. The second step, consists to characterise the situation and\nfinally, it is compared with other similar known situations. In this paper, we\npresent an information modelling of an observed environment, that we have\napplied on the RoboCupRescue Simulation System. Information coming from the\nenvironment are formatted according to a taxonomy and using semantic features.\nThe latter are defined thanks to a fine ontology of the domain and are managed\nby factual agents that aim to represent dynamically the current situation.\n","vector":null,"chunk_id":"fc31ce5f7f73c2be94461345854184b0"}
{"title":"On the Influence of Selection Operators on Performances in Cellular\n  Genetic Algorithms","authors":"David Simoncini (I3S), Philippe Collard (I3S), S\\'ebastien Verel\n  (I3S), Manuel Clergue (I3S)","category":"cs.AI","abstract":"  In this paper, we study the influence of the selective pressure on the\nperformance of cellular genetic algorithms. Cellular genetic algorithms are\ngenetic algorithms where the population is embedded on a toroidal grid. This\nstructure makes the propagation of the best so far individual slow down, and\nallows to keep in the population potentially good solutions. We present two\nselective pressure reducing strategies in order to slow down even more the best\nsolution propagation. We experiment these strategies on a hard optimization\nproblem, the quadratic assignment problem, and we show that there is a value\nfor of the control parameter for both which gives the best performance. This\noptimal value does not find explanation on only the selective pressure,\nmeasured either by take over time and diversity evolution. This study makes us\nconclude that we need other tools than the sole selective pressure measures to\nexplain the performances of cellular genetic algorithms.\n","text":"Title:On the Influence of Selection Operators on Performances in Cellular\n  Genetic Algorithms\nAbstract:  In this paper, we study the influence of the selective pressure on the\nperformance of cellular genetic algorithms. Cellular genetic algorithms are\ngenetic algorithms where the population is embedded on a toroidal grid. This\nstructure makes the propagation of the best so far individual slow down, and\nallows to keep in the population potentially good solutions. We present two\nselective pressure reducing strategies in order to slow down even more the best\nsolution propagation. We experiment these strategies on a hard optimization\nproblem, the quadratic assignment problem, and we show that there is a value\nfor of the control parameter for both which gives the best performance. This\noptimal value does not find explanation on only the selective pressure,\nmeasured either by take over time and diversity evolution. This study makes us\nconclude that we need other tools than the sole selective pressure measures to\nexplain the performances of cellular genetic algorithms.\n","vector":null,"chunk_id":"b147abc9d2ed6a3205766e4b3f5eda7b"}
{"title":"Testing for Homogeneity with Kernel Fisher Discriminant Analysis","authors":"Zaid Harchaoui (LTCI), Francis Bach (INRIA Rocquencourt), Eric\n  Moulines (LTCI)","category":"stat.ML","abstract":"  We propose to investigate test statistics for testing homogeneity in\nreproducing kernel Hilbert spaces. Asymptotic null distributions under null\nhypothesis are derived, and consistency against fixed and local alternatives is\nassessed. Finally, experimental evidence of the performance of the proposed\napproach on both artificial data and a speaker verification task is provided.\n","text":"Title:Testing for Homogeneity with Kernel Fisher Discriminant Analysis\nAbstract:  We propose to investigate test statistics for testing homogeneity in\nreproducing kernel Hilbert spaces. Asymptotic null distributions under null\nhypothesis are derived, and consistency against fixed and local alternatives is\nassessed. Finally, experimental evidence of the performance of the proposed\napproach on both artificial data and a speaker verification task is provided.\n","vector":null,"chunk_id":"d364d381c6866bcd115bfe9a8d988641"}
{"title":"Geometric Data Analysis, From Correspondence Analysis to Structured Data\n  Analysis (book review)","authors":"Fionn Murtagh","category":"cs.AI","abstract":"  Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From\nCorrespondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,\nxi+475 pp.\n","text":"Title:Geometric Data Analysis, From Correspondence Analysis to Structured Data\n  Analysis (book review)\nAbstract:  Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From\nCorrespondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,\nxi+475 pp.\n","vector":null,"chunk_id":"87128f28e28a4a68c22f35509529cd0c"}
{"title":"On the underestimation of model uncertainty by Bayesian K-nearest\n  neighbors","authors":"Wanhua Su, Hugh Chipman, Mu Zhu","category":"stat.ML","abstract":"  When using the K-nearest neighbors method, one often ignores uncertainty in\nthe choice of K. To account for such uncertainty, Holmes and Adams (2002)\nproposed a Bayesian framework for K-nearest neighbors (KNN). Their Bayesian KNN\n(BKNN) approach uses a pseudo-likelihood function, and standard Markov chain\nMonte Carlo (MCMC) techniques to draw posterior samples. Holmes and Adams\n(2002) focused on the performance of BKNN in terms of misclassification error\nbut did not assess its ability to quantify uncertainty. We present some\nevidence to show that BKNN still significantly underestimates model\nuncertainty.\n","text":"Title:On the underestimation of model uncertainty by Bayesian K-nearest\n  neighbors\nAbstract:  When using the K-nearest neighbors method, one often ignores uncertainty in\nthe choice of K. To account for such uncertainty, Holmes and Adams (2002)\nproposed a Bayesian framework for K-nearest neighbors (KNN). Their Bayesian KNN\n(BKNN) approach uses a pseudo-likelihood function, and standard Markov chain\nMonte Carlo (MCMC) techniques to draw posterior samples. Holmes and Adams\n(2002) focused on the performance of BKNN in terms of misclassification error\nbut did not assess its ability to quantify uncertainty. We present some\nevidence to show that BKNN still significantly underestimates model\nuncertainty.\n","vector":null,"chunk_id":"e43e09c2a4e609a3cf072588f43db45d"}
{"title":"Comparing and Combining Methods for Automatic Query Expansion","authors":"Jos\\'e R. P\\'erez-Ag\\\"uera and Lourdes Araujo","category":"cs.IR","abstract":"  Query expansion is a well known method to improve the performance of\ninformation retrieval systems. In this work we have tested different approaches\nto extract the candidate query terms from the top ranked documents returned by\nthe first-pass retrieval.\n  One of them is the cooccurrence approach, based on measures of cooccurrence\nof the candidate and the query terms in the retrieved documents. The other one,\nthe probabilistic approach, is based on the probability distribution of terms\nin the collection and in the top ranked set.\n  We compare the retrieval improvement achieved by expanding the query with\nterms obtained with different methods belonging to both approaches. Besides, we\nhave developed a na\\\"ive combination of both kinds of method, with which we\nhave obtained results that improve those obtained with any of them separately.\nThis result confirms that the information provided by each approach is of a\ndifferent nature and, therefore, can be used in a combined manner.\n","text":"Title:Comparing and Combining Methods for Automatic Query Expansion\nAbstract:  Query expansion is a well known method to improve the performance of\ninformation retrieval systems. In this work we have tested different approaches\nto extract the candidate query terms from the top ranked documents returned by\nthe first-pass retrieval.\n  One of them is the cooccurrence approach, based on measures of cooccurrence\nof the candidate and the query terms in the retrieved documents. The other one,\nthe probabilistic approach, is based on the probability distribution of terms\nin the collection and in the top ranked set.\n  We compare the retrieval improvement achieved by expanding the query with\nterms obtained with different methods belonging to both approaches. Besides, we\nhave developed a na\\\"ive combination of both kinds of method, with which we\nhave obtained results that improve those obtained with any of them separately.\nThis result confirms that the information provided by each approach is of a\ndifferent nature and, therefore, can be used in a combined manner.\n","vector":null,"chunk_id":"91402098b24088f5b524d619d17b201c"}
{"title":"Information Preserving Component Analysis: Data Projections for Flow\n  Cytometry Analysis","authors":"Kevin M. Carter, Raviv Raich, William G. Finn, Alfred O. Hero III","category":"stat.ML","abstract":"  Flow cytometry is often used to characterize the malignant cells in leukemia\nand lymphoma patients, traced to the level of the individual cell. Typically,\nflow cytometric data analysis is performed through a series of 2-dimensional\nprojections onto the axes of the data set. Through the years, clinicians have\ndetermined combinations of different fluorescent markers which generate\nrelatively known expression patterns for specific subtypes of leukemia and\nlymphoma -- cancers of the hematopoietic system. By only viewing a series of\n2-dimensional projections, the high-dimensional nature of the data is rarely\nexploited. In this paper we present a means of determining a low-dimensional\nprojection which maintains the high-dimensional relationships (i.e.\ninformation) between differing oncological data sets. By using machine learning\ntechniques, we allow clinicians to visualize data in a low dimension defined by\na linear combination of all of the available markers, rather than just 2 at a\ntime. This provides an aid in diagnosing similar forms of cancer, as well as a\nmeans for variable selection in exploratory flow cytometric research. We refer\nto our method as Information Preserving Component Analysis (IPCA).\n","text":"Title:Information Preserving Component Analysis: Data Projections for Flow\n  Cytometry Analysis\nAbstract:  Flow cytometry is often used to characterize the malignant cells in leukemia\nand lymphoma patients, traced to the level of the individual cell. Typically,\nflow cytometric data analysis is performed through a series of 2-dimensional\nprojections onto the axes of the data set. Through the years, clinicians have\ndetermined combinations of different fluorescent markers which generate\nrelatively known expression patterns for specific subtypes of leukemia and\nlymphoma -- cancers of the hematopoietic system. By only viewing a series of\n2-dimensional projections, the high-dimensional nature of the data is rarely\nexploited. In this paper we present a means of determining a low-dimensional\nprojection which maintains the high-dimensional relationships (i.e.\ninformation) between differing oncological data sets. By using machine learning\ntechniques, we allow clinicians to visualize data in a low dimension defined by\na linear combination of all of the available markers, rather than just 2 at a\ntime. This provides an aid in diagnosing similar forms of cancer, as well as a\nmeans for variable selection in exploratory flow cytometric research. We refer\nto our method as Information Preserving Component Analysis (IPCA).\n","vector":null,"chunk_id":"37810547755155cc628983ccee65b9d6"}
{"title":"Phase transition in SONFIS&SORST","authors":"Hamed Owladeghaffari","category":"cs.AI","abstract":"  In this study, we introduce general frame of MAny Connected Intelligent\nParticles Systems (MACIPS). Connections and interconnections between particles\nget a complex behavior of such merely simple system (system in\nsystem).Contribution of natural computing, under information granulation\ntheory, are the main topics of this spacious skeleton. Upon this clue, we\norganize two algorithms involved a few prominent intelligent computing and\napproximate reasoning methods: self organizing feature map (SOM), Neuro- Fuzzy\nInference System and Rough Set Theory (RST). Over this, we show how our\nalgorithms can be taken as a linkage of government-society interaction, where\ngovernment catches various fashions of behavior: solid (absolute) or flexible.\nSo, transition of such society, by changing of connectivity parameters (noise)\nfrom order to disorder is inferred. Add to this, one may find an indirect\nmapping among financial systems and eventual market fluctuations with MACIPS.\nKeywords: phase transition, SONFIS, SORST, many connected intelligent particles\nsystem, society-government interaction\n","text":"Title:Phase transition in SONFIS&SORST\nAbstract:  In this study, we introduce general frame of MAny Connected Intelligent\nParticles Systems (MACIPS). Connections and interconnections between particles\nget a complex behavior of such merely simple system (system in\nsystem).Contribution of natural computing, under information granulation\ntheory, are the main topics of this spacious skeleton. Upon this clue, we\norganize two algorithms involved a few prominent intelligent computing and\napproximate reasoning methods: self organizing feature map (SOM), Neuro- Fuzzy\nInference System and Rough Set Theory (RST). Over this, we show how our\nalgorithms can be taken as a linkage of government-society interaction, where\ngovernment catches various fashions of behavior: solid (absolute) or flexible.\nSo, transition of such society, by changing of connectivity parameters (noise)\nfrom order to disorder is inferred. Add to this, one may find an indirect\nmapping among financial systems and eventual market fluctuations with MACIPS.\nKeywords: phase transition, SONFIS, SORST, many connected intelligent particles\nsystem, society-government interaction\n","vector":null,"chunk_id":"1420b5da986f5ea0bbfedb2076435098"}
{"title":"Adaptive Affinity Propagation Clustering","authors":"Kaijun Wang, Junying Zhang, Dan Li, Xinna Zhang and Tao Guo","category":"cs.AI","abstract":"  Affinity propagation clustering (AP) has two limitations: it is hard to know\nwhat value of parameter 'preference' can yield an optimal clustering solution,\nand oscillations cannot be eliminated automatically if occur. The adaptive AP\nmethod is proposed to overcome these limitations, including adaptive scanning\nof preferences to search space of the number of clusters for finding the\noptimal clustering solution, adaptive adjustment of damping factors to\neliminate oscillations, and adaptive escaping from oscillations when the\ndamping adjustment technique fails. Experimental results on simulated and real\ndata sets show that the adaptive AP is effective and can outperform AP in\nquality of clustering results.\n","text":"Title:Adaptive Affinity Propagation Clustering\nAbstract:  Affinity propagation clustering (AP) has two limitations: it is hard to know\nwhat value of parameter 'preference' can yield an optimal clustering solution,\nand oscillations cannot be eliminated automatically if occur. The adaptive AP\nmethod is proposed to overcome these limitations, including adaptive scanning\nof preferences to search space of the number of clusters for finding the\noptimal clustering solution, adaptive adjustment of damping factors to\neliminate oscillations, and adaptive escaping from oscillations when the\ndamping adjustment technique fails. Experimental results on simulated and real\ndata sets show that the adaptive AP is effective and can outperform AP in\nquality of clustering results.\n","vector":null,"chunk_id":"eab78a982b0d78e2cd55c9536fa9a3b2"}
{"title":"Assessment of effective parameters on dilution using approximate\n  reasoning methods in longwall mining method, Iran coal mines","authors":"H. Owladeghaffari, K. Shahriar, G. H. R. Saeedi","category":"cs.AI","abstract":"  Approximately more than 90% of all coal production in Iranian underground\nmines is derived directly longwall mining method. Out of seam dilution is one\nof the essential problems in these mines. Therefore the dilution can impose the\nadditional cost of mining and milling. As a result, recognition of the\neffective parameters on the dilution has a remarkable role in industry. In this\nway, this paper has analyzed the influence of 13 parameters (attributed\nvariables) versus the decision attribute (dilution value), so that using two\napproximate reasoning methods, namely Rough Set Theory (RST) and Self\nOrganizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our\ncollected data sets has been extracted. The other benefit of later methods is\nto predict new unknown cases. So, the reduced sets (reducts) by RST have been\nobtained. Therefore the emerged results by utilizing mentioned methods shows\nthat the high sensitive variables are thickness of layer, length of stope, rate\nof advance, number of miners, type of advancing.\n","text":"Title:Assessment of effective parameters on dilution using approximate\n  reasoning methods in longwall mining method, Iran coal mines\nAbstract:  Approximately more than 90% of all coal production in Iranian underground\nmines is derived directly longwall mining method. Out of seam dilution is one\nof the essential problems in these mines. Therefore the dilution can impose the\nadditional cost of mining and milling. As a result, recognition of the\neffective parameters on the dilution has a remarkable role in industry. In this\nway, this paper has analyzed the influence of 13 parameters (attributed\nvariables) versus the decision attribute (dilution value), so that using two\napproximate reasoning methods, namely Rough Set Theory (RST) and Self\nOrganizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our\ncollected data sets has been extracted. The other benefit of later methods is\nto predict new unknown cases. So, the reduced sets (reducts) by RST have been\nobtained. Therefore the emerged results by utilizing mentioned methods shows\nthat the high sensitive variables are thickness of layer, length of stope, rate\nof advance, number of miners, type of advancing.\n","vector":null,"chunk_id":"4dd814538cd868886dcd779febbc09ea"}
{"title":"Random projection trees for vector quantization","authors":"Sanjoy Dasgupta and Yoav Freund","category":"stat.ML","abstract":"  A simple and computationally efficient scheme for tree-structured vector\nquantization is presented. Unlike previous methods, its quantization error\ndepends only on the intrinsic dimension of the data distribution, rather than\nthe apparent dimension of the space in which the data happen to lie.\n","text":"Title:Random projection trees for vector quantization\nAbstract:  A simple and computationally efficient scheme for tree-structured vector\nquantization is presented. Unlike previous methods, its quantization error\ndepends only on the intrinsic dimension of the data distribution, rather than\nthe apparent dimension of the space in which the data happen to lie.\n","vector":null,"chunk_id":"1b3310156e1c122900339f366d5a7000"}
{"title":"Toward Fuzzy block theory","authors":"H.Owladeghaffari","category":"cs.AI","abstract":"  This study, fundamentals of fuzzy block theory, and its application in\nassessment of stability in underground openings, has surveyed. Using fuzzy\ntopics and inserting them in to key block theory, in two ways, fundamentals of\nfuzzy block theory has been presented. In indirect combining, by coupling of\nadaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could\nextract possible damage parts around a tunnel. In direct solution, some\nprinciples of block theory, by means of different fuzzy facets theory, were\nrewritten.\n","text":"Title:Toward Fuzzy block theory\nAbstract:  This study, fundamentals of fuzzy block theory, and its application in\nassessment of stability in underground openings, has surveyed. Using fuzzy\ntopics and inserting them in to key block theory, in two ways, fundamentals of\nfuzzy block theory has been presented. In indirect combining, by coupling of\nadaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could\nextract possible damage parts around a tunnel. In direct solution, some\nprinciples of block theory, by means of different fuzzy facets theory, were\nrewritten.\n","vector":null,"chunk_id":"9296b0848fc4f18c362cb738ae2f2c61"}
{"title":"Analysis of hydrocyclone performance based on information granulation\n  theory","authors":"Hamed Owladeghaffari, Majid Ejtemaei, Mehdi Irannajad","category":"cs.AI","abstract":"  This paper describes application of information granulation theory, on the\nanalysis of hydrocyclone perforamance. In this manner, using a combining of\nSelf Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and\nfuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules\nand sub fuzzy granules, within non fuzzy information (initial granulation), is\nrendered in an open-close iteration. Using two criteria, \"simplicity of rules\n\"and \"adaptive threoshold error level\", stability of algorithm is guaranteed.\nValidation of the proposed method, on the data set of the hydrocyclone is\nrendered.\n","text":"Title:Analysis of hydrocyclone performance based on information granulation\n  theory\nAbstract:  This paper describes application of information granulation theory, on the\nanalysis of hydrocyclone perforamance. In this manner, using a combining of\nSelf Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and\nfuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules\nand sub fuzzy granules, within non fuzzy information (initial granulation), is\nrendered in an open-close iteration. Using two criteria, \"simplicity of rules\n\"and \"adaptive threoshold error level\", stability of algorithm is guaranteed.\nValidation of the proposed method, on the data set of the hydrocyclone is\nrendered.\n","vector":null,"chunk_id":"a5b586ca57ebc4521431f247af744d7e"}
{"title":"Logic programming with social features","authors":"Francesco Buccafurri and Gianluca Caminiti","category":"cs.AI","abstract":"  In everyday life it happens that a person has to reason about what other\npeople think and how they behave, in order to achieve his goals. In other\nwords, an individual may be required to adapt his behaviour by reasoning about\nthe others' mental state. In this paper we focus on a knowledge representation\nlanguage derived from logic programming which both supports the representation\nof mental states of individual communities and provides each with the\ncapability of reasoning about others' mental states and acting accordingly. The\nproposed semantics is shown to be translatable into stable model semantics of\nlogic programs with aggregates.\n","text":"Title:Logic programming with social features\nAbstract:  In everyday life it happens that a person has to reason about what other\npeople think and how they behave, in order to achieve his goals. In other\nwords, an individual may be required to adapt his behaviour by reasoning about\nthe others' mental state. In this paper we focus on a knowledge representation\nlanguage derived from logic programming which both supports the representation\nof mental states of individual communities and provides each with the\ncapability of reasoning about others' mental states and acting accordingly. The\nproposed semantics is shown to be translatable into stable model semantics of\nlogic programs with aggregates.\n","vector":null,"chunk_id":"48ee5f821b678f997f8dc0a1f9203cb4"}
{"title":"Constructing Folksonomies from User-specified Relations on Flickr","authors":"Anon Plangprasopchok and Kristina Lerman","category":"cs.AI","abstract":"  Many social Web sites allow users to publish content and annotate with\ndescriptive metadata. In addition to flat tags, some social Web sites have\nrecently began to allow users to organize their content and metadata\nhierarchically. The social photosharing site Flickr, for example, allows users\nto group related photos in sets, and related sets in collections. The social\nbookmarking site Del.icio.us similarly lets users group related tags into\nbundles. Although the sites themselves don't impose any constraints on how\nthese hierarchies are used, individuals generally use them to capture\nrelationships between concepts, most commonly the broader/narrower relations.\nCollective annotation of content with hierarchical relations may lead to an\nemergent classification system, called a folksonomy. While some researchers\nhave explored using tags as evidence for learning folksonomies, we believe that\nhierarchical relations described above offer a high-quality source of evidence\nfor this task.\n  We propose a simple approach to aggregate shallow hierarchies created by many\ndistinct Flickr users into a common folksonomy. Our approach uses statistics to\ndetermine if a particular relation should be retained or discarded. The\nrelations are then woven together into larger hierarchies. Although we have not\ncarried out a detailed quantitative evaluation of the approach, it looks very\npromising since it generates very reasonable, non-trivial hierarchies.\n","text":"Title:Constructing Folksonomies from User-specified Relations on Flickr\nAbstract:  Many social Web sites allow users to publish content and annotate with\ndescriptive metadata. In addition to flat tags, some social Web sites have\nrecently began to allow users to organize their content and metadata\nhierarchically. The social photosharing site Flickr, for example, allows users\nto group related photos in sets, and related sets in collections. The social\nbookmarking site Del.icio.us similarly lets users group related tags into\nbundles. Although the sites themselves don't impose any constraints on how\nthese hierarchies are used, individuals generally use them to capture\nrelationships between concepts, most commonly the broader/narrower relations.\nCollective annotation of content with hierarchical relations may lead to an\nemergent classification system, called a folksonomy. While some researchers\nhave explored using tags as evidence for learning folksonomies, we believe that\nhierarchical relations described above offer a high-quality source of evidence\nfor this task.\n  We propose a simple approach to aggregate shallow hierarchies created by many\ndistinct Flickr users into a common folksonomy. Our approach uses statistics to\ndetermine if a particular relation should be retained or discarded. The\nrelations are then woven together into larger hierarchies. Although we have not\ncarried out a detailed quantitative evaluation of the approach, it looks very\npromising since it generates very reasonable, non-trivial hierarchies.\n","vector":null,"chunk_id":"f0a3cb7ed04c4b7d2208eb5ceb6a5621"}
{"title":"The Structure of Narrative: the Case of Film Scripts","authors":"Fionn Murtagh, Adam Ganz and Stewart McKie","category":"cs.AI","abstract":"  We analyze the style and structure of story narrative using the case of film\nscripts. The practical importance of this is noted, especially the need to have\nsupport tools for television movie writing. We use the Casablanca film script,\nand scripts from six episodes of CSI (Crime Scene Investigation). For analysis\nof style and structure, we quantify various central perspectives discussed in\nMcKee's book, \"Story: Substance, Structure, Style, and the Principles of\nScreenwriting\". Film scripts offer a useful point of departure for exploration\nof the analysis of more general narratives. Our methodology, using\nCorrespondence Analysis, and hierarchical clustering, is innovative in a range\nof areas that we discuss. In particular this work is groundbreaking in taking\nthe qualitative analysis of McKee and grounding this analysis in a quantitative\nand algorithmic framework.\n","text":"Title:The Structure of Narrative: the Case of Film Scripts\nAbstract:  We analyze the style and structure of story narrative using the case of film\nscripts. The practical importance of this is noted, especially the need to have\nsupport tools for television movie writing. We use the Casablanca film script,\nand scripts from six episodes of CSI (Crime Scene Investigation). For analysis\nof style and structure, we quantify various central perspectives discussed in\nMcKee's book, \"Story: Substance, Structure, Style, and the Principles of\nScreenwriting\". Film scripts offer a useful point of departure for exploration\nof the analysis of more general narratives. Our methodology, using\nCorrespondence Analysis, and hierarchical clustering, is innovative in a range\nof areas that we discuss. In particular this work is groundbreaking in taking\nthe qualitative analysis of McKee and grounding this analysis in a quantitative\nand algorithmic framework.\n","vector":null,"chunk_id":"a310661c7ee08f9aff00500297092b4f"}
{"title":"Feature Selection for Bayesian Evaluation of Trauma Death Risk","authors":"L. Jakaite and V. Schetinin","category":"cs.AI","abstract":"  In the last year more than 70,000 people have been brought to the UK\nhospitals with serious injuries. Each time a clinician has to urgently take a\npatient through a screening procedure to make a reliable decision on the trauma\ntreatment. Typically, such procedure comprises around 20 tests; however the\ncondition of a trauma patient remains very difficult to be tested properly.\nWhat happens if these tests are ambiguously interpreted, and information about\nthe severity of the injury will come misleading? The mistake in a decision can\nbe fatal: using a mild treatment can put a patient at risk of dying from\nposttraumatic shock, while using an overtreatment can also cause death. How can\nwe reduce the risk of the death caused by unreliable decisions? It has been\nshown that probabilistic reasoning, based on the Bayesian methodology of\naveraging over decision models, allows clinicians to evaluate the uncertainty\nin decision making. Based on this methodology, in this paper we aim at\nselecting the most important screening tests, keeping a high performance. We\nassume that the probabilistic reasoning within the Bayesian methodology allows\nus to discover new relationships between the screening tests and uncertainty in\ndecisions. In practice, selection of the most informative tests can also reduce\nthe cost of a screening procedure in trauma care centers. In our experiments we\nuse the UK Trauma data to compare the efficiency of the proposed technique in\nterms of the performance. We also compare the uncertainty in decisions in terms\nof entropy.\n","text":"Title:Feature Selection for Bayesian Evaluation of Trauma Death Risk\nAbstract:  In the last year more than 70,000 people have been brought to the UK\nhospitals with serious injuries. Each time a clinician has to urgently take a\npatient through a screening procedure to make a reliable decision on the trauma\ntreatment. Typically, such procedure comprises around 20 tests; however the\ncondition of a trauma patient remains very difficult to be tested properly.\nWhat happens if these tests are ambiguously interpreted, and information about\nthe severity of the injury will come misleading? The mistake in a decision can\nbe fatal: using a mild treatment can put a patient at risk of dying from\nposttraumatic shock, while using an overtreatment can also cause death. How can\nwe reduce the risk of the death caused by unreliable decisions? It has been\nshown that probabilistic reasoning, based on the Bayesian methodology of\naveraging over decision models, allows clinicians to evaluate the uncertainty\nin decision making. Based on this methodology, in this paper we aim at\nselecting the most important screening tests, keeping a high performance. We\nassume that the probabilistic reasoning within the Bayesian methodology allows\nus to discover new relationships between the screening tests and uncertainty in\ndecisions. In practice, selection of the most informative tests can also reduce\nthe cost of a screening procedure in trauma care centers. In our experiments we\nuse the UK Trauma data to compare the efficiency of the proposed technique in\nterms of the performance. We also compare the uncertainty in decisions in terms\nof entropy.\n","vector":null,"chunk_id":"32ff2e8a1312b259b4d871cca2c0b5c8"}
{"title":"Fusion for Evaluation of Image Classification in Uncertain Environments","authors":"Arnaud Martin (E3I2)","category":"cs.AI","abstract":"  We present in this article a new evaluation method for classification and\nsegmentation of textured images in uncertain environments. In uncertain\nenvironments, real classes and boundaries are known with only a partial\ncertainty given by the experts. Most of the time, in many presented papers,\nonly classification or only segmentation are considered and evaluated. Here, we\npropose to take into account both the classification and segmentation results\naccording to the certainty given by the experts. We present the results of this\nmethod on a fusion of classifiers of sonar images for a seabed\ncharacterization.\n","text":"Title:Fusion for Evaluation of Image Classification in Uncertain Environments\nAbstract:  We present in this article a new evaluation method for classification and\nsegmentation of textured images in uncertain environments. In uncertain\nenvironments, real classes and boundaries are known with only a partial\ncertainty given by the experts. Most of the time, in many presented papers,\nonly classification or only segmentation are considered and evaluated. Here, we\npropose to take into account both the classification and segmentation results\naccording to the certainty given by the experts. We present the results of this\nmethod on a fusion of classifiers of sonar images for a seabed\ncharacterization.\n","vector":null,"chunk_id":"b992d1a34ced6e5b8bdcc8e7cc6478dc"}
{"title":"Intuitive visualization of the intelligence for the run-down of\n  terrorist wire-pullers","authors":"Yoshiharu Maeno, and Yukio Ohsawa","category":"cs.AI","abstract":"  The investigation of the terrorist attack is a time-critical task. The\ninvestigators have a limited time window to diagnose the organizational\nbackground of the terrorists, to run down and arrest the wire-pullers, and to\ntake an action to prevent or eradicate the terrorist attack. The intuitive\ninterface to visualize the intelligence data set stimulates the investigators'\nexperience and knowledge, and aids them in decision-making for an immediately\neffective action. This paper presents a computational method to analyze the\nintelligence data set on the collective actions of the perpetrators of the\nattack, and to visualize it into the form of a social network diagram which\npredicts the positions where the wire-pullers conceals themselves.\n","text":"Title:Intuitive visualization of the intelligence for the run-down of\n  terrorist wire-pullers\nAbstract:  The investigation of the terrorist attack is a time-critical task. The\ninvestigators have a limited time window to diagnose the organizational\nbackground of the terrorists, to run down and arrest the wire-pullers, and to\ntake an action to prevent or eradicate the terrorist attack. The intuitive\ninterface to visualize the intelligence data set stimulates the investigators'\nexperience and knowledge, and aids them in decision-making for an immediately\neffective action. This paper presents a computational method to analyze the\nintelligence data set on the collective actions of the perpetrators of the\nattack, and to visualize it into the form of a social network diagram which\npredicts the positions where the wire-pullers conceals themselves.\n","vector":null,"chunk_id":"175480ec37032b276e89bc1234e08bf3"}
{"title":"Rock mechanics modeling based on soft granulation theory","authors":"H.Owladeghaffari","category":"cs.AI","abstract":"  This paper describes application of information granulation theory, on the\ndesign of rock engineering flowcharts. Firstly, an overall flowchart, based on\ninformation granulation theory has been highlighted. Information granulation\ntheory, in crisp (non-fuzzy) or fuzzy format, can take into account engineering\nexperiences (especially in fuzzy shape-incomplete information or superfluous),\nor engineering judgments, in each step of designing procedure, while the\nsuitable instruments modeling are employed. In this manner and to extension of\nsoft modeling instruments, using three combinations of Self Organizing Map\n(SOM), Neuro-Fuzzy Inference System (NFIS), and Rough Set Theory (RST) crisp\nand fuzzy granules, from monitored data sets are obtained. The main underlined\ncore of our algorithms are balancing of crisp(rough or non-fuzzy) granules and\nsub fuzzy granules, within non fuzzy information (initial granulation) upon the\nopen-close iterations. Using different criteria on balancing best granules\n(information pockets), are obtained. Validations of our proposed methods, on\nthe data set of in-situ permeability in rock masses in Shivashan dam, Iran have\nbeen highlighted.\n","text":"Title:Rock mechanics modeling based on soft granulation theory\nAbstract:  This paper describes application of information granulation theory, on the\ndesign of rock engineering flowcharts. Firstly, an overall flowchart, based on\ninformation granulation theory has been highlighted. Information granulation\ntheory, in crisp (non-fuzzy) or fuzzy format, can take into account engineering\nexperiences (especially in fuzzy shape-incomplete information or superfluous),\nor engineering judgments, in each step of designing procedure, while the\nsuitable instruments modeling are employed. In this manner and to extension of\nsoft modeling instruments, using three combinations of Self Organizing Map\n(SOM), Neuro-Fuzzy Inference System (NFIS), and Rough Set Theory (RST) crisp\nand fuzzy granules, from monitored data sets are obtained. The main underlined\ncore of our algorithms are balancing of crisp(rough or non-fuzzy) granules and\nsub fuzzy granules, within non fuzzy information (initial granulation) upon the\nopen-close iterations. Using different criteria on balancing best granules\n(information pockets), are obtained. Validations of our proposed methods, on\nthe data set of in-situ permeability in rock masses in Shivashan dam, Iran have\nbeen highlighted.\n","vector":null,"chunk_id":"91a4099cb4762dca8949ac56a07e6e9a"}
{"title":"An Ontology-based Knowledge Management System for Industry Clusters","authors":"Pradorn Sureephong (LIESP, CAMT), Nopasit Chakpitak (CAMT), Yacine\n  Ouzrout (LIESP), Abdelaziz Bouras (LIESP)","category":"cs.AI","abstract":"  Knowledge-based economy forces companies in the nation to group together as a\ncluster in order to maintain their competitiveness in the world market. The\ncluster development relies on two key success factors which are knowledge\nsharing and collaboration between the actors in the cluster. Thus, our study\ntries to propose knowledge management system to support knowledge management\nactivities within the cluster. To achieve the objectives of this study,\nontology takes a very important role in knowledge management process in various\nways; such as building reusable and faster knowledge-bases, better way for\nrepresenting the knowledge explicitly. However, creating and representing\nontology create difficulties to organization due to the ambiguity and\nunstructured of source of knowledge. Therefore, the objectives of this paper\nare to propose the methodology to create and represent ontology for the\norganization development by using knowledge engineering approach. The\nhandicraft cluster in Thailand is used as a case study to illustrate our\nproposed methodology.\n","text":"Title:An Ontology-based Knowledge Management System for Industry Clusters\nAbstract:  Knowledge-based economy forces companies in the nation to group together as a\ncluster in order to maintain their competitiveness in the world market. The\ncluster development relies on two key success factors which are knowledge\nsharing and collaboration between the actors in the cluster. Thus, our study\ntries to propose knowledge management system to support knowledge management\nactivities within the cluster. To achieve the objectives of this study,\nontology takes a very important role in knowledge management process in various\nways; such as building reusable and faster knowledge-bases, better way for\nrepresenting the knowledge explicitly. However, creating and representing\nontology create difficulties to organization due to the ambiguity and\nunstructured of source of knowledge. Therefore, the objectives of this paper\nare to propose the methodology to create and represent ontology for the\norganization development by using knowledge engineering approach. The\nhandicraft cluster in Thailand is used as a case study to illustrate our\nproposed methodology.\n","vector":null,"chunk_id":"fae210ec7d91befc963dba683ecce397"}
{"title":"The Role of Artificial Intelligence Technologies in Crisis Response","authors":"Khaled M. Khalil, M. Abdel-Aziz, Taymour T. Nazmy and Abdel-Badeeh M.\n  Salem","category":"cs.AI","abstract":"  Crisis response poses many of the most difficult information technology in\ncrisis management. It requires information and communication-intensive efforts,\nutilized for reducing uncertainty, calculating and comparing costs and\nbenefits, and managing resources in a fashion beyond those regularly available\nto handle routine problems. In this paper, we explore the benefits of\nartificial intelligence technologies in crisis response. This paper discusses\nthe role of artificial intelligence technologies; namely, robotics, ontology\nand semantic web, and multi-agent systems in crisis response.\n","text":"Title:The Role of Artificial Intelligence Technologies in Crisis Response\nAbstract:  Crisis response poses many of the most difficult information technology in\ncrisis management. It requires information and communication-intensive efforts,\nutilized for reducing uncertainty, calculating and comparing costs and\nbenefits, and managing resources in a fashion beyond those regularly available\nto handle routine problems. In this paper, we explore the benefits of\nartificial intelligence technologies in crisis response. This paper discusses\nthe role of artificial intelligence technologies; namely, robotics, ontology\nand semantic web, and multi-agent systems in crisis response.\n","vector":null,"chunk_id":"9382f30fcc2c9ba7e2cd2703f7bf6d70"}
{"title":"Toward a combination rule to deal with partial conflict and specificity\n  in belief functions theory","authors":"Arnaud Martin (E3I2), Christophe Osswald (E3I2)","category":"cs.AI","abstract":"  We present and discuss a mixed conjunctive and disjunctive rule, a\ngeneralization of conflict repartition rules, and a combination of these two\nrules. In the belief functions theory one of the major problem is the conflict\nrepartition enlightened by the famous Zadeh's example. To date, many\ncombination rules have been proposed in order to solve a solution to this\nproblem. Moreover, it can be important to consider the specificity of the\nresponses of the experts. Since few year some unification rules are proposed.\nWe have shown in our previous works the interest of the proportional conflict\nredistribution rule. We propose here a mixed combination rule following the\nproportional conflict redistribution rule modified by a discounting procedure.\nThis rule generalizes many combination rules.\n","text":"Title:Toward a combination rule to deal with partial conflict and specificity\n  in belief functions theory\nAbstract:  We present and discuss a mixed conjunctive and disjunctive rule, a\ngeneralization of conflict repartition rules, and a combination of these two\nrules. In the belief functions theory one of the major problem is the conflict\nrepartition enlightened by the famous Zadeh's example. To date, many\ncombination rules have been proposed in order to solve a solution to this\nproblem. Moreover, it can be important to consider the specificity of the\nresponses of the experts. Since few year some unification rules are proposed.\nWe have shown in our previous works the interest of the proportional conflict\nredistribution rule. We propose here a mixed combination rule following the\nproportional conflict redistribution rule modified by a discounting procedure.\nThis rule generalizes many combination rules.\n","vector":null,"chunk_id":"efd7aa9528c558492bd95326e504084e"}
{"title":"A new generalization of the proportional conflict redistribution rule\n  stable in terms of decision","authors":"Arnaud Martin (E3I2), Christophe Osswald (E3I2)","category":"cs.AI","abstract":"  In this chapter, we present and discuss a new generalized proportional\nconflict redistribution rule. The Dezert-Smarandache extension of the\nDemster-Shafer theory has relaunched the studies on the combination rules\nespecially for the management of the conflict. Many combination rules have been\nproposed in the last few years. We study here different combination rules and\ncompare them in terms of decision on didactic example and on generated data.\nIndeed, in real applications, we need a reliable decision and it is the final\nresults that matter. This chapter shows that a fine proportional conflict\nredistribution rule must be preferred for the combination in the belief\nfunction theory.\n","text":"Title:A new generalization of the proportional conflict redistribution rule\n  stable in terms of decision\nAbstract:  In this chapter, we present and discuss a new generalized proportional\nconflict redistribution rule. The Dezert-Smarandache extension of the\nDemster-Shafer theory has relaunched the studies on the combination rules\nespecially for the management of the conflict. Many combination rules have been\nproposed in the last few years. We study here different combination rules and\ncompare them in terms of decision on didactic example and on generated data.\nIndeed, in real applications, we need a reliable decision and it is the final\nresults that matter. This chapter shows that a fine proportional conflict\nredistribution rule must be preferred for the combination in the belief\nfunction theory.\n","vector":null,"chunk_id":"95f43def0236362f0f903faa1a4e71b7"}
{"title":"Une nouvelle r\\`egle de combinaison r\\'epartissant le conflit -\n  Applications en imagerie Sonar et classification de cibles Radar","authors":"Arnaud Martin (E3I2), Christophe Osswald (E3I2)","category":"cs.AI","abstract":"  These last years, there were many studies on the problem of the conflict\ncoming from information combination, especially in evidence theory. We can\nsummarise the solutions for manage the conflict into three different\napproaches: first, we can try to suppress or reduce the conflict before the\ncombination step, secondly, we can manage the conflict in order to give no\ninfluence of the conflict in the combination step, and then take into account\nthe conflict in the decision step, thirdly, we can take into account the\nconflict in the combination step. The first approach is certainly the better,\nbut not always feasible. It is difficult to say which approach is the best\nbetween the second and the third. However, the most important is the produced\nresults in applications. We propose here a new combination rule that\ndistributes the conflict proportionally on the element given this conflict. We\ncompare these different combination rules on real data in Sonar imagery and\nRadar target classification.\n","text":"Title:Une nouvelle r\\`egle de combinaison r\\'epartissant le conflit -\n  Applications en imagerie Sonar et classification de cibles Radar\nAbstract:  These last years, there were many studies on the problem of the conflict\ncoming from information combination, especially in evidence theory. We can\nsummarise the solutions for manage the conflict into three different\napproaches: first, we can try to suppress or reduce the conflict before the\ncombination step, secondly, we can manage the conflict in order to give no\ninfluence of the conflict in the combination step, and then take into account\nthe conflict in the decision step, thirdly, we can take into account the\nconflict in the combination step. The first approach is certainly the better,\nbut not always feasible. It is difficult to say which approach is the best\nbetween the second and the third. However, the most important is the produced\nresults in applications. We propose here a new combination rule that\ndistributes the conflict proportionally on the element given this conflict. We\ncompare these different combination rules on real data in Sonar imagery and\nRadar target classification.\n","vector":null,"chunk_id":"92e8f27965bc6cff0364176faee1f5c2"}
{"title":"Perfect Derived Propagators","authors":"Christian Schulte and Guido Tack","category":"cs.AI","abstract":"  When implementing a propagator for a constraint, one must decide about\nvariants: When implementing min, should one also implement max? Should one\nimplement linear equations both with and without coefficients? Constraint\nvariants are ubiquitous: implementing them requires considerable (if not\nprohibitive) effort and decreases maintainability, but will deliver better\nperformance.\n  This paper shows how to use variable views, previously introduced for an\nimplementation architecture, to derive perfect propagator variants. A model for\nviews and derived propagators is introduced. Derived propagators are proved to\nbe indeed perfect in that they inherit essential properties such as correctness\nand domain and bounds consistency. Techniques for systematically deriving\npropagators such as transformation, generalization, specialization, and\nchanneling are developed for several variable domains. We evaluate the massive\nimpact of derived propagators. Without derived propagators, Gecode would\nrequire 140000 rather than 40000 lines of code for propagators.\n","text":"Title:Perfect Derived Propagators\nAbstract:  When implementing a propagator for a constraint, one must decide about\nvariants: When implementing min, should one also implement max? Should one\nimplement linear equations both with and without coefficients? Constraint\nvariants are ubiquitous: implementing them requires considerable (if not\nprohibitive) effort and decreases maintainability, but will deliver better\nperformance.\n  This paper shows how to use variable views, previously introduced for an\nimplementation architecture, to derive perfect propagator variants. A model for\nviews and derived propagators is introduced. Derived propagators are proved to\nbe indeed perfect in that they inherit essential properties such as correctness\nand domain and bounds consistency. Techniques for systematically deriving\npropagators such as transformation, generalization, specialization, and\nchanneling are developed for several variable domains. We evaluate the massive\nimpact of derived propagators. Without derived propagators, Gecode would\nrequire 140000 rather than 40000 lines of code for propagators.\n","vector":null,"chunk_id":"bc9ad21c086287cd949870a4144b6811"}
{"title":"Defaults and Normality in Causal Structures","authors":"Joseph Y. Halpern","category":"cs.AI","abstract":"  A serious defect with the Halpern-Pearl (HP) definition of causality is\nrepaired by combining a theory of causality with a theory of defaults. In\naddition, it is shown that (despite a claim to the contrary) a cause according\nto the HP condition need not be a single conjunct. A definition of causality\nmotivated by Wright's NESS test is shown to always hold for a single conjunct.\nMoreover, conditions that hold for all the examples considered by HP are given\nthat guarantee that causality according to (this version) of the NESS test is\nequivalent to the HP definition.\n","text":"Title:Defaults and Normality in Causal Structures\nAbstract:  A serious defect with the Halpern-Pearl (HP) definition of causality is\nrepaired by combining a theory of causality with a theory of defaults. In\naddition, it is shown that (despite a claim to the contrary) a cause according\nto the HP condition need not be a single conjunct. A definition of causality\nmotivated by Wright's NESS test is shown to always hold for a single conjunct.\nMoreover, conditions that hold for all the examples considered by HP are given\nthat guarantee that causality according to (this version) of the NESS test is\nequivalent to the HP definition.\n","vector":null,"chunk_id":"fc0dfd8d9a627daf12702bda37ce6df2"}
{"title":"Manifold Learning: The Price of Normalization","authors":"Y. Goldberg, A. Zakai, D. Kushnir, Y. Ritov","category":"stat.ML","abstract":"  We analyze the performance of a class of manifold-learning algorithms that\nfind their output by minimizing a quadratic form under some normalization\nconstraints. This class consists of Locally Linear Embedding (LLE), Laplacian\nEigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and\nDiffusion maps. We present and prove conditions on the manifold that are\nnecessary for the success of the algorithms. Both the finite sample case and\nthe limit case are analyzed. We show that there are simple manifolds in which\nthe necessary conditions are violated, and hence the algorithms cannot recover\nthe underlying manifolds. Finally, we present numerical results that\ndemonstrate our claims.\n","text":"Title:Manifold Learning: The Price of Normalization\nAbstract:  We analyze the performance of a class of manifold-learning algorithms that\nfind their output by minimizing a quadratic form under some normalization\nconstraints. This class consists of Locally Linear Embedding (LLE), Laplacian\nEigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and\nDiffusion maps. We present and prove conditions on the manifold that are\nnecessary for the success of the algorithms. Both the finite sample case and\nthe limit case are analyzed. We show that there are simple manifolds in which\nthe necessary conditions are violated, and hence the algorithms cannot recover\nthe underlying manifolds. Finally, we present numerical results that\ndemonstrate our claims.\n","vector":null,"chunk_id":"8bbb51e7a724f846bb41a3b2cfa643e2"}
{"title":"Local Procrustes for Manifold Embedding: A Measure of Embedding Quality\n  and Embedding Algorithms","authors":"Y. Goldberg, Y. Ritov","category":"stat.ML","abstract":"  We present the Procrustes measure, a novel measure based on Procrustes\nrotation that enables quantitative comparison of the output of manifold-based\nembedding algorithms (such as LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum\net al, 2000)). The measure also serves as a natural tool when choosing\ndimension-reduction parameters. We also present two novel dimension-reduction\ntechniques that attempt to minimize the suggested measure, and compare the\nresults of these techniques to the results of existing algorithms. Finally, we\nsuggest a simple iterative method that can be used to improve the output of\nexisting algorithms.\n","text":"Title:Local Procrustes for Manifold Embedding: A Measure of Embedding Quality\n  and Embedding Algorithms\nAbstract:  We present the Procrustes measure, a novel measure based on Procrustes\nrotation that enables quantitative comparison of the output of manifold-based\nembedding algorithms (such as LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum\net al, 2000)). The measure also serves as a natural tool when choosing\ndimension-reduction parameters. We also present two novel dimension-reduction\ntechniques that attempt to minimize the suggested measure, and compare the\nresults of these techniques to the results of existing algorithms. Finally, we\nsuggest a simple iterative method that can be used to improve the output of\nexisting algorithms.\n","vector":null,"chunk_id":"8b65b1e4c133bb675582b7ddeb5a6fe9"}
{"title":"Supervised functional classification: A theoretical remark and some\n  comparisons","authors":"Amparo Baillo and Antonio Cuevas","category":"stat.ML","abstract":"  The problem of supervised classification (or discrimination) with functional\ndata is considered, with a special interest on the popular k-nearest neighbors\n(k-NN) classifier. First, relying on a recent result by Cerou and Guyader\n(2006), we prove the consistency of the k-NN classifier for functional data\nwhose distribution belongs to a broad family of Gaussian processes with\ntriangular covariance functions. Second, on a more practical side, we check the\nbehavior of the k-NN method when compared with a few other functional\nclassifiers. This is carried out through a small simulation study and the\nanalysis of several real functional data sets. While no global \"uniform\" winner\nemerges from such comparisons, the overall performance of the k-NN method,\ntogether with its sound intuitive motivation and relative simplicity, suggests\nthat it could represent a reasonable benchmark for the classification problem\nwith functional data.\n","text":"Title:Supervised functional classification: A theoretical remark and some\n  comparisons\nAbstract:  The problem of supervised classification (or discrimination) with functional\ndata is considered, with a special interest on the popular k-nearest neighbors\n(k-NN) classifier. First, relying on a recent result by Cerou and Guyader\n(2006), we prove the consistency of the k-NN classifier for functional data\nwhose distribution belongs to a broad family of Gaussian processes with\ntriangular covariance functions. Second, on a more practical side, we check the\nbehavior of the k-NN method when compared with a few other functional\nclassifiers. This is carried out through a small simulation study and the\nanalysis of several real functional data sets. While no global \"uniform\" winner\nemerges from such comparisons, the overall performance of the k-NN method,\ntogether with its sound intuitive motivation and relative simplicity, suggests\nthat it could represent a reasonable benchmark for the classification problem\nwith functional data.\n","vector":null,"chunk_id":"8ee01b2375ad6ee180c3e9f122de19dd"}
{"title":"High-dimensional additive modeling","authors":"Lukas Meier, Sara van de Geer, Peter B\\\"uhlmann","category":"stat.ML","abstract":"  We propose a new sparsity-smoothness penalty for high-dimensional generalized\nadditive models. The combination of sparsity and smoothness is crucial for\nmathematical theory as well as performance for finite-sample data. We present a\ncomputationally efficient algorithm, with provable numerical convergence\nproperties, for optimizing the penalized likelihood. Furthermore, we provide\noracle results which yield asymptotic optimality of our estimator for high\ndimensional but sparse additive models. Finally, an adaptive version of our\nsparsity-smoothness penalized approach yields large additional performance\ngains.\n","text":"Title:High-dimensional additive modeling\nAbstract:  We propose a new sparsity-smoothness penalty for high-dimensional generalized\nadditive models. The combination of sparsity and smoothness is crucial for\nmathematical theory as well as performance for finite-sample data. We present a\ncomputationally efficient algorithm, with provable numerical convergence\nproperties, for optimizing the penalized likelihood. Furthermore, we provide\noracle results which yield asymptotic optimality of our estimator for high\ndimensional but sparse additive models. Finally, an adaptive version of our\nsparsity-smoothness penalized approach yields large additional performance\ngains.\n","vector":null,"chunk_id":"343bfe7d1f6b6a092359131e68f8e76a"}
{"title":"The model of quantum evolution","authors":"Konstantin P. Wishnevsky","category":"cs.AI","abstract":"  This paper has been withdrawn by the author due to extremely unscientific\nerrors.\n","text":"Title:The model of quantum evolution\nAbstract:  This paper has been withdrawn by the author due to extremely unscientific\nerrors.\n","vector":null,"chunk_id":"1f1d0e357d196ac2de6bdddcbcc681d6"}
{"title":"Interpr\\'etation vague des contraintes structurelles pour la RI dans des\n  corpus de documents XML - \\'Evaluation d'une m\\'ethode approch\\'ee de RI\n  structur\\'ee","authors":"Eugen Popovici (VALORIA), Gilbas M\\'enier (VALORIA),\n  Pierre-Fran\\c{c}ois Marteau (VALORIA)","category":"cs.IR","abstract":"  We propose specific data structures designed to the indexing and retrieval of\ninformation elements in heterogeneous XML data bases. The indexing scheme is\nwell suited to the management of various contextual searches, expressed either\nat a structural level or at an information content level. The approximate\nsearch mechanisms are based on a modified Levenshtein editing distance and\ninformation fusion heuristics. The implementation described highlights the\nmixing of structured information presented as field/value instances and free\ntext elements. The retrieval performances of the proposed approach are\nevaluated within the INEX 2005 evaluation campaign. The evaluation results rank\nthe proposed approach among the best evaluated XML IR systems for the VVCAS\ntask.\n","text":"Title:Interpr\\'etation vague des contraintes structurelles pour la RI dans des\n  corpus de documents XML - \\'Evaluation d'une m\\'ethode approch\\'ee de RI\n  structur\\'ee\nAbstract:  We propose specific data structures designed to the indexing and retrieval of\ninformation elements in heterogeneous XML data bases. The indexing scheme is\nwell suited to the management of various contextual searches, expressed either\nat a structural level or at an information content level. The approximate\nsearch mechanisms are based on a modified Levenshtein editing distance and\ninformation fusion heuristics. The implementation described highlights the\nmixing of structured information presented as field/value instances and free\ntext elements. The retrieval performances of the proposed approach are\nevaluated within the INEX 2005 evaluation campaign. The evaluation results rank\nthe proposed approach among the best evaluated XML IR systems for the VVCAS\ntask.\n","vector":null,"chunk_id":"25a49c84ea59fa4a8c38edb4f19ec6ca"}
{"title":"Belief decision support and reject for textured images characterization","authors":"Arnaud Martin (E3I2)","category":"cs.AI","abstract":"  The textured images' classification assumes to consider the images in terms\nof area with the same texture. In uncertain environment, it could be better to\ntake an imprecise decision or to reject the area corresponding to an unlearning\nclass. Moreover, on the areas that are the classification units, we can have\nmore than one texture. These considerations allows us to develop a belief\ndecision model permitting to reject an area as unlearning and to decide on\nunions and intersections of learning classes. The proposed approach finds all\nits justification in an application of seabed characterization from sonar\nimages, which contributes to an illustration.\n","text":"Title:Belief decision support and reject for textured images characterization\nAbstract:  The textured images' classification assumes to consider the images in terms\nof area with the same texture. In uncertain environment, it could be better to\ntake an imprecise decision or to reject the area corresponding to an unlearning\nclass. Moreover, on the areas that are the classification units, we can have\nmore than one texture. These considerations allows us to develop a belief\ndecision model permitting to reject an area as unlearning and to decide on\nunions and intersections of learning classes. The proposed approach finds all\nits justification in an application of seabed characterization from sonar\nimages, which contributes to an illustration.\n","vector":null,"chunk_id":"bd4ebcd3bcb5bf00e649b5acb49a9c9d"}
{"title":"The Correspondence Analysis Platform for Uncovering Deep Structure in\n  Data and Information","authors":"Fionn Murtagh","category":"cs.AI","abstract":"  We study two aspects of information semantics: (i) the collection of all\nrelationships, (ii) tracking and spotting anomaly and change. The first is\nimplemented by endowing all relevant information spaces with a Euclidean metric\nin a common projected space. The second is modelled by an induced ultrametric.\nA very general way to achieve a Euclidean embedding of different information\nspaces based on cross-tabulation counts (and from other input data formats) is\nprovided by Correspondence Analysis. From there, the induced ultrametric that\nwe are particularly interested in takes a sequential - e.g. temporal - ordering\nof the data into account. We employ such a perspective to look at narrative,\n\"the flow of thought and the flow of language\" (Chafe). In application to\npolicy decision making, we show how we can focus analysis in a small number of\ndimensions.\n","text":"Title:The Correspondence Analysis Platform for Uncovering Deep Structure in\n  Data and Information\nAbstract:  We study two aspects of information semantics: (i) the collection of all\nrelationships, (ii) tracking and spotting anomaly and change. The first is\nimplemented by endowing all relevant information spaces with a Euclidean metric\nin a common projected space. The second is modelled by an induced ultrametric.\nA very general way to achieve a Euclidean embedding of different information\nspaces based on cross-tabulation counts (and from other input data formats) is\nprovided by Correspondence Analysis. From there, the induced ultrametric that\nwe are particularly interested in takes a sequential - e.g. temporal - ordering\nof the data into account. We employ such a perspective to look at narrative,\n\"the flow of thought and the flow of language\" (Chafe). In application to\npolicy decision making, we show how we can focus analysis in a small number of\ndimensions.\n","vector":null,"chunk_id":"75b99a1d1464d82e4a083c6eca275c99"}
{"title":"Extension of Inagaki General Weighted Operators and A New Fusion Rule\n  Class of Proportional Redistribution of Intersection Masses","authors":"Florentin Smarandache","category":"cs.AI","abstract":"  In this paper we extend Inagaki Weighted Operators fusion rule (WO) in\ninformation fusion by doing redistribution of not only the conflicting mass,\nbut also of masses of non-empty intersections, that we call Double Weighted\nOperators (DWO). Then we propose a new fusion rule Class of Proportional\nRedistribution of Intersection Masses (CPRIM), which generates many interesting\nparticular fusion rules in information fusion. Both formulas are presented for\nany number of sources of information. An application and comparison with other\nfusion rules are given in the last section.\n","text":"Title:Extension of Inagaki General Weighted Operators and A New Fusion Rule\n  Class of Proportional Redistribution of Intersection Masses\nAbstract:  In this paper we extend Inagaki Weighted Operators fusion rule (WO) in\ninformation fusion by doing redistribution of not only the conflicting mass,\nbut also of masses of non-empty intersections, that we call Double Weighted\nOperators (DWO). Then we propose a new fusion rule Class of Proportional\nRedistribution of Intersection Masses (CPRIM), which generates many interesting\nparticular fusion rules in information fusion. Both formulas are presented for\nany number of sources of information. An application and comparison with other\nfusion rules are given in the last section.\n","vector":null,"chunk_id":"cd945f16152925465bb5c2014f1d561e"}
{"title":"Implementing general belief function framework with a practical\n  codification for low complexity","authors":"Arnaud Martin (E3I2)","category":"cs.AI","abstract":"  In this chapter, we propose a new practical codification of the elements of\nthe Venn diagram in order to easily manipulate the focal elements. In order to\nreduce the complexity, the eventual constraints must be integrated in the\ncodification at the beginning. Hence, we only consider a reduced hyper power\nset $D_r^\\Theta$ that can be $2^\\Theta$ or $D^\\Theta$. We describe all the\nsteps of a general belief function framework. The step of decision is\nparticularly studied, indeed, when we can decide on intersections of the\nsingletons of the discernment space no actual decision functions are easily to\nuse. Hence, two approaches are proposed, an extension of previous one and an\napproach based on the specificity of the elements on which to decide. The\nprincipal goal of this chapter is to provide practical codes of a general\nbelief function framework for the researchers and users needing the belief\nfunction theory.\n","text":"Title:Implementing general belief function framework with a practical\n  codification for low complexity\nAbstract:  In this chapter, we propose a new practical codification of the elements of\nthe Venn diagram in order to easily manipulate the focal elements. In order to\nreduce the complexity, the eventual constraints must be integrated in the\ncodification at the beginning. Hence, we only consider a reduced hyper power\nset $D_r^\\Theta$ that can be $2^\\Theta$ or $D^\\Theta$. We describe all the\nsteps of a general belief function framework. The step of decision is\nparticularly studied, indeed, when we can decide on intersections of the\nsingletons of the discernment space no actual decision functions are easily to\nuse. Hence, two approaches are proposed, an extension of previous one and an\napproach based on the specificity of the elements on which to decide. The\nprincipal goal of this chapter is to provide practical codes of a general\nbelief function framework for the researchers and users needing the belief\nfunction theory.\n","vector":null,"chunk_id":"5136467de5142efd1db16363a1c99c0b"}
{"title":"A new probabilistic transformation of belief mass assignment","authors":"Jean Dezert (ONERA), Florentin Smarandache","category":"cs.AI","abstract":"  In this paper, we propose in Dezert-Smarandache Theory (DSmT) framework, a\nnew probabilistic transformation, called DSmP, in order to build a subjective\nprobability measure from any basic belief assignment defined on any model of\nthe frame of discernment. Several examples are given to show how the DSmP\ntransformation works and we compare it to main existing transformations\nproposed in the literature so far. We show the advantages of DSmP over\nclassical transformations in term of Probabilistic Information Content (PIC).\nThe direct extension of this transformation for dealing with qualitative belief\nassignments is also presented.\n","text":"Title:A new probabilistic transformation of belief mass assignment\nAbstract:  In this paper, we propose in Dezert-Smarandache Theory (DSmT) framework, a\nnew probabilistic transformation, called DSmP, in order to build a subjective\nprobability measure from any basic belief assignment defined on any model of\nthe frame of discernment. Several examples are given to show how the DSmP\ntransformation works and we compare it to main existing transformations\nproposed in the literature so far. We show the advantages of DSmP over\nclassical transformations in term of Probabilistic Information Content (PIC).\nThe direct extension of this transformation for dealing with qualitative belief\nassignments is also presented.\n","vector":null,"chunk_id":"616cc01d7359b86779a6d479f6805609"}
{"title":"On Introspection, Metacognitive Control and Augmented Data Mining Live\n  Cycles","authors":"Daniel Sonntag","category":"cs.AI","abstract":"  We discuss metacognitive modelling as an enhancement to cognitive modelling\nand computing. Metacognitive control mechanisms should enable AI systems to\nself-reflect, reason about their actions, and to adapt to new situations. In\nthis respect, we propose implementation details of a knowledge taxonomy and an\naugmented data mining life cycle which supports a live integration of obtained\nmodels.\n","text":"Title:On Introspection, Metacognitive Control and Augmented Data Mining Live\n  Cycles\nAbstract:  We discuss metacognitive modelling as an enhancement to cognitive modelling\nand computing. Metacognitive control mechanisms should enable AI systems to\nself-reflect, reason about their actions, and to adapt to new situations. In\nthis respect, we propose implementation details of a knowledge taxonomy and an\naugmented data mining life cycle which supports a live integration of obtained\nmodels.\n","vector":null,"chunk_id":"819defe6a12b5db30c7321fd98acf4c6"}
{"title":"Hacia una teoria de unificacion para los comportamientos cognitivos","authors":"Sergio Miguel","category":"cs.AI","abstract":"  Each cognitive science tries to understand a set of cognitive behaviors. The\nstructuring of knowledge of this nature's aspect is far from what it can be\nexpected about a science. Until now universal standard consistently describing\nthe set of cognitive behaviors has not been found, and there are many questions\nabout the cognitive behaviors for which only there are opinions of members of\nthe scientific community. This article has three proposals. The first proposal\nis to raise to the scientific community the necessity of unified the cognitive\nbehaviors. The second proposal is claim the application of the Newton's\nreasoning rules about nature of his book, Philosophiae Naturalis Principia\nMathematica, to the cognitive behaviors. The third is to propose a scientific\ntheory, currently developing, that follows the rules established by Newton to\nmake sense of nature, and could be the theory to explain all the cognitive\nbehaviors.\n","text":"Title:Hacia una teoria de unificacion para los comportamientos cognitivos\nAbstract:  Each cognitive science tries to understand a set of cognitive behaviors. The\nstructuring of knowledge of this nature's aspect is far from what it can be\nexpected about a science. Until now universal standard consistently describing\nthe set of cognitive behaviors has not been found, and there are many questions\nabout the cognitive behaviors for which only there are opinions of members of\nthe scientific community. This article has three proposals. The first proposal\nis to raise to the scientific community the necessity of unified the cognitive\nbehaviors. The second proposal is claim the application of the Newton's\nreasoning rules about nature of his book, Philosophiae Naturalis Principia\nMathematica, to the cognitive behaviors. The third is to propose a scientific\ntheory, currently developing, that follows the rules established by Newton to\nmake sense of nature, and could be the theory to explain all the cognitive\nbehaviors.\n","vector":null,"chunk_id":"7af388dc391459bbe783d5f63b8ddc81"}
{"title":"LLE with low-dimensional neighborhood representation","authors":"Yair Goldberg and Ya'acov Ritov","category":"stat.ML","abstract":"  The local linear embedding algorithm (LLE) is a non-linear dimension-reducing\ntechnique, widely used due to its computational simplicity and intuitive\napproach. LLE first linearly reconstructs each input point from its nearest\nneighbors and then preserves these neighborhood relations in the\nlow-dimensional embedding. We show that the reconstruction weights computed by\nLLE capture the high-dimensional structure of the neighborhoods, and not the\nlow-dimensional manifold structure. Consequently, the weight vectors are highly\nsensitive to noise. Moreover, this causes LLE to converge to a linear\nprojection of the input, as opposed to its non-linear embedding goal. To\novercome both of these problems, we propose to compute the weight vectors using\na low-dimensional neighborhood representation. We prove theoretically that this\nstraightforward and computationally simple modification of LLE reduces LLE's\nsensitivity to noise. This modification also removes the need for\nregularization when the number of neighbors is larger than the dimension of the\ninput. We present numerical examples demonstrating both the perturbation and\nlinear projection problems, and the improved outputs using the low-dimensional\nneighborhood representation.\n","text":"Title:LLE with low-dimensional neighborhood representation\nAbstract:  The local linear embedding algorithm (LLE) is a non-linear dimension-reducing\ntechnique, widely used due to its computational simplicity and intuitive\napproach. LLE first linearly reconstructs each input point from its nearest\nneighbors and then preserves these neighborhood relations in the\nlow-dimensional embedding. We show that the reconstruction weights computed by\nLLE capture the high-dimensional structure of the neighborhoods, and not the\nlow-dimensional manifold structure. Consequently, the weight vectors are highly\nsensitive to noise. Moreover, this causes LLE to converge to a linear\nprojection of the input, as opposed to its non-linear embedding goal. To\novercome both of these problems, we propose to compute the weight vectors using\na low-dimensional neighborhood representation. We prove theoretically that this\nstraightforward and computationally simple modification of LLE reduces LLE's\nsensitivity to noise. This modification also removes the need for\nregularization when the number of neighbors is larger than the dimension of the\ninput. We present numerical examples demonstrating both the perturbation and\nlinear projection problems, and the improved outputs using the low-dimensional\nneighborhood representation.\n","vector":null,"chunk_id":"1917c5a6bbfc2d56e1b5e8cc9e3b435d"}
{"title":"Verified Null-Move Pruning","authors":"Omid David-Tabibi and Nathan S. Netanyahu","category":"cs.AI","abstract":"  In this article we review standard null-move pruning and introduce our\nextended version of it, which we call verified null-move pruning. In verified\nnull-move pruning, whenever the shallow null-move search indicates a fail-high,\ninstead of cutting off the search from the current node, the search is\ncontinued with reduced depth.\n  Our experiments with verified null-move pruning show that on average, it\nconstructs a smaller search tree with greater tactical strength in comparison\nto standard null-move pruning. Moreover, unlike standard null-move pruning,\nwhich fails badly in zugzwang positions, verified null-move pruning manages to\ndetect most zugzwangs and in such cases conducts a re-search to obtain the\ncorrect result. In addition, verified null-move pruning is very easy to\nimplement, and any standard null-move pruning program can use verified\nnull-move pruning by modifying only a few lines of code.\n","text":"Title:Verified Null-Move Pruning\nAbstract:  In this article we review standard null-move pruning and introduce our\nextended version of it, which we call verified null-move pruning. In verified\nnull-move pruning, whenever the shallow null-move search indicates a fail-high,\ninstead of cutting off the search from the current node, the search is\ncontinued with reduced depth.\n  Our experiments with verified null-move pruning show that on average, it\nconstructs a smaller search tree with greater tactical strength in comparison\nto standard null-move pruning. Moreover, unlike standard null-move pruning,\nwhich fails badly in zugzwang positions, verified null-move pruning manages to\ndetect most zugzwangs and in such cases conducts a re-search to obtain the\ncorrect result. In addition, verified null-move pruning is very easy to\nimplement, and any standard null-move pruning program can use verified\nnull-move pruning by modifying only a few lines of code.\n","vector":null,"chunk_id":"41a820dbbfb9b4b4d457981fea5794e4"}
{"title":"Persistent Clustering and a Theorem of J. Kleinberg","authors":"Gunnar Carlsson and Facundo Memoli","category":"stat.ML","abstract":"  We construct a framework for studying clustering algorithms, which includes\ntwo key ideas: persistence and functoriality. The first encodes the idea that\nthe output of a clustering scheme should carry a multiresolution structure, the\nsecond the idea that one should be able to compare the results of clustering\nalgorithms as one varies the data set, for example by adding points or by\napplying functions to it. We show that within this framework, one can prove a\ntheorem analogous to one of J. Kleinberg, in which one obtains an existence and\nuniqueness theorem instead of a non-existence result. We explore further\nproperties of this unique scheme, stability and convergence are established.\n","text":"Title:Persistent Clustering and a Theorem of J. Kleinberg\nAbstract:  We construct a framework for studying clustering algorithms, which includes\ntwo key ideas: persistence and functoriality. The first encodes the idea that\nthe output of a clustering scheme should carry a multiresolution structure, the\nsecond the idea that one should be able to compare the results of clustering\nalgorithms as one varies the data set, for example by adding points or by\napplying functions to it. We show that within this framework, one can prove a\ntheorem analogous to one of J. Kleinberg, in which one obtains an existence and\nuniqueness theorem instead of a non-existence result. We explore further\nproperties of this unique scheme, stability and convergence are established.\n","vector":null,"chunk_id":"671a1024218ee1ee3dd22da1b3c30935"}
{"title":"Decomposable Principal Component Analysis","authors":"Ami Wiesel and Alfred O. Hero III","category":"stat.ML","abstract":"  We consider principal component analysis (PCA) in decomposable Gaussian\ngraphical models. We exploit the prior information in these models in order to\ndistribute its computation. For this purpose, we reformulate the problem in the\nsparse inverse covariance (concentration) domain and solve the global\neigenvalue problem using a sequence of local eigenvalue problems in each of the\ncliques of the decomposable graph. We demonstrate the application of our\nmethodology in the context of decentralized anomaly detection in the Abilene\nbackbone network. Based on the topology of the network, we propose an\napproximate statistical graphical model and distribute the computation of PCA.\n","text":"Title:Decomposable Principal Component Analysis\nAbstract:  We consider principal component analysis (PCA) in decomposable Gaussian\ngraphical models. We exploit the prior information in these models in order to\ndistribute its computation. For this purpose, we reformulate the problem in the\nsparse inverse covariance (concentration) domain and solve the global\neigenvalue problem using a sequence of local eigenvalue problems in each of the\ncliques of the decomposable graph. We demonstrate the application of our\nmethodology in the context of decentralized anomaly detection in the Abilene\nbackbone network. Based on the topology of the network, we propose an\napproximate statistical graphical model and distribute the computation of PCA.\n","vector":null,"chunk_id":"97a21db108f28122f7ff902004f264e4"}
{"title":"n-ary Fuzzy Logic and Neutrosophic Logic Operators","authors":"Florentin Smarandache, V. Christianto","category":"cs.AI","abstract":"  We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and\nneutrosophic logic binary operators. Then we generalize them to n-ary fuzzy\nlogic and neutrosophic logic operators using the smarandache codification of\nthe Venn diagram and a defined vector neutrosophic law. In such way, new\noperators in neutrosophic logic/set/probability are built.\n","text":"Title:n-ary Fuzzy Logic and Neutrosophic Logic Operators\nAbstract:  We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and\nneutrosophic logic binary operators. Then we generalize them to n-ary fuzzy\nlogic and neutrosophic logic operators using the smarandache codification of\nthe Venn diagram and a defined vector neutrosophic law. In such way, new\noperators in neutrosophic logic/set/probability are built.\n","vector":null,"chunk_id":"5da3880b0c4e2e2710e9a973cfd1701a"}
{"title":"Randomised Variable Neighbourhood Search for Multi Objective\n  Optimisation","authors":"Martin Josef Geiger","category":"cs.AI","abstract":"  Various local search approaches have recently been applied to machine\nscheduling problems under multiple objectives. Their foremost consideration is\nthe identification of the set of Pareto optimal alternatives. An important\naspect of successfully solving these problems lies in the definition of an\nappropriate neighbourhood structure. Unclear in this context remains, how\ninterdependencies within the fitness landscape affect the resolution of the\nproblem.\n  The paper presents a study of neighbourhood search operators for multiple\nobjective flow shop scheduling. Experiments have been carried out with twelve\ndifferent combinations of criteria. To derive exact conclusions, small problem\ninstances, for which the optimal solutions are known, have been chosen.\nStatistical tests show that no single neighbourhood operator is able to equally\nidentify all Pareto optimal alternatives. Significant improvements however have\nbeen obtained by hybridising the solution algorithm using a randomised variable\nneighbourhood search technique.\n","text":"Title:Randomised Variable Neighbourhood Search for Multi Objective\n  Optimisation\nAbstract:  Various local search approaches have recently been applied to machine\nscheduling problems under multiple objectives. Their foremost consideration is\nthe identification of the set of Pareto optimal alternatives. An important\naspect of successfully solving these problems lies in the definition of an\nappropriate neighbourhood structure. Unclear in this context remains, how\ninterdependencies within the fitness landscape affect the resolution of the\nproblem.\n  The paper presents a study of neighbourhood search operators for multiple\nobjective flow shop scheduling. Experiments have been carried out with twelve\ndifferent combinations of criteria. To derive exact conclusions, small problem\ninstances, for which the optimal solutions are known, have been chosen.\nStatistical tests show that no single neighbourhood operator is able to equally\nidentify all Pareto optimal alternatives. Significant improvements however have\nbeen obtained by hybridising the solution algorithm using a randomised variable\nneighbourhood search technique.\n","vector":null,"chunk_id":"0efdaf775ae9aa4777055a0934fef522"}
{"title":"Foundations of the Pareto Iterated Local Search Metaheuristic","authors":"Martin Josef Geiger","category":"cs.AI","abstract":"  The paper describes the proposition and application of a local search\nmetaheuristic for multi-objective optimization problems. It is based on two\nmain principles of heuristic search, intensification through variable\nneighborhoods, and diversification through perturbations and successive\niterations in favorable regions of the search space. The concept is\nsuccessfully tested on permutation flow shop scheduling problems under multiple\nobjectives. While the obtained results are encouraging in terms of their\nquality, another positive attribute of the approach is its' simplicity as it\ndoes require the setting of only very few parameters. The implementation of the\nPareto Iterated Local Search metaheuristic is based on the MOOPPS computer\nsystem of local search heuristics for multi-objective scheduling which has been\nawarded the European Academic Software Award 2002 in Ronneby, Sweden\n(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)\n","text":"Title:Foundations of the Pareto Iterated Local Search Metaheuristic\nAbstract:  The paper describes the proposition and application of a local search\nmetaheuristic for multi-objective optimization problems. It is based on two\nmain principles of heuristic search, intensification through variable\nneighborhoods, and diversification through perturbations and successive\niterations in favorable regions of the search space. The concept is\nsuccessfully tested on permutation flow shop scheduling problems under multiple\nobjectives. While the obtained results are encouraging in terms of their\nquality, another positive attribute of the approach is its' simplicity as it\ndoes require the setting of only very few parameters. The implementation of the\nPareto Iterated Local Search metaheuristic is based on the MOOPPS computer\nsystem of local search heuristics for multi-objective scheduling which has been\nawarded the European Academic Software Award 2002 in Ronneby, Sweden\n(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)\n","vector":null,"chunk_id":"7edb0823694cf3eb2beae4392c747304"}
{"title":"A Computational Study of Genetic Crossover Operators for Multi-Objective\n  Vehicle Routing Problem with Soft Time Windows","authors":"Martin Josef Geiger","category":"cs.AI","abstract":"  The article describes an investigation of the effectiveness of genetic\nalgorithms for multi-objective combinatorial optimization (MOCO) by presenting\nan application for the vehicle routing problem with soft time windows. The work\nis motivated by the question, if and how the problem structure influences the\neffectiveness of different configurations of the genetic algorithm.\nComputational results are presented for different classes of vehicle routing\nproblems, varying in their coverage with time windows, time window size,\ndistribution and number of customers. The results are compared with a simple,\nbut effective local search approach for multi-objective combinatorial\noptimization problems.\n","text":"Title:A Computational Study of Genetic Crossover Operators for Multi-Objective\n  Vehicle Routing Problem with Soft Time Windows\nAbstract:  The article describes an investigation of the effectiveness of genetic\nalgorithms for multi-objective combinatorial optimization (MOCO) by presenting\nan application for the vehicle routing problem with soft time windows. The work\nis motivated by the question, if and how the problem structure influences the\neffectiveness of different configurations of the genetic algorithm.\nComputational results are presented for different classes of vehicle routing\nproblems, varying in their coverage with time windows, time window size,\ndistribution and number of customers. The results are compared with a simple,\nbut effective local search approach for multi-objective combinatorial\noptimization problems.\n","vector":null,"chunk_id":"19d535c5ed0906556a1cdd7240ad16a7"}
{"title":"Genetic Algorithms for multiple objective vehicle routing","authors":"Martin Josef Geiger","category":"cs.AI","abstract":"  The talk describes a general approach of a genetic algorithm for multiple\nobjective optimization problems. A particular dominance relation between the\nindividuals of the population is used to define a fitness operator, enabling\nthe genetic algorithm to adress even problems with efficient, but\nconvex-dominated alternatives. The algorithm is implemented in a multilingual\ncomputer program, solving vehicle routing problems with time windows under\nmultiple objectives. The graphical user interface of the program shows the\nprogress of the genetic algorithm and the main parameters of the approach can\nbe easily modified. In addition to that, the program provides powerful decision\nsupport to the decision maker. The software has proved it's excellence at the\nfinals of the European Academic Software Award EASA, held at the Keble college/\nUniversity of Oxford/ Great Britain.\n","text":"Title:Genetic Algorithms for multiple objective vehicle routing\nAbstract:  The talk describes a general approach of a genetic algorithm for multiple\nobjective optimization problems. A particular dominance relation between the\nindividuals of the population is used to define a fitness operator, enabling\nthe genetic algorithm to adress even problems with efficient, but\nconvex-dominated alternatives. The algorithm is implemented in a multilingual\ncomputer program, solving vehicle routing problems with time windows under\nmultiple objectives. The graphical user interface of the program shows the\nprogress of the genetic algorithm and the main parameters of the approach can\nbe easily modified. In addition to that, the program provides powerful decision\nsupport to the decision maker. The software has proved it's excellence at the\nfinals of the European Academic Software Award EASA, held at the Keble college/\nUniversity of Oxford/ Great Britain.\n","vector":null,"chunk_id":"dfaed7f2fea9218ba447f34a6ef1b5a1"}
{"title":"A framework for the interactive resolution of multi-objective vehicle\n  routing problems","authors":"Martin Josef Geiger, Wolf Wenger","category":"cs.AI","abstract":"  The article presents a framework for the resolution of rich vehicle routing\nproblems which are difficult to address with standard optimization techniques.\nWe use local search on the basis on variable neighborhood search for the\nconstruction of the solutions, but embed the techniques in a flexible framework\nthat allows the consideration of complex side constraints of the problem such\nas time windows, multiple depots, heterogeneous fleets, and, in particular,\nmultiple optimization criteria. In order to identify a compromise alternative\nthat meets the requirements of the decision maker, an interactive procedure is\nintegrated in the resolution of the problem, allowing the modification of the\npreference information articulated by the decision maker. The framework is\nprototypically implemented in a computer system. First results of test runs on\nmultiple depot vehicle routing problems with time windows are reported.\n","text":"Title:A framework for the interactive resolution of multi-objective vehicle\n  routing problems\nAbstract:  The article presents a framework for the resolution of rich vehicle routing\nproblems which are difficult to address with standard optimization techniques.\nWe use local search on the basis on variable neighborhood search for the\nconstruction of the solutions, but embed the techniques in a flexible framework\nthat allows the consideration of complex side constraints of the problem such\nas time windows, multiple depots, heterogeneous fleets, and, in particular,\nmultiple optimization criteria. In order to identify a compromise alternative\nthat meets the requirements of the decision maker, an interactive procedure is\nintegrated in the resolution of the problem, allowing the modification of the\npreference information articulated by the decision maker. The framework is\nprototypically implemented in a computer system. First results of test runs on\nmultiple depot vehicle routing problems with time windows are reported.\n","vector":null,"chunk_id":"eaa50a2edbb2ce733de7867f0707937f"}
{"title":"Improving Local Search for Fuzzy Scheduling Problems","authors":"Martin Josef Geiger, Sanja Petrovic","category":"cs.AI","abstract":"  The integration of fuzzy set theory and fuzzy logic into scheduling is a\nrather new aspect with growing importance for manufacturing applications,\nresulting in various unsolved aspects. In the current paper, we investigate an\nimproved local search technique for fuzzy scheduling problems with fitness\nplateaus, using a multi criteria formulation of the problem. We especially\naddress the problem of changing job priorities over time as studied at the\nSherwood Press Ltd, a Nottingham based printing company, who is a collaborator\non the project.\n","text":"Title:Improving Local Search for Fuzzy Scheduling Problems\nAbstract:  The integration of fuzzy set theory and fuzzy logic into scheduling is a\nrather new aspect with growing importance for manufacturing applications,\nresulting in various unsolved aspects. In the current paper, we investigate an\nimproved local search technique for fuzzy scheduling problems with fitness\nplateaus, using a multi criteria formulation of the problem. We especially\naddress the problem of changing job priorities over time as studied at the\nSherwood Press Ltd, a Nottingham based printing company, who is a collaborator\non the project.\n","vector":null,"chunk_id":"327f52d55e632105cfd4e67e2fd83c53"}
{"title":"Bin Packing Under Multiple Objectives - a Heuristic Approximation\n  Approach","authors":"Martin Josef Geiger","category":"cs.AI","abstract":"  The article proposes a heuristic approximation approach to the bin packing\nproblem under multiple objectives. In addition to the traditional objective of\nminimizing the number of bins, the heterogeneousness of the elements in each\nbin is minimized, leading to a biobjective formulation of the problem with a\ntradeoff between the number of bins and their heterogeneousness. An extension\nof the Best-Fit approximation algorithm is presented to solve the problem.\nExperimental investigations have been carried out on benchmark instances of\ndifferent size, ranging from 100 to 1000 items. Encouraging results have been\nobtained, showing the applicability of the heuristic approach to the described\nproblem.\n","text":"Title:Bin Packing Under Multiple Objectives - a Heuristic Approximation\n  Approach\nAbstract:  The article proposes a heuristic approximation approach to the bin packing\nproblem under multiple objectives. In addition to the traditional objective of\nminimizing the number of bins, the heterogeneousness of the elements in each\nbin is minimized, leading to a biobjective formulation of the problem with a\ntradeoff between the number of bins and their heterogeneousness. An extension\nof the Best-Fit approximation algorithm is presented to solve the problem.\nExperimental investigations have been carried out on benchmark instances of\ndifferent size, ranging from 100 to 1000 items. Encouraging results have been\nobtained, showing the applicability of the heuristic approach to the described\nproblem.\n","vector":null,"chunk_id":"858c0d5f426285211380bdf9689bbcff"}
{"title":"An application of the Threshold Accepting metaheuristic for curriculum\n  based course timetabling","authors":"Martin Josef Geiger","category":"cs.AI","abstract":"  The article presents a local search approach for the solution of timetabling\nproblems in general, with a particular implementation for competition track 3\nof the International Timetabling Competition 2007 (ITC 2007). The heuristic\nsearch procedure is based on Threshold Accepting to overcome local optima. A\nstochastic neighborhood is proposed and implemented, randomly removing and\nreassigning events from the current solution.\n  The overall concept has been incrementally obtained from a series of\nexperiments, which we describe in each (sub)section of the paper. In result, we\nsuccessfully derived a potential candidate solution approach for the finals of\ntrack 3 of the ITC 2007.\n","text":"Title:An application of the Threshold Accepting metaheuristic for curriculum\n  based course timetabling\nAbstract:  The article presents a local search approach for the solution of timetabling\nproblems in general, with a particular implementation for competition track 3\nof the International Timetabling Competition 2007 (ITC 2007). The heuristic\nsearch procedure is based on Threshold Accepting to overcome local optima. A\nstochastic neighborhood is proposed and implemented, randomly removing and\nreassigning events from the current solution.\n  The overall concept has been incrementally obtained from a series of\nexperiments, which we describe in each (sub)section of the paper. In result, we\nsuccessfully derived a potential candidate solution approach for the finals of\ntrack 3 of the ITC 2007.\n","vector":null,"chunk_id":"350dab8dd7d39aab2a71e8a0c65bd995"}
{"title":"Variable Neighborhood Search for the University Lecturer-Student\n  Assignment Problem","authors":"Martin Josef Geiger, Wolf Wenger","category":"cs.AI","abstract":"  The paper presents a study of local search heuristics in general and variable\nneighborhood search in particular for the resolution of an assignment problem\nstudied in the practical work of universities. Here, students have to be\nassigned to scientific topics which are proposed and supported by members of\nstaff. The problem involves the optimization under given preferences of\nstudents which may be expressed when applying for certain topics.\n  It is possible to observe that variable neighborhood search leads to superior\nresults for the tested problem instances. One instance is taken from an actual\ncase, while others have been generated based on the real world data to support\nthe analysis with a deeper analysis.\n  An extension of the problem has been formulated by integrating a second\nobjective function that simultaneously balances the workload of the members of\nstaff while maximizing utility of the students. The algorithmic approach has\nbeen prototypically implemented in a computer system. One important aspect in\nthis context is the application of the research work to problems of other\nscientific institutions, and therefore the provision of decision support\nfunctionalities.\n","text":"Title:Variable Neighborhood Search for the University Lecturer-Student\n  Assignment Problem\nAbstract:  The paper presents a study of local search heuristics in general and variable\nneighborhood search in particular for the resolution of an assignment problem\nstudied in the practical work of universities. Here, students have to be\nassigned to scientific topics which are proposed and supported by members of\nstaff. The problem involves the optimization under given preferences of\nstudents which may be expressed when applying for certain topics.\n  It is possible to observe that variable neighborhood search leads to superior\nresults for the tested problem instances. One instance is taken from an actual\ncase, while others have been generated based on the real world data to support\nthe analysis with a deeper analysis.\n  An extension of the problem has been formulated by integrating a second\nobjective function that simultaneously balances the workload of the members of\nstaff while maximizing utility of the students. The algorithmic approach has\nbeen prototypically implemented in a computer system. One important aspect in\nthis context is the application of the research work to problems of other\nscientific institutions, and therefore the provision of decision support\nfunctionalities.\n","vector":null,"chunk_id":"a3241c39e8e2eaf8026cb51428727c98"}
{"title":"Extended ASP tableaux and rule redundancy in normal logic programs","authors":"Matti J\\\"arvisalo and Emilia Oikarinen","category":"cs.AI","abstract":"  We introduce an extended tableau calculus for answer set programming (ASP).\nThe proof system is based on the ASP tableaux defined in [Gebser&Schaub, ICLP\n2006], with an added extension rule. We investigate the power of Extended ASP\nTableaux both theoretically and empirically. We study the relationship of\nExtended ASP Tableaux with the Extended Resolution proof system defined by\nTseitin for sets of clauses, and separate Extended ASP Tableaux from ASP\nTableaux by giving a polynomial-length proof for a family of normal logic\nprograms P_n for which ASP Tableaux has exponential-length minimal proofs with\nrespect to n. Additionally, Extended ASP Tableaux imply interesting insight\ninto the effect of program simplification on the lengths of proofs in ASP.\nClosely related to Extended ASP Tableaux, we empirically investigate the effect\nof redundant rules on the efficiency of ASP solving.\n  To appear in Theory and Practice of Logic Programming (TPLP).\n","text":"Title:Extended ASP tableaux and rule redundancy in normal logic programs\nAbstract:  We introduce an extended tableau calculus for answer set programming (ASP).\nThe proof system is based on the ASP tableaux defined in [Gebser&Schaub, ICLP\n2006], with an added extension rule. We investigate the power of Extended ASP\nTableaux both theoretically and empirically. We study the relationship of\nExtended ASP Tableaux with the Extended Resolution proof system defined by\nTseitin for sets of clauses, and separate Extended ASP Tableaux from ASP\nTableaux by giving a polynomial-length proof for a family of normal logic\nprograms P_n for which ASP Tableaux has exponential-length minimal proofs with\nrespect to n. Additionally, Extended ASP Tableaux imply interesting insight\ninto the effect of program simplification on the lengths of proofs in ASP.\nClosely related to Extended ASP Tableaux, we empirically investigate the effect\nof redundant rules on the efficiency of ASP solving.\n  To appear in Theory and Practice of Logic Programming (TPLP).\n","vector":null,"chunk_id":"9065acea5e4fe7ea6ce1df26b9b544dd"}
{"title":"Achieving compositionality of the stable model semantics for Smodels\n  programs","authors":"Emilia Oikarinen, Tomi Janhunen","category":"cs.AI","abstract":"  In this paper, a Gaifman-Shapiro-style module architecture is tailored to the\ncase of Smodels programs under the stable model semantics. The composition of\nSmodels program modules is suitably limited by module conditions which ensure\nthe compatibility of the module system with stable models. Hence the semantics\nof an entire Smodels program depends directly on stable models assigned to its\nmodules. This result is formalized as a module theorem which truly strengthens\nLifschitz and Turner's splitting-set theorem for the class of Smodels programs.\nTo streamline generalizations in the future, the module theorem is first proved\nfor normal programs and then extended to cover Smodels programs using a\ntranslation from the latter class of programs to the former class. Moreover,\nthe respective notion of module-level equivalence, namely modular equivalence,\nis shown to be a proper congruence relation: it is preserved under\nsubstitutions of modules that are modularly equivalent. Principles for program\ndecomposition are also addressed. The strongly connected components of the\nrespective dependency graph can be exploited in order to extract a module\nstructure when there is no explicit a priori knowledge about the modules of a\nprogram. The paper includes a practical demonstration of tools that have been\ndeveloped for automated (de)composition of Smodels programs.\n  To appear in Theory and Practice of Logic Programming.\n","text":"Title:Achieving compositionality of the stable model semantics for Smodels\n  programs\nAbstract:  In this paper, a Gaifman-Shapiro-style module architecture is tailored to the\ncase of Smodels programs under the stable model semantics. The composition of\nSmodels program modules is suitably limited by module conditions which ensure\nthe compatibility of the module system with stable models. Hence the semantics\nof an entire Smodels program depends directly on stable models assigned to its\nmodules. This result is formalized as a module theorem which truly strengthens\nLifschitz and Turner's splitting-set theorem for the class of Smodels programs.\nTo streamline generalizations in the future, the module theorem is first proved\nfor normal programs and then extended to cover Smodels programs using a\ntranslation from the latter class of programs to the former class. Moreover,\nthe respective notion of module-level equivalence, namely modular equivalence,\nis shown to be a proper congruence relation: it is preserved under\nsubstitutions of modules that are modularly equivalent. Principles for program\ndecomposition are also addressed. The strongly connected components of the\nrespective dependency graph can be exploited in order to extract a module\nstructure when there is no explicit a priori knowledge about the modules of a\nprogram. The paper includes a practical demonstration of tools that have been\ndeveloped for automated (de)composition of Smodels programs.\n  To appear in Theory and Practice of Logic Programming.\n","vector":null,"chunk_id":"a764b0aaee98aa6892c6dceb1b848f60"}
{"title":"Faceted Ranking of Egos in Collaborative Tagging Systems","authors":"Jose Ignacio Orlicki (CoreLabs, ITBA), Pablo Ignacio Fierens (ITBA),\n  Jos\\'e Ignacio Alvarez-Hamelin (ITBA, CONICET)","category":"cs.IR","abstract":"  Multimedia uploaded content is tagged and recommended by users of\ncollaborative systems, resulting in informal classifications also known as\nfolksonomies. Faceted web ranking has been proved a reasonable alternative to a\nsingle ranking which does not take into account a personalized context. In this\npaper we analyze the online computation of rankings of users associated to\nfacets made up of multiple tags. Possible applications are user reputation\nevaluation (ego-ranking) and improvement of content quality in case of\nretrieval. We propose a solution based on PageRank as centrality measure: (i) a\nranking for each tag is computed offline on the basis of the corresponding\ntag-dependent subgraph; (ii) a faceted order is generated by merging rankings\ncorresponding to all the tags in the facet. The fundamental assumption,\nvalidated by empirical observations, is that step (i) is scalable. We also\npresent algorithms for part (ii) having time complexity O(k), where k is the\nnumber of tags in the facet, well suited to online computation.\n","text":"Title:Faceted Ranking of Egos in Collaborative Tagging Systems\nAbstract:  Multimedia uploaded content is tagged and recommended by users of\ncollaborative systems, resulting in informal classifications also known as\nfolksonomies. Faceted web ranking has been proved a reasonable alternative to a\nsingle ranking which does not take into account a personalized context. In this\npaper we analyze the online computation of rankings of users associated to\nfacets made up of multiple tags. Possible applications are user reputation\nevaluation (ego-ranking) and improvement of content quality in case of\nretrieval. We propose a solution based on PageRank as centrality measure: (i) a\nranking for each tag is computed offline on the basis of the corresponding\ntag-dependent subgraph; (ii) a faceted order is generated by merging rankings\ncorresponding to all the tags in the facet. The fundamental assumption,\nvalidated by empirical observations, is that step (i) is scalable. We also\npresent algorithms for part (ii) having time complexity O(k), where k is the\nnumber of tags in the facet, well suited to online computation.\n","vector":null,"chunk_id":"af777d792deddcb58f5e60e023476b01"}
{"title":"Relevance Feedback in Conceptual Image Retrieval: A User Evaluation","authors":"Jose Torres, Luis Paulo Reis","category":"cs.IR","abstract":"  The Visual Object Information Retrieval (VOIR) system described in this paper\nimplements an image retrieval approach that combines two layers, the conceptual\nand the visual layer. It uses terms from a textual thesaurus to represent the\nconceptual information and also works with image regions, the visual\ninformation. The terms are related with the image regions through a weighted\nassociation enabling the execution of concept-level queries. VOIR uses\nregion-based relevance feedback to improve the quality of the results in each\nquery session and to discover new associations between text and image. This\npaper describes a user-centred and task-oriented comparative evaluation of VOIR\nwhich was undertaken considering three distinct versions of VOIR: a full-fledge\nversion; one supporting relevance feedback only at image level; and a third\nversion not supporting relevance feedback at all. The evaluation performed\nshowed the usefulness of region based relevance feedback in the context of VOIR\nprototype.\n","text":"Title:Relevance Feedback in Conceptual Image Retrieval: A User Evaluation\nAbstract:  The Visual Object Information Retrieval (VOIR) system described in this paper\nimplements an image retrieval approach that combines two layers, the conceptual\nand the visual layer. It uses terms from a textual thesaurus to represent the\nconceptual information and also works with image regions, the visual\ninformation. The terms are related with the image regions through a weighted\nassociation enabling the execution of concept-level queries. VOIR uses\nregion-based relevance feedback to improve the quality of the results in each\nquery session and to discover new associations between text and image. This\npaper describes a user-centred and task-oriented comparative evaluation of VOIR\nwhich was undertaken considering three distinct versions of VOIR: a full-fledge\nversion; one supporting relevance feedback only at image level; and a third\nversion not supporting relevance feedback at all. The evaluation performed\nshowed the usefulness of region based relevance feedback in the context of VOIR\nprototype.\n","vector":null,"chunk_id":"46583e46e7c053b7198a0e6da23e771f"}
{"title":"Determining the Unithood of Word Sequences using a Probabilistic\n  Approach","authors":"Wilson Wong, Wei Liu, Mohammed Bennamoun","category":"cs.AI","abstract":"  Most research related to unithood were conducted as part of a larger effort\nfor the determination of termhood. Consequently, novelties are rare in this\nsmall sub-field of term extraction. In addition, existing work were mostly\nempirically motivated and derived. We propose a new probabilistically-derived\nmeasure, independent of any influences of termhood, that provides dedicated\nmeasures to gather linguistic evidence from parsed text and statistical\nevidence from Google search engine for the measurement of unithood. Our\ncomparative study using 1,825 test cases against an existing\nempirically-derived function revealed an improvement in terms of precision,\nrecall and accuracy.\n","text":"Title:Determining the Unithood of Word Sequences using a Probabilistic\n  Approach\nAbstract:  Most research related to unithood were conducted as part of a larger effort\nfor the determination of termhood. Consequently, novelties are rare in this\nsmall sub-field of term extraction. In addition, existing work were mostly\nempirically motivated and derived. We propose a new probabilistically-derived\nmeasure, independent of any influences of termhood, that provides dedicated\nmeasures to gather linguistic evidence from parsed text and statistical\nevidence from Google search engine for the measurement of unithood. Our\ncomparative study using 1,825 test cases against an existing\nempirically-derived function revealed an improvement in terms of precision,\nrecall and accuracy.\n","vector":null,"chunk_id":"0ee4ccd30ed9e946dabb04d14ecfbca9"}
{"title":"Determining the Unithood of Word Sequences using Mutual Information and\n  Independence Measure","authors":"Wilson Wong, Wei Liu, Mohammed Bennamoun","category":"cs.AI","abstract":"  Most works related to unithood were conducted as part of a larger effort for\nthe determination of termhood. Consequently, the number of independent research\nthat study the notion of unithood and produce dedicated techniques for\nmeasuring unithood is extremely small. We propose a new approach, independent\nof any influences of termhood, that provides dedicated measures to gather\nlinguistic evidence from parsed text and statistical evidence from Google\nsearch engine for the measurement of unithood. Our evaluations revealed a\nprecision and recall of 98.68% and 91.82% respectively with an accuracy at\n95.42% in measuring the unithood of 1005 test cases.\n","text":"Title:Determining the Unithood of Word Sequences using Mutual Information and\n  Independence Measure\nAbstract:  Most works related to unithood were conducted as part of a larger effort for\nthe determination of termhood. Consequently, the number of independent research\nthat study the notion of unithood and produce dedicated techniques for\nmeasuring unithood is extremely small. We propose a new approach, independent\nof any influences of termhood, that provides dedicated measures to gather\nlinguistic evidence from parsed text and statistical evidence from Google\nsearch engine for the measurement of unithood. Our evaluations revealed a\nprecision and recall of 98.68% and 91.82% respectively with an accuracy at\n95.42% in measuring the unithood of 1005 test cases.\n","vector":null,"chunk_id":"fab7df7e7ca11a7d6c90f2268944227a"}
{"title":"Large Scale Variational Inference and Experimental Design for Sparse\n  Generalized Linear Models","authors":"Matthias W. Seeger, Hannes Nickisch","category":"stat.ML","abstract":"  Many problems of low-level computer vision and image processing, such as\ndenoising, deconvolution, tomographic reconstruction or super-resolution, can\nbe addressed by maximizing the posterior distribution of a sparse linear model\n(SLM). We show how higher-order Bayesian decision-making problems, such as\noptimizing image acquisition in magnetic resonance scanners, can be addressed\nby querying the SLM posterior covariance, unrelated to the density's mode. We\npropose a scalable algorithmic framework, with which SLM posteriors over full,\nhigh-resolution images can be approximated for the first time, solving a\nvariational optimization problem which is convex iff posterior mode finding is\nconvex. These methods successfully drive the optimization of sampling\ntrajectories for real-world magnetic resonance imaging through Bayesian\nexperimental design, which has not been attempted before. Our methodology\nprovides new insight into similarities and differences between sparse\nreconstruction and approximate Bayesian inference, and has important\nimplications for compressive sensing of real-world images.\n","text":"Title:Large Scale Variational Inference and Experimental Design for Sparse\n  Generalized Linear Models\nAbstract:  Many problems of low-level computer vision and image processing, such as\ndenoising, deconvolution, tomographic reconstruction or super-resolution, can\nbe addressed by maximizing the posterior distribution of a sparse linear model\n(SLM). We show how higher-order Bayesian decision-making problems, such as\noptimizing image acquisition in magnetic resonance scanners, can be addressed\nby querying the SLM posterior covariance, unrelated to the density's mode. We\npropose a scalable algorithmic framework, with which SLM posteriors over full,\nhigh-resolution images can be approximated for the first time, solving a\nvariational optimization problem which is convex iff posterior mode finding is\nconvex. These methods successfully drive the optimization of sampling\ntrajectories for real-world magnetic resonance imaging through Bayesian\nexperimental design, which has not been attempted before. Our methodology\nprovides new insight into similarities and differences between sparse\nreconstruction and approximate Bayesian inference, and has important\nimplications for compressive sensing of real-world images.\n","vector":null,"chunk_id":"6eb2e1ca20c8e5278f9bfc0dc8684b40"}
{"title":"Introduction to Searching with Regular Expressions","authors":"Christopher M. Frenz","category":"cs.IR","abstract":"  The explosive rate of information growth and availability often makes it\nincreasingly difficult to locate information pertinent to your needs. These\nproblems are often compounded when keyword based search methodologies are not\nadequate for describing the information you seek. In many instances,\ninformation such as Web site URLs, phone numbers, etc. can often be better\nidentified through the use of a textual pattern than by keyword. For example,\nmany more phone numbers could be picked up by a search for the pattern (XXX)\nXXX-XXXX, where X could be any digit, than would be by a search for any\nspecific phone number (i.e. the keyword approach). Programming languages\ntypically allow for the matching of textual patterns via the usage of regular\nexpressions. This tutorial will provide an introduction to the basics of\nprogramming regular expressions as well as provide an introduction to how\nregular expressions can be applied to data processing tasks such as information\nextraction and search refinement.\n","text":"Title:Introduction to Searching with Regular Expressions\nAbstract:  The explosive rate of information growth and availability often makes it\nincreasingly difficult to locate information pertinent to your needs. These\nproblems are often compounded when keyword based search methodologies are not\nadequate for describing the information you seek. In many instances,\ninformation such as Web site URLs, phone numbers, etc. can often be better\nidentified through the use of a textual pattern than by keyword. For example,\nmany more phone numbers could be picked up by a search for the pattern (XXX)\nXXX-XXXX, where X could be any digit, than would be by a search for any\nspecific phone number (i.e. the keyword approach). Programming languages\ntypically allow for the matching of textual patterns via the usage of regular\nexpressions. This tutorial will provide an introduction to the basics of\nprogramming regular expressions as well as provide an introduction to how\nregular expressions can be applied to data processing tasks such as information\nextraction and search refinement.\n","vector":null,"chunk_id":"812393e45852d7008fc28223c1b0e440"}
{"title":"Online Coordinate Boosting","authors":"Raphael Pelossof, Michael Jones, Ilia Vovsha, Cynthia Rudin","category":"stat.ML","abstract":"  We present a new online boosting algorithm for adapting the weights of a\nboosted classifier, which yields a closer approximation to Freund and\nSchapire's AdaBoost algorithm than previous online boosting algorithms. We also\ncontribute a new way of deriving the online algorithm that ties together\nprevious online boosting work. We assume that the weak hypotheses were selected\nbeforehand, and only their weights are updated during online boosting. The\nupdate rule is derived by minimizing AdaBoost's loss when viewed in an\nincremental form. The equations show that optimization is computationally\nexpensive. However, a fast online approximation is possible. We compare\napproximation error to batch AdaBoost on synthetic datasets and generalization\nerror on face datasets and the MNIST dataset.\n","text":"Title:Online Coordinate Boosting\nAbstract:  We present a new online boosting algorithm for adapting the weights of a\nboosted classifier, which yields a closer approximation to Freund and\nSchapire's AdaBoost algorithm than previous online boosting algorithms. We also\ncontribute a new way of deriving the online algorithm that ties together\nprevious online boosting work. We assume that the weak hypotheses were selected\nbeforehand, and only their weights are updated during online boosting. The\nupdate rule is derived by minimizing AdaBoost's loss when viewed in an\nincremental form. The equations show that optimization is computationally\nexpensive. However, a fast online approximation is possible. We compare\napproximation error to batch AdaBoost on synthetic datasets and generalization\nerror on face datasets and the MNIST dataset.\n","vector":null,"chunk_id":"a0bc8d1548860aa02d617c8424ec4cd7"}
{"title":"A non-negative expansion for small Jensen-Shannon Divergences","authors":"Anil Raj and Chris H. Wiggins","category":"stat.ML","abstract":"  In this report, we derive a non-negative series expansion for the\nJensen-Shannon divergence (JSD) between two probability distributions. This\nseries expansion is shown to be useful for numerical calculations of the JSD,\nwhen the probability distributions are nearly equal, and for which,\nconsequently, small numerical errors dominate evaluation.\n","text":"Title:A non-negative expansion for small Jensen-Shannon Divergences\nAbstract:  In this report, we derive a non-negative series expansion for the\nJensen-Shannon divergence (JSD) between two probability distributions. This\nseries expansion is shown to be useful for numerical calculations of the JSD,\nwhen the probability distributions are nearly equal, and for which,\nconsequently, small numerical errors dominate evaluation.\n","vector":null,"chunk_id":"a0137dae7544eae9fd460af35afde94f"}
{"title":"Query Refinement by Multi Word Term expansions and semantic synonymy","authors":"Veronila Lux-Pogodalla (INIST), Eric San Juan","category":"cs.IR","abstract":"  We developed a system, TermWatch\n(https://stid-bdd.iut.univ-metz.fr/TermWatch/index.pl), which combines a\nlinguistic extraction of terms, their structuring into a terminological network\nwith a clustering algorithm. In this paper we explore its ability in\nintegrating the most promising aspects of the studies on query refinement:\nchoice of meaningful text units to cluster (domain terms), choice of tight\nsemantic relations with which to cluster terms, structuring of terms in a\nnetwork enabling abetter perception of domain concepts. We have run this\nexperiment on the 367 645 English abstracts of PASCAL 2005-2006 bibliographic\ndatabase (http://www.inist.fr) and compared the structured terminological\nresource automatically build by TermWarch to the English segment of TermScience\nresource (http://termsciences.inist.fr/) containing 88 211 terms.\n","text":"Title:Query Refinement by Multi Word Term expansions and semantic synonymy\nAbstract:  We developed a system, TermWatch\n(https://stid-bdd.iut.univ-metz.fr/TermWatch/index.pl), which combines a\nlinguistic extraction of terms, their structuring into a terminological network\nwith a clustering algorithm. In this paper we explore its ability in\nintegrating the most promising aspects of the studies on query refinement:\nchoice of meaningful text units to cluster (domain terms), choice of tight\nsemantic relations with which to cluster terms, structuring of terms in a\nnetwork enabling abetter perception of domain concepts. We have run this\nexperiment on the 367 645 English abstracts of PASCAL 2005-2006 bibliographic\ndatabase (http://www.inist.fr) and compared the structured terminological\nresource automatically build by TermWarch to the English segment of TermScience\nresource (http://termsciences.inist.fr/) containing 88 211 terms.\n","vector":null,"chunk_id":"c1a6c1c8be79827aa9cfb42912b2d6ef"}
{"title":"Improved Estimation of High-dimensional Ising Models","authors":"M. Kolar, E. P. Xing","category":"stat.ML","abstract":"  We consider the problem of jointly estimating the parameters as well as the\nstructure of binary valued Markov Random Fields, in contrast to earlier work\nthat focus on one of the two problems. We formulate the problem as a\nmaximization of $\\ell_1$-regularized surrogate likelihood that allows us to\nfind a sparse solution. Our optimization technique efficiently incorporates the\ncutting-plane algorithm in order to obtain a tighter outer bound on the\nmarginal polytope, which results in improvement of both parameter estimates and\napproximation to marginals. On synthetic data, we compare our algorithm on the\ntwo estimation tasks to the other existing methods. We analyze the method in\nthe high-dimensional setting, where the number of dimensions $p$ is allowed to\ngrow with the number of observations $n$. The rate of convergence of the\nestimate is demonstrated to depend explicitly on the sparsity of the underlying\ngraph.\n","text":"Title:Improved Estimation of High-dimensional Ising Models\nAbstract:  We consider the problem of jointly estimating the parameters as well as the\nstructure of binary valued Markov Random Fields, in contrast to earlier work\nthat focus on one of the two problems. We formulate the problem as a\nmaximization of $\\ell_1$-regularized surrogate likelihood that allows us to\nfind a sparse solution. Our optimization technique efficiently incorporates the\ncutting-plane algorithm in order to obtain a tighter outer bound on the\nmarginal polytope, which results in improvement of both parameter estimates and\napproximation to marginals. On synthetic data, we compare our algorithm on the\ntwo estimation tasks to the other existing methods. We analyze the method in\nthe high-dimensional setting, where the number of dimensions $p$ is allowed to\ngrow with the number of observations $n$. The rate of convergence of the\nestimate is demonstrated to depend explicitly on the sparsity of the underlying\ngraph.\n","vector":null,"chunk_id":"486bef00538d71bd6f27016dc976df2f"}
{"title":"Kernel Regression by Mode Calculation of the Conditional Probability\n  Distribution","authors":"Steffen Kuehn","category":"stat.ML","abstract":"  The most direct way to express arbitrary dependencies in datasets is to\nestimate the joint distribution and to apply afterwards the argmax-function to\nobtain the mode of the corresponding conditional distribution. This method is\nin practice difficult, because it requires a global optimization of a\ncomplicated function, the joint distribution by fixed input variables. This\narticle proposes a method for finding global maxima if the joint distribution\nis modeled by a kernel density estimation. Some experiments show advantages and\nshortcomings of the resulting regression method in comparison to the standard\nNadaraya-Watson regression technique, which approximates the optimum by the\nexpectation value.\n","text":"Title:Kernel Regression by Mode Calculation of the Conditional Probability\n  Distribution\nAbstract:  The most direct way to express arbitrary dependencies in datasets is to\nestimate the joint distribution and to apply afterwards the argmax-function to\nobtain the mode of the corresponding conditional distribution. This method is\nin practice difficult, because it requires a global optimization of a\ncomplicated function, the joint distribution by fixed input variables. This\narticle proposes a method for finding global maxima if the joint distribution\nis modeled by a kernel density estimation. Some experiments show advantages and\nshortcomings of the resulting regression method in comparison to the standard\nNadaraya-Watson regression technique, which approximates the optimum by the\nexpectation value.\n","vector":null,"chunk_id":"bf2c908f56b17bed58791ec34d087e4d"}
{"title":"Entropy inference and the James-Stein estimator, with application to\n  nonlinear gene association networks","authors":"Jean Hausser and Korbinian Strimmer","category":"stat.ML","abstract":"  We present a procedure for effective estimation of entropy and mutual\ninformation from small-sample data, and apply it to the problem of inferring\nhigh-dimensional gene association networks. Specifically, we develop a\nJames-Stein-type shrinkage estimator, resulting in a procedure that is highly\nefficient statistically as well as computationally. Despite its simplicity, we\nshow that it outperforms eight other entropy estimation procedures across a\ndiverse range of sampling scenarios and data-generating models, even in cases\nof severe undersampling. We illustrate the approach by analyzing E. coli gene\nexpression data and computing an entropy-based gene-association network from\ngene expression data. A computer program is available that implements the\nproposed shrinkage estimator.\n","text":"Title:Entropy inference and the James-Stein estimator, with application to\n  nonlinear gene association networks\nAbstract:  We present a procedure for effective estimation of entropy and mutual\ninformation from small-sample data, and apply it to the problem of inferring\nhigh-dimensional gene association networks. Specifically, we develop a\nJames-Stein-type shrinkage estimator, resulting in a procedure that is highly\nefficient statistically as well as computationally. Despite its simplicity, we\nshow that it outperforms eight other entropy estimation procedures across a\ndiverse range of sampling scenarios and data-generating models, even in cases\nof severe undersampling. We illustrate the approach by analyzing E. coli gene\nexpression data and computing an entropy-based gene-association network from\ngene expression data. A computer program is available that implements the\nproposed shrinkage estimator.\n","vector":null,"chunk_id":"e7bc65c2b755e32aa71fb15506d0af01"}
{"title":"Random Forests: some methodological insights","authors":"Robin Genuer (LM-Orsay), Jean-Michel Poggi (LM-Orsay), Christine\n  Tuleau (JAD)","category":"stat.ML","abstract":"  This paper examines from an experimental perspective random forests, the\nincreasingly used statistical method for classification and regression problems\nintroduced by Leo Breiman in 2001. It first aims at confirming, known but\nsparse, advice for using random forests and at proposing some complementary\nremarks for both standard problems as well as high dimensional ones for which\nthe number of variables hugely exceeds the sample size. But the main\ncontribution of this paper is twofold: to provide some insights about the\nbehavior of the variable importance index based on random forests and in\naddition, to propose to investigate two classical issues of variable selection.\nThe first one is to find important variables for interpretation and the second\none is more restrictive and try to design a good prediction model. The strategy\ninvolves a ranking of explanatory variables using the random forests score of\nimportance and a stepwise ascending variable introduction strategy.\n","text":"Title:Random Forests: some methodological insights\nAbstract:  This paper examines from an experimental perspective random forests, the\nincreasingly used statistical method for classification and regression problems\nintroduced by Leo Breiman in 2001. It first aims at confirming, known but\nsparse, advice for using random forests and at proposing some complementary\nremarks for both standard problems as well as high dimensional ones for which\nthe number of variables hugely exceeds the sample size. But the main\ncontribution of this paper is twofold: to provide some insights about the\nbehavior of the variable importance index based on random forests and in\naddition, to propose to investigate two classical issues of variable selection.\nThe first one is to find important variables for interpretation and the second\none is more restrictive and try to design a good prediction model. The strategy\ninvolves a ranking of explanatory variables using the random forests score of\nimportance and a stepwise ascending variable introduction strategy.\n","vector":null,"chunk_id":"9168947aaf7ed622775a0c446365209c"}
{"title":"An information-theoretic derivation of min-cut based clustering","authors":"Anil Raj and Chris H. Wiggins","category":"stat.ML","abstract":"  Min-cut clustering, based on minimizing one of two heuristic cost-functions\nproposed by Shi and Malik, has spawned tremendous research, both analytic and\nalgorithmic, in the graph partitioning and image segmentation communities over\nthe last decade. It is however unclear if these heuristics can be derived from\na more general principle facilitating generalization to new problem settings.\nMotivated by an existing graph partitioning framework, we derive relationships\nbetween optimizing relevance information, as defined in the Information\nBottleneck method, and the regularized cut in a K-partitioned graph. For fast\nmixing graphs, we show that the cost functions introduced by Shi and Malik can\nbe well approximated as the rate of loss of predictive information about the\nlocation of random walkers on the graph. For graphs generated from a stochastic\nalgorithm designed to model community structure, the optimal information\ntheoretic partition and the optimal min-cut partition are shown to be the same\nwith high probability.\n","text":"Title:An information-theoretic derivation of min-cut based clustering\nAbstract:  Min-cut clustering, based on minimizing one of two heuristic cost-functions\nproposed by Shi and Malik, has spawned tremendous research, both analytic and\nalgorithmic, in the graph partitioning and image segmentation communities over\nthe last decade. It is however unclear if these heuristics can be derived from\na more general principle facilitating generalization to new problem settings.\nMotivated by an existing graph partitioning framework, we derive relationships\nbetween optimizing relevance information, as defined in the Information\nBottleneck method, and the regularized cut in a K-partitioned graph. For fast\nmixing graphs, we show that the cost functions introduced by Shi and Malik can\nbe well approximated as the rate of loss of predictive information about the\nlocation of random walkers on the graph. For graphs generated from a stochastic\nalgorithm designed to model community structure, the optimal information\ntheoretic partition and the optimal min-cut partition are shown to be the same\nwith high probability.\n","vector":null,"chunk_id":"fbce1b2fc5c0fd0eb1cfd9f36c0a79ab"}
{"title":"Conceptual approach through an annotation process for the representation\n  and the information contents enhancement in economic intelligence (EI)","authors":"Sahbi Sidhom (LORIA, Sii)","category":"cs.IR","abstract":"  In the era of the information society, the impact of the information systems\non the economy of material and immaterial is certainly perceptible. With\nregards to the information resources of an organization, the annotation\ninvolved to enrich informational content, to track the intellectual activities\non a document and to set the added value on information for the benefit of\nsolving a decision-making problem in the context of economic intelligence. Our\ncontribution is distinguished by the representation of an annotation process\nand its inherent concepts to lead the decisionmaker to an anticipated decision:\nthe provision of relevant and annotated information. Such information in the\nsystem is made easy by taking into account the diversity of resources and those\nthat are well annotated so formally and informally by the EI actors. A capital\nresearch framework consist of integrating in the decision-making process the\nannotator activity, the software agent (or the reasoning mechanisms) and the\ninformation resources enhancement.\n","text":"Title:Conceptual approach through an annotation process for the representation\n  and the information contents enhancement in economic intelligence (EI)\nAbstract:  In the era of the information society, the impact of the information systems\non the economy of material and immaterial is certainly perceptible. With\nregards to the information resources of an organization, the annotation\ninvolved to enrich informational content, to track the intellectual activities\non a document and to set the added value on information for the benefit of\nsolving a decision-making problem in the context of economic intelligence. Our\ncontribution is distinguished by the representation of an annotation process\nand its inherent concepts to lead the decisionmaker to an anticipated decision:\nthe provision of relevant and annotated information. Such information in the\nsystem is made easy by taking into account the diversity of resources and those\nthat are well annotated so formally and informally by the EI actors. A capital\nresearch framework consist of integrating in the decision-making process the\nannotator activity, the software agent (or the reasoning mechanisms) and the\ninformation resources enhancement.\n","vector":null,"chunk_id":"12540a3ff138fa4a2623c934681ae5a8"}
{"title":"Missing Data using Decision Forest and Computational Intelligence","authors":"D. Moon and T. Marwala","category":"stat.ML","abstract":"  Autoencoder neural network is implemented to estimate the missing data.\nGenetic algorithm is implemented for network optimization and estimating the\nmissing data. Missing data is treated as Missing At Random mechanism by\nimplementing maximum likelihood algorithm. The network performance is\ndetermined by calculating the mean square error of the network prediction. The\nnetwork is further optimized by implementing Decision Forest. The impact of\nmissing data is then investigated and decision forrests are found to improve\nthe results.\n","text":"Title:Missing Data using Decision Forest and Computational Intelligence\nAbstract:  Autoencoder neural network is implemented to estimate the missing data.\nGenetic algorithm is implemented for network optimization and estimating the\nmissing data. Missing data is treated as Missing At Random mechanism by\nimplementing maximum likelihood algorithm. The network performance is\ndetermined by calculating the mean square error of the network prediction. The\nnetwork is further optimized by implementing Decision Forest. The impact of\nmissing data is then investigated and decision forrests are found to improve\nthe results.\n","vector":null,"chunk_id":"cebc2446fb3a7817e6bd57d9069e2ff7"}
{"title":"Prediction with Restricted Resources and Finite Automata","authors":"Finn Macleod, James Gleeson","category":"stat.ML","abstract":"  We obtain an index of the complexity of a random sequence by allowing the\nrole of the measure in classical probability theory to be played by a function\nwe call the generating mechanism. Typically, this generating mechanism will be\na finite automata. We generate a set of biased sequences by applying a finite\nstate automata with a specified number, $m$, of states to the set of all binary\nsequences. Thus we can index the complexity of our random sequence by the\nnumber of states of the automata. We detail optimal algorithms to predict\nsequences generated in this way.\n","text":"Title:Prediction with Restricted Resources and Finite Automata\nAbstract:  We obtain an index of the complexity of a random sequence by allowing the\nrole of the measure in classical probability theory to be played by a function\nwe call the generating mechanism. Typically, this generating mechanism will be\na finite automata. We generate a set of biased sequences by applying a finite\nstate automata with a specified number, $m$, of states to the set of all binary\nsequences. Thus we can index the complexity of our random sequence by the\nnumber of states of the automata. We detail optimal algorithms to predict\nsequences generated in this way.\n","vector":null,"chunk_id":"0536d60a7e2cdce1d0e2b29d76af7aed"}
{"title":"Mining User Profiles to Support Structure and Explanation in Open Social\n  Networking","authors":"Avare Stewart, Ernesto Diaz-Aviles, and Wolfgang Nejdl","category":"cs.IR","abstract":"  The proliferation of media sharing and social networking websites has brought\nwith it vast collections of site-specific user generated content. The result is\na Social Networking Divide in which the concepts and structure common across\ndifferent sites are hidden. The knowledge and structures from one social site\nare not adequately exploited to provide new information and resources to the\nsame or different users in comparable social sites. For music bloggers, this\nlatent structure, forces bloggers to select sub-optimal blogrolls. However, by\nintegrating the social activities of music bloggers and listeners, we are able\nto overcome this limitation: improving the quality of the blogroll\nneighborhoods, in terms of similarity, by 85 percent when using tracks and by\n120 percent when integrating tags from another site.\n","text":"Title:Mining User Profiles to Support Structure and Explanation in Open Social\n  Networking\nAbstract:  The proliferation of media sharing and social networking websites has brought\nwith it vast collections of site-specific user generated content. The result is\na Social Networking Divide in which the concepts and structure common across\ndifferent sites are hidden. The knowledge and structures from one social site\nare not adequately exploited to provide new information and resources to the\nsame or different users in comparable social sites. For music bloggers, this\nlatent structure, forces bloggers to select sub-optimal blogrolls. However, by\nintegrating the social activities of music bloggers and listeners, we are able\nto overcome this limitation: improving the quality of the blogroll\nneighborhoods, in terms of similarity, by 85 percent when using tracks and by\n120 percent when integrating tags from another site.\n","vector":null,"chunk_id":"974bbb4344d3b5551153ce5569ace490"}
{"title":"On the Geometry of Discrete Exponential Families with Application to\n  Exponential Random Graph Models","authors":"Stephen E. Fienberg, Alessandro Rinaldo, Yi Zhou","category":"stat.ML","abstract":"  There has been an explosion of interest in statistical models for analyzing\nnetwork data, and considerable interest in the class of exponential random\ngraph (ERG) models, especially in connection with difficulties in computing\nmaximum likelihood estimates. The issues associated with these difficulties\nrelate to the broader structure of discrete exponential families. This paper\nre-examines the issues in two parts. First we consider the closure of\n$k$-dimensional exponential families of distribution with discrete base measure\nand polyhedral convex support $\\mathrm{P}$. We show that the normal fan of\n$\\mathrm{P}$ is a geometric object that plays a fundamental role in deriving\nthe statistical and geometric properties of the corresponding extended\nexponential families. We discuss its relevance to maximum likelihood\nestimation, both from a theoretical and computational standpoint. Second, we\napply our results to the analysis of ERG models. In particular, by means of a\ndetailed example, we provide some characterization of the properties of ERG\nmodels, and, in particular, of certain behaviors of ERG models known as\ndegeneracy.\n","text":"Title:On the Geometry of Discrete Exponential Families with Application to\n  Exponential Random Graph Models\nAbstract:  There has been an explosion of interest in statistical models for analyzing\nnetwork data, and considerable interest in the class of exponential random\ngraph (ERG) models, especially in connection with difficulties in computing\nmaximum likelihood estimates. The issues associated with these difficulties\nrelate to the broader structure of discrete exponential families. This paper\nre-examines the issues in two parts. First we consider the closure of\n$k$-dimensional exponential families of distribution with discrete base measure\nand polyhedral convex support $\\mathrm{P}$. We show that the normal fan of\n$\\mathrm{P}$ is a geometric object that plays a fundamental role in deriving\nthe statistical and geometric properties of the corresponding extended\nexponential families. We discuss its relevance to maximum likelihood\nestimation, both from a theoretical and computational standpoint. Second, we\napply our results to the analysis of ERG models. In particular, by means of a\ndetailed example, we provide some characterization of the properties of ERG\nmodels, and, in particular, of certain behaviors of ERG models known as\ndegeneracy.\n","vector":null,"chunk_id":"26dd8ffe2325296127b540a2e1363a7a"}
{"title":"Weighted Naive Bayes Model for Semi-Structured Document Categorization","authors":"Pierre-Fran\\c{c}ois Marteau (VALORIA), Gilbas M\\'enier (VALORIA),\n  Eugen Popovici (VALORIA)","category":"cs.IR","abstract":"  The aim of this paper is the supervised classification of semi-structured\ndata. A formal model based on bayesian classification is developed while\naddressing the integration of the document structure into classification tasks.\nWe define what we call the structural context of occurrence for unstructured\ndata, and we derive a recursive formulation in which parameters are used to\nweight the contribution of structural element relatively to the others. A\nsimplified version of this formal model is implemented to carry out textual\ndocuments classification experiments. First results show, for a adhoc weighting\nstrategy, that the structural context of word occurrences has a significant\nimpact on classification results comparing to the performance of a simple\nmultinomial naive Bayes classifier. The proposed implementation competes on the\nReuters-21578 data with the SVM classifier associated or not with the splitting\nof structural components. These results encourage exploring the learning of\nacceptable weighting strategies for this model, in particular boosting\nstrategies.\n","text":"Title:Weighted Naive Bayes Model for Semi-Structured Document Categorization\nAbstract:  The aim of this paper is the supervised classification of semi-structured\ndata. A formal model based on bayesian classification is developed while\naddressing the integration of the document structure into classification tasks.\nWe define what we call the structural context of occurrence for unstructured\ndata, and we derive a recursive formulation in which parameters are used to\nweight the contribution of structural element relatively to the others. A\nsimplified version of this formal model is implemented to carry out textual\ndocuments classification experiments. First results show, for a adhoc weighting\nstrategy, that the structural context of word occurrences has a significant\nimpact on classification results comparing to the performance of a simple\nmultinomial naive Bayes classifier. The proposed implementation competes on the\nReuters-21578 data with the SVM classifier associated or not with the splitting\nof structural components. These results encourage exploring the learning of\nacceptable weighting strategies for this model, in particular boosting\nstrategies.\n","vector":null,"chunk_id":"0d2f819215baca01b5d845c0a436d346"}
{"title":"Reconstruction of Epsilon-Machines in Predictive Frameworks and\n  Decisional States","authors":"Nicolas Brodu","category":"stat.ML","abstract":"  This article introduces both a new algorithm for reconstructing\nepsilon-machines from data, as well as the decisional states. These are defined\nas the internal states of a system that lead to the same decision, based on a\nuser-provided utility or pay-off function. The utility function encodes some a\npriori knowledge external to the system, it quantifies how bad it is to make\nmistakes. The intrinsic underlying structure of the system is modeled by an\nepsilon-machine and its causal states. The decisional states form a partition\nof the lower-level causal states that is defined according to the higher-level\nuser's knowledge. In a complex systems perspective, the decisional states are\nthus the \"emerging\" patterns corresponding to the utility function. The\ntransitions between these decisional states correspond to events that lead to a\nchange of decision. The new REMAPF algorithm estimates both the epsilon-machine\nand the decisional states from data. Application examples are given for hidden\nmodel reconstruction, cellular automata filtering, and edge detection in\nimages.\n","text":"Title:Reconstruction of Epsilon-Machines in Predictive Frameworks and\n  Decisional States\nAbstract:  This article introduces both a new algorithm for reconstructing\nepsilon-machines from data, as well as the decisional states. These are defined\nas the internal states of a system that lead to the same decision, based on a\nuser-provided utility or pay-off function. The utility function encodes some a\npriori knowledge external to the system, it quantifies how bad it is to make\nmistakes. The intrinsic underlying structure of the system is modeled by an\nepsilon-machine and its causal states. The decisional states form a partition\nof the lower-level causal states that is defined according to the higher-level\nuser's knowledge. In a complex systems perspective, the decisional states are\nthus the \"emerging\" patterns corresponding to the utility function. The\ntransitions between these decisional states correspond to events that lead to a\nchange of decision. The new REMAPF algorithm estimates both the epsilon-machine\nand the decisional states from data. Application examples are given for hidden\nmodel reconstruction, cellular automata filtering, and edge detection in\nimages.\n","vector":null,"chunk_id":"2dcd8a0ec9715b28745c79bd1629302c"}
{"title":"Lanczos Approximations for the Speedup of Kernel Partial Least Squares\n  Regression","authors":"Nicole Kraemer, Masashi Sugiyama, Mikio Braun","category":"stat.ML","abstract":"  The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is\nquadratic in the number of examples. However, the necessity of obtaining\nsensitivity measures as degrees of freedom for model selection or confidence\nintervals for more detailed analysis requires cubic runtime, and thus\nconstitutes a computational bottleneck in real-world data analysis. We propose\na novel algorithm for KPLS which not only computes (a) the fit, but also (b)\nits approximate degrees of freedom and (c) error bars in quadratic runtime. The\nalgorithm exploits a close connection between Kernel PLS and the Lanczos\nalgorithm for approximating the eigenvalues of symmetric matrices, and uses\nthis approximation to compute the trace of powers of the kernel matrix in\nquadratic runtime.\n","text":"Title:Lanczos Approximations for the Speedup of Kernel Partial Least Squares\n  Regression\nAbstract:  The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is\nquadratic in the number of examples. However, the necessity of obtaining\nsensitivity measures as degrees of freedom for model selection or confidence\nintervals for more detailed analysis requires cubic runtime, and thus\nconstitutes a computational bottleneck in real-world data analysis. We propose\na novel algorithm for KPLS which not only computes (a) the fit, but also (b)\nits approximate degrees of freedom and (c) error bars in quadratic runtime. The\nalgorithm exploits a close connection between Kernel PLS and the Lanczos\nalgorithm for approximating the eigenvalues of symmetric matrices, and uses\nthis approximation to compute the trace of powers of the kernel matrix in\nquadratic runtime.\n","vector":null,"chunk_id":"b153966399ff2269e9e25513e86c6daf"}
{"title":"Escaping the curse of dimensionality with a tree-based regressor","authors":"Samory Kpotufe","category":"stat.ML","abstract":"  We present the first tree-based regressor whose convergence rate depends only\non the intrinsic dimension of the data, namely its Assouad dimension. The\nregressor uses the RPtree partitioning procedure, a simple randomized variant\nof k-d trees.\n","text":"Title:Escaping the curse of dimensionality with a tree-based regressor\nAbstract:  We present the first tree-based regressor whose convergence rate depends only\non the intrinsic dimension of the data, namely its Assouad dimension. The\nregressor uses the RPtree partitioning procedure, a simple randomized variant\nof k-d trees.\n","vector":null,"chunk_id":"c4404dad0707f9c0368b7090887e76ec"}
{"title":"Document Relevance Evaluation via Term Distribution Analysis Using\n  Fourier Series Expansion","authors":"Patricio Galeas (1), Ralph Kretschmer (2), Bernd Freisleben (1) ((1)\n  University of Marburg, Germany, (2) Kretschmer Software, Siegen, Germany)","category":"cs.IR","abstract":"  In addition to the frequency of terms in a document collection, the\ndistribution of terms plays an important role in determining the relevance of\ndocuments for a given search query. In this paper, term distribution analysis\nusing Fourier series expansion as a novel approach for calculating an abstract\nrepresentation of term positions in a document corpus is introduced. Based on\nthis approach, two methods for improving the evaluation of document relevance\nare proposed: (a) a function-based ranking optimization representing a user\ndefined document region, and (b) a query expansion technique based on\noverlapping the term distributions in the top-ranked documents. Experimental\nresults demonstrate the effectiveness of the proposed approach in providing new\npossibilities for optimizing the retrieval process.\n","text":"Title:Document Relevance Evaluation via Term Distribution Analysis Using\n  Fourier Series Expansion\nAbstract:  In addition to the frequency of terms in a document collection, the\ndistribution of terms plays an important role in determining the relevance of\ndocuments for a given search query. In this paper, term distribution analysis\nusing Fourier series expansion as a novel approach for calculating an abstract\nrepresentation of term positions in a document corpus is introduced. Based on\nthis approach, two methods for improving the evaluation of document relevance\nare proposed: (a) a function-based ranking optimization representing a user\ndefined document region, and (b) a query expansion technique based on\noverlapping the term distributions in the top-ranked documents. Experimental\nresults demonstrate the effectiveness of the proposed approach in providing new\npossibilities for optimizing the retrieval process.\n","vector":null,"chunk_id":"1b56de1fadc10d0623d01796febb04c8"}
{"title":"The Nonparanormal: Semiparametric Estimation of High Dimensional\n  Undirected Graphs","authors":"Han Liu, John Lafferty and Larry Wasserman","category":"stat.ML","abstract":"  Recent methods for estimating sparse undirected graphs for real-valued data\nin high dimensional problems rely heavily on the assumption of normality. We\nshow how to use a semiparametric Gaussian copula--or \"nonparanormal\"--for high\ndimensional inference. Just as additive models extend linear models by\nreplacing linear functions with a set of one-dimensional smooth functions, the\nnonparanormal extends the normal by transforming the variables by smooth\nfunctions. We derive a method for estimating the nonparanormal, study the\nmethod's theoretical properties, and show that it works well in many examples.\n","text":"Title:The Nonparanormal: Semiparametric Estimation of High Dimensional\n  Undirected Graphs\nAbstract:  Recent methods for estimating sparse undirected graphs for real-valued data\nin high dimensional problems rely heavily on the assumption of normality. We\nshow how to use a semiparametric Gaussian copula--or \"nonparanormal\"--for high\ndimensional inference. Just as additive models extend linear models by\nreplacing linear functions with a set of one-dimensional smooth functions, the\nnonparanormal extends the normal by transforming the variables by smooth\nfunctions. We derive a method for estimating the nonparanormal, study the\nmethod's theoretical properties, and show that it works well in many examples.\n","vector":null,"chunk_id":"d9619eecd90495506ac89deaf64597e5"}
{"title":"To Click or not to Click? The Role of Contextualized and User-Centric\n  Web Snippets","authors":"N. Zotos, P. Tzekou, G. Tsatsaronis, L. Kozanidis, S. Stamou, I.\n  Varlamis","category":"cs.IR","abstract":"  When searching the web, it is often possible that there are too many results\navailable for ambiguous queries. Text snippets, extracted from the retrieved\npages, are an indicator of the pages' usefulness to the query intention and can\nbe used to focus the scope of search results. In this paper, we propose a novel\nmethod for automatically extracting web page snippets that are highly relevant\nto the query intention and expressive of the pages' entire content. We show\nthat the usage of semantics, as a basis for focused retrieval, produces high\nquality text snippet suggestions. The snippets delivered by our method are\nsignificantly better in terms of retrieval performance compared to those\nderived using the pages' statistical content. Furthermore, our study suggests\nthat semantically-driven snippet generation can also be used to augment\ntraditional passage retrieval algorithms based on word overlap or statistical\nweights, since they typically differ in coverage and produce different results.\nUser clicks on the query relevant snippets can be used to refine the query\nresults and promote the most comprehensive among the relevant documents.\n","text":"Title:To Click or not to Click? The Role of Contextualized and User-Centric\n  Web Snippets\nAbstract:  When searching the web, it is often possible that there are too many results\navailable for ambiguous queries. Text snippets, extracted from the retrieved\npages, are an indicator of the pages' usefulness to the query intention and can\nbe used to focus the scope of search results. In this paper, we propose a novel\nmethod for automatically extracting web page snippets that are highly relevant\nto the query intention and expressive of the pages' entire content. We show\nthat the usage of semantics, as a basis for focused retrieval, produces high\nquality text snippet suggestions. The snippets delivered by our method are\nsignificantly better in terms of retrieval performance compared to those\nderived using the pages' statistical content. Furthermore, our study suggests\nthat semantically-driven snippet generation can also be used to augment\ntraditional passage retrieval algorithms based on word overlap or statistical\nweights, since they typically differ in coverage and produce different results.\nUser clicks on the query relevant snippets can be used to refine the query\nresults and promote the most comprehensive among the relevant documents.\n","vector":null,"chunk_id":"79b070ce438599bc99dcb0568b312890"}
{"title":"BLOGRANK: Ranking Weblogs Based On Connectivity And Similarity Features","authors":"A. Kritikopoulos, M. Sideri, I. Varlamis","category":"cs.IR","abstract":"  A large part of the hidden web resides in weblog servers. New content is\nproduced in a daily basis and the work of traditional search engines turns to\nbe insufficient due to the nature of weblogs. This work summarizes the\nstructure of the blogosphere and highlights the special features of weblogs. In\nthis paper we present a method for ranking weblogs based on the link graph and\non several similarity characteristics between weblogs. First we create an\nenhanced graph of connected weblogs and add new types of edges and weights\nutilising many weblog features. Then, we assign a ranking to each weblog using\nour algorithm, BlogRank, which is a modified version of PageRank. For the\nvalidation of our method we run experiments on a weblog dataset, which we\nprocess and adapt to our search engine. (http://spiderwave.aueb.gr/Blogwave).\nThe results suggest that the use of the enhanced graph and the BlogRank\nalgorithm is preferred by the users.\n","text":"Title:BLOGRANK: Ranking Weblogs Based On Connectivity And Similarity Features\nAbstract:  A large part of the hidden web resides in weblog servers. New content is\nproduced in a daily basis and the work of traditional search engines turns to\nbe insufficient due to the nature of weblogs. This work summarizes the\nstructure of the blogosphere and highlights the special features of weblogs. In\nthis paper we present a method for ranking weblogs based on the link graph and\non several similarity characteristics between weblogs. First we create an\nenhanced graph of connected weblogs and add new types of edges and weights\nutilising many weblog features. Then, we assign a ranking to each weblog using\nour algorithm, BlogRank, which is a modified version of PageRank. For the\nvalidation of our method we run experiments on a weblog dataset, which we\nprocess and adapt to our search engine. (http://spiderwave.aueb.gr/Blogwave).\nThe results suggest that the use of the enhanced graph and the BlogRank\nalgorithm is preferred by the users.\n","vector":null,"chunk_id":"ee265d4d3ca85968c92199521dc026e8"}
{"title":"Finding Exogenous Variables in Data with Many More Variables than\n  Observations","authors":"Shohei Shimizu, Takashi Washio, Aapo Hyvarinen, Seiya Imoto","category":"stat.ML","abstract":"  Many statistical methods have been proposed to estimate causal models in\nclassical situations with fewer variables than observations (p<n, p: the number\nof variables and n: the number of observations). However, modern datasets\nincluding gene expression data need high-dimensional causal modeling in\nchallenging situations with orders of magnitude more variables than\nobservations (p>>n). In this paper, we propose a method to find exogenous\nvariables in a linear non-Gaussian causal model, which requires much smaller\nsample sizes than conventional methods and works even when p>>n. The key idea\nis to identify which variables are exogenous based on non-Gaussianity instead\nof estimating the entire structure of the model. Exogenous variables work as\ntriggers that activate a causal chain in the model, and their identification\nleads to more efficient experimental designs and better understanding of the\ncausal mechanism. We present experiments with artificial data and real-world\ngene expression data to evaluate the method.\n","text":"Title:Finding Exogenous Variables in Data with Many More Variables than\n  Observations\nAbstract:  Many statistical methods have been proposed to estimate causal models in\nclassical situations with fewer variables than observations (p<n, p: the number\nof variables and n: the number of observations). However, modern datasets\nincluding gene expression data need high-dimensional causal modeling in\nchallenging situations with orders of magnitude more variables than\nobservations (p>>n). In this paper, we propose a method to find exogenous\nvariables in a linear non-Gaussian causal model, which requires much smaller\nsample sizes than conventional methods and works even when p>>n. The key idea\nis to identify which variables are exogenous based on non-Gaussianity instead\nof estimating the entire structure of the model. Exogenous variables work as\ntriggers that activate a causal chain in the model, and their identification\nleads to more efficient experimental designs and better understanding of the\ncausal mechanism. We present experiments with artificial data and real-world\ngene expression data to evaluate the method.\n","vector":null,"chunk_id":"2da632779c7f21e424bcb87145e13b96"}
{"title":"Approche conceptuelle par un processus d'annotation pour la\n  repr\\'esentation et la valorisation de contenus informationnels en\n  intelligence \\'economique (IE)","authors":"Sahbi Sidhom (LORIA)","category":"cs.IR","abstract":"  In the era of the information society, the impact of the information systems\non the economy of material and immaterial is certainly perceptible. With\nregards to the information resources of an organization, the annotation\ninvolved to enrich informational content, to track the intellectual activities\non a document and to set the added value on information for the benefit of\nsolving a decision-making problem in the context of economic intelligence. Our\ncontribution is distinguished by the representation of an annotation process\nand its inherent concepts to lead the decisionmaker to an anticipated decision:\nthe provision of relevant and annotated information. Such information in the\nsystem is made easy by taking into account the diversity of resources and those\nthat are well annotated so formally and informally by the EI actors. A capital\nresearch framework consist of integrating in the decision-making process the\nannotator activity, the software agent (or the reasoning mechanisms) and the\ninformation resources enhancement.\n","text":"Title:Approche conceptuelle par un processus d'annotation pour la\n  repr\\'esentation et la valorisation de contenus informationnels en\n  intelligence \\'economique (IE)\nAbstract:  In the era of the information society, the impact of the information systems\non the economy of material and immaterial is certainly perceptible. With\nregards to the information resources of an organization, the annotation\ninvolved to enrich informational content, to track the intellectual activities\non a document and to set the added value on information for the benefit of\nsolving a decision-making problem in the context of economic intelligence. Our\ncontribution is distinguished by the representation of an annotation process\nand its inherent concepts to lead the decisionmaker to an anticipated decision:\nthe provision of relevant and annotated information. Such information in the\nsystem is made easy by taking into account the diversity of resources and those\nthat are well annotated so formally and informally by the EI actors. A capital\nresearch framework consist of integrating in the decision-making process the\nannotator activity, the software agent (or the reasoning mechanisms) and the\ninformation resources enhancement.\n","vector":null,"chunk_id":"4c5f6b53c2a90b475cf2c267e252bae8"}
{"title":"Personalized Recommendation via Integrated Diffusion on User-Item-Tag\n  Tripartite Graphs","authors":"Zi-Ke Zhang, Tao Zhou, Yi-Cheng Zhang","category":"cs.IR","abstract":"  Personalized recommender systems are confronting great challenges of\naccuracy, diversification and novelty, especially when the data set is sparse\nand lacks accessorial information, such as user profiles, item attributes and\nexplicit ratings. Collaborative tags contain rich information about\npersonalized preferences and item contents, and are therefore potential to help\nin providing better recommendations. In this paper, we propose a recommendation\nalgorithm based on an integrated diffusion on user-item-tag tripartite graphs.\nWe use three benchmark data sets, Del.icio.us, MovieLens and BibSonomy, to\nevaluate our algorithm. Experimental results demonstrate that the usage of tag\ninformation can significantly improve accuracy, diversification and novelty of\nrecommendations.\n","text":"Title:Personalized Recommendation via Integrated Diffusion on User-Item-Tag\n  Tripartite Graphs\nAbstract:  Personalized recommender systems are confronting great challenges of\naccuracy, diversification and novelty, especially when the data set is sparse\nand lacks accessorial information, such as user profiles, item attributes and\nexplicit ratings. Collaborative tags contain rich information about\npersonalized preferences and item contents, and are therefore potential to help\nin providing better recommendations. In this paper, we propose a recommendation\nalgorithm based on an integrated diffusion on user-item-tag tripartite graphs.\nWe use three benchmark data sets, Del.icio.us, MovieLens and BibSonomy, to\nevaluate our algorithm. Experimental results demonstrate that the usage of tag\ninformation can significantly improve accuracy, diversification and novelty of\nrecommendations.\n","vector":null,"chunk_id":"ad2082e54cb305d56007ca92695f4fb4"}
{"title":"Structured Variable Selection with Sparsity-Inducing Norms","authors":"Rodolphe Jenatton (INRIA Rocquencourt), Jean-Yves Audibert (INRIA\n  Rocquencourt), Francis Bach (INRIA Rocquencourt)","category":"stat.ML","abstract":"  We consider the empirical risk minimization problem for linear supervised\nlearning, with regularization by structured sparsity-inducing norms. These are\ndefined as sums of Euclidean norms on certain subsets of variables, extending\nthe usual $\\ell_1$-norm and the group $\\ell_1$-norm by allowing the subsets to\noverlap. This leads to a specific set of allowed nonzero patterns for the\nsolutions of such problems. We first explore the relationship between the\ngroups defining the norm and the resulting nonzero patterns, providing both\nforward and backward algorithms to go back and forth from groups to patterns.\nThis allows the design of norms adapted to specific prior knowledge expressed\nin terms of nonzero patterns. We also present an efficient active set\nalgorithm, and analyze the consistency of variable selection for least-squares\nlinear regression in low and high-dimensional settings.\n","text":"Title:Structured Variable Selection with Sparsity-Inducing Norms\nAbstract:  We consider the empirical risk minimization problem for linear supervised\nlearning, with regularization by structured sparsity-inducing norms. These are\ndefined as sums of Euclidean norms on certain subsets of variables, extending\nthe usual $\\ell_1$-norm and the group $\\ell_1$-norm by allowing the subsets to\noverlap. This leads to a specific set of allowed nonzero patterns for the\nsolutions of such problems. We first explore the relationship between the\ngroups defining the norm and the resulting nonzero patterns, providing both\nforward and backward algorithms to go back and forth from groups to patterns.\nThis allows the design of norms adapted to specific prior knowledge expressed\nin terms of nonzero patterns. We also present an efficient active set\nalgorithm, and analyze the consistency of variable selection for least-squares\nlinear regression in low and high-dimensional settings.\n","vector":null,"chunk_id":"d79c45b2778772a932211ec91afbc197"}
{"title":"Supplementary material for Markov equivalence for ancestral graphs","authors":"R. A. Ali, T. Richardson, and P. Spirtes","category":"stat.ML","abstract":"  We prove that the criterion for Markov equivalence provided by Zhao et al.\n(2005) may involve a set of features of a graph that is exponential in the\nnumber of vertices.\n","text":"Title:Supplementary material for Markov equivalence for ancestral graphs\nAbstract:  We prove that the criterion for Markov equivalence provided by Zhao et al.\n(2005) may involve a set of features of a graph that is exponential in the\nnumber of vertices.\n","vector":null,"chunk_id":"52774dbeb56bee3f7123ad5e1469d16a"}
{"title":"A more robust boosting algorithm","authors":"Yoav Freund","category":"stat.ML","abstract":"  We present a new boosting algorithm, motivated by the large margins theory\nfor boosting. We give experimental evidence that the new algorithm is\nsignificantly more robust against label noise than existing boosting algorithm.\n","text":"Title:A more robust boosting algorithm\nAbstract:  We present a new boosting algorithm, motivated by the large margins theory\nfor boosting. We give experimental evidence that the new algorithm is\nsignificantly more robust against label noise than existing boosting algorithm.\n","vector":null,"chunk_id":"355dc7fbe85177cef29662de5afa9e03"}
{"title":"Macrodynamics of users' behavior in Information Retrieval","authors":"Daniel Sonntag, Rom\\`an R. Zapatrin","category":"cs.IR","abstract":"  We present a method to geometrize massive data sets from search engines query\nlogs. For this purpose, a macrodynamic-like quantitative model of the\nInformation Retrieval (IR) process is developed, whose paradigm is inspired by\nbasic constructions of Einstein's general relativity theory in which all IR\nobjects are uniformly placed in a common Room. The Room has a structure similar\nto Einsteinian spacetime, namely that of a smooth manifold. Documents and\nqueries are treated as matter objects and sources of material fields.\nRelevance, the central notion of IR, becomes a dynamical issue controlled by\nboth gravitation (or, more precisely, as the motion in a curved spacetime) and\nforces originating from the interactions of matter fields. The spatio-temporal\ndescription ascribes dynamics to any document or query, thus providing a\nuniform description for documents of both initially static and dynamical\nnature. Within the IR context, the techniques presented are based on two ideas.\nThe first is the placement of all objects participating in IR into a common\ncontinuous space. The second idea is the `objectivization' of the IR process;\ninstead of expressing users' wishes, we consider the overall IR as an objective\nphysical process, representing the IR process in terms of motion in a given\nexternal-fields configuration. Various semantic environments are treated as\nvarious IR universes.\n","text":"Title:Macrodynamics of users' behavior in Information Retrieval\nAbstract:  We present a method to geometrize massive data sets from search engines query\nlogs. For this purpose, a macrodynamic-like quantitative model of the\nInformation Retrieval (IR) process is developed, whose paradigm is inspired by\nbasic constructions of Einstein's general relativity theory in which all IR\nobjects are uniformly placed in a common Room. The Room has a structure similar\nto Einsteinian spacetime, namely that of a smooth manifold. Documents and\nqueries are treated as matter objects and sources of material fields.\nRelevance, the central notion of IR, becomes a dynamical issue controlled by\nboth gravitation (or, more precisely, as the motion in a curved spacetime) and\nforces originating from the interactions of matter fields. The spatio-temporal\ndescription ascribes dynamics to any document or query, thus providing a\nuniform description for documents of both initially static and dynamical\nnature. Within the IR context, the techniques presented are based on two ideas.\nThe first is the placement of all objects participating in IR into a common\ncontinuous space. The second idea is the `objectivization' of the IR process;\ninstead of expressing users' wishes, we consider the overall IR as an objective\nphysical process, representing the IR process in terms of motion in a given\nexternal-fields configuration. Various semantic environments are treated as\nvarious IR universes.\n","vector":null,"chunk_id":"d0a32ecd002808772a488faed720da74"}
{"title":"Google matrix, dynamical attractors and Ulam networks","authors":"D.L. Shepelyansky and O.V. Zhirov (CNRS, Toulouse & BINP, Novosibirsk)","category":"cs.IR","abstract":"  We study the properties of the Google matrix generated by a coarse-grained\nPerron-Frobenius operator of the Chirikov typical map with dissipation. The\nfinite size matrix approximant of this operator is constructed by the Ulam\nmethod. This method applied to the simple dynamical model creates the directed\nUlam networks with approximate scale-free scaling and characteristics being\nrather similar to those of the World Wide Web. The simple dynamical attractors\nplay here the role of popular web sites with a strong concentration of\nPageRank. A variation of the Google parameter $\\alpha$ or other parameters of\nthe dynamical map can drive the PageRank of the Google matrix to a delocalized\nphase with a strange attractor where the Google search becomes inefficient.\n","text":"Title:Google matrix, dynamical attractors and Ulam networks\nAbstract:  We study the properties of the Google matrix generated by a coarse-grained\nPerron-Frobenius operator of the Chirikov typical map with dissipation. The\nfinite size matrix approximant of this operator is constructed by the Ulam\nmethod. This method applied to the simple dynamical model creates the directed\nUlam networks with approximate scale-free scaling and characteristics being\nrather similar to those of the World Wide Web. The simple dynamical attractors\nplay here the role of popular web sites with a strong concentration of\nPageRank. A variation of the Google parameter $\\alpha$ or other parameters of\nthe dynamical map can drive the PageRank of the Google matrix to a delocalized\nphase with a strange attractor where the Google search becomes inefficient.\n","vector":null,"chunk_id":"03a1dac891d6bcedefee861060f2cb02"}
{"title":"Collaborative filtering based on multi-channel diffusion","authors":"Ming-Sheng Shang, Ci-Hang Jin, Tao Zhou, Yi-Cheng Zhang","category":"cs.IR","abstract":"  In this paper, by applying a diffusion process, we propose a new index to\nquantify the similarity between two users in a user-object bipartite graph. To\ndeal with the discrete ratings on objects, we use a multi-channel\nrepresentation where each object is mapped to several channels with the number\nof channels being equal to the number of different ratings. Each channel\nrepresents a certain rating and a user having voted an object will be connected\nto the channel corresponding to the rating. Diffusion process taking place on\nsuch a user-channel bipartite graph gives a new similarity measure of user\npairs, which is further demonstrated to be more accurate than the classical\nPearson correlation coefficient under the standard collaborative filtering\nframework.\n","text":"Title:Collaborative filtering based on multi-channel diffusion\nAbstract:  In this paper, by applying a diffusion process, we propose a new index to\nquantify the similarity between two users in a user-object bipartite graph. To\ndeal with the discrete ratings on objects, we use a multi-channel\nrepresentation where each object is mapped to several channels with the number\nof channels being equal to the number of different ratings. Each channel\nrepresents a certain rating and a user having voted an object will be connected\nto the channel corresponding to the rating. Diffusion process taking place on\nsuch a user-channel bipartite graph gives a new similarity measure of user\npairs, which is further demonstrated to be more accurate than the classical\nPearson correlation coefficient under the standard collaborative filtering\nframework.\n","vector":null,"chunk_id":"25049ac58da54f53852295e759b4b094"}
{"title":"Poset representation and similarity comparisons os systems in IR","authors":"Christine Michel (LIESP, Ictt)","category":"cs.IR","abstract":"  In this paper we are using the poset representation to describe the complex\nanswers given by IR systems after a clustering and ranking processes. The\nanswers considered may be given by cartographical representations or by\nthematic sub-lists of documents. The poset representation, with the graph\ntheory and the relational representation opens many perspectives in the\ndefinition of new similarity measures capable of taking into account both the\nclustering and ranking processes. We present a general method for constructing\nnew similarity measures and give several examples. These measures can be used\nfor semi-ordered partitions; moreover, in the comparison of two sets of\nanswers, the corresponding similarity indicator is an increasing function of\nthe ranks of presentation of common answers.\n","text":"Title:Poset representation and similarity comparisons os systems in IR\nAbstract:  In this paper we are using the poset representation to describe the complex\nanswers given by IR systems after a clustering and ranking processes. The\nanswers considered may be given by cartographical representations or by\nthematic sub-lists of documents. The poset representation, with the graph\ntheory and the relational representation opens many perspectives in the\ndefinition of new similarity measures capable of taking into account both the\nclustering and ranking processes. We present a general method for constructing\nnew similarity measures and give several examples. These measures can be used\nfor semi-ordered partitions; moreover, in the comparison of two sets of\nanswers, the corresponding similarity indicator is an increasing function of\nthe ranks of presentation of common answers.\n","vector":null,"chunk_id":"7699ddc407a3de4aca8c082d8a178522"}
{"title":"Forest Garrote","authors":"Nicolai Meinshausen","category":"stat.ML","abstract":"  Variable selection for high-dimensional linear models has received a lot of\nattention lately, mostly in the context of l1-regularization. Part of the\nattraction is the variable selection effect: parsimonious models are obtained,\nwhich are very suitable for interpretation. In terms of predictive power,\nhowever, these regularized linear models are often slightly inferior to machine\nlearning procedures like tree ensembles. Tree ensembles, on the other hand,\nlack usually a formal way of variable selection and are difficult to visualize.\nA Garrote-style convex penalty for trees ensembles, in particular Random\nForests, is proposed. The penalty selects functional groups of nodes in the\ntrees. These could be as simple as monotone functions of individual predictor\nvariables. This yields a parsimonious function fit, which lends itself easily\nto visualization and interpretation. The predictive power is maintained at\nleast at the same level as the original tree ensemble. A key feature of the\nmethod is that, once a tree ensemble is fitted, no further tuning parameter\nneeds to be selected. The empirical performance is demonstrated on a wide array\nof datasets.\n","text":"Title:Forest Garrote\nAbstract:  Variable selection for high-dimensional linear models has received a lot of\nattention lately, mostly in the context of l1-regularization. Part of the\nattraction is the variable selection effect: parsimonious models are obtained,\nwhich are very suitable for interpretation. In terms of predictive power,\nhowever, these regularized linear models are often slightly inferior to machine\nlearning procedures like tree ensembles. Tree ensembles, on the other hand,\nlack usually a formal way of variable selection and are difficult to visualize.\nA Garrote-style convex penalty for trees ensembles, in particular Random\nForests, is proposed. The penalty selects functional groups of nodes in the\ntrees. These could be as simple as monotone functions of individual predictor\nvariables. This yields a parsimonious function fit, which lends itself easily\nto visualization and interpretation. The predictive power is maintained at\nleast at the same level as the original tree ensemble. A key feature of the\nmethod is that, once a tree ensemble is fitted, no further tuning parameter\nneeds to be selected. The empirical performance is demonstrated on a wide array\nof datasets.\n","vector":null,"chunk_id":"97cd19cb5b105764686e3bdfe1ca78e2"}
{"title":"The Feature Importance Ranking Measure","authors":"Alexander Zien, Nicole Kraemer, Soeren Sonnenburg, Gunnar Raetsch","category":"stat.ML","abstract":"  Most accurate predictions are typically obtained by learning machines with\ncomplex feature spaces (as e.g. induced by kernels). Unfortunately, such\ndecision rules are hardly accessible to humans and cannot easily be used to\ngain insights about the application domain. Therefore, one often resorts to\nlinear models in combination with variable selection, thereby sacrificing some\npredictive power for presumptive interpretability. Here, we introduce the\nFeature Importance Ranking Measure (FIRM), which by retrospective analysis of\narbitrary learning machines allows to achieve both excellent predictive\nperformance and superior interpretation. In contrast to standard raw feature\nweighting, FIRM takes the underlying correlation structure of the features into\naccount. Thereby, it is able to discover the most relevant features, even if\ntheir appearance in the training data is entirely prevented by noise. The\ndesirable properties of FIRM are investigated analytically and illustrated in\nsimulations.\n","text":"Title:The Feature Importance Ranking Measure\nAbstract:  Most accurate predictions are typically obtained by learning machines with\ncomplex feature spaces (as e.g. induced by kernels). Unfortunately, such\ndecision rules are hardly accessible to humans and cannot easily be used to\ngain insights about the application domain. Therefore, one often resorts to\nlinear models in combination with variable selection, thereby sacrificing some\npredictive power for presumptive interpretability. Here, we introduce the\nFeature Importance Ranking Measure (FIRM), which by retrospective analysis of\narbitrary learning machines allows to achieve both excellent predictive\nperformance and superior interpretation. In contrast to standard raw feature\nweighting, FIRM takes the underlying correlation structure of the features into\naccount. Thereby, it is able to discover the most relevant features, even if\ntheir appearance in the training data is entirely prevented by noise. The\ndesirable properties of FIRM are investigated analytically and illustrated in\nsimulations.\n","vector":null,"chunk_id":"a71755bdc2a52b05233dfae99b68dae6"}
{"title":"KNIFE: Kernel Iterative Feature Extraction","authors":"Genevera I. Allen","category":"stat.ML","abstract":"  Selecting important features in non-linear or kernel spaces is a difficult\nchallenge in both classification and regression problems. When many of the\nfeatures are irrelevant, kernel methods such as the support vector machine and\nkernel ridge regression can sometimes perform poorly. We propose weighting the\nfeatures within a kernel with a sparse set of weights that are estimated in\nconjunction with the original classification or regression problem. The\niterative algorithm, KNIFE, alternates between finding the coefficients of the\noriginal problem and finding the feature weights through kernel linearization.\nIn addition, a slight modification of KNIFE yields an efficient algorithm for\nfinding feature regularization paths, or the paths of each feature's weight.\nSimulation results demonstrate the utility of KNIFE for both kernel regression\nand support vector machines with a variety of kernels. Feature path\nrealizations also reveal important non-linear correlations among features that\nprove useful in determining a subset of significant variables. Results on vowel\nrecognition data, Parkinson's disease data, and microarray data are also given.\n","text":"Title:KNIFE: Kernel Iterative Feature Extraction\nAbstract:  Selecting important features in non-linear or kernel spaces is a difficult\nchallenge in both classification and regression problems. When many of the\nfeatures are irrelevant, kernel methods such as the support vector machine and\nkernel ridge regression can sometimes perform poorly. We propose weighting the\nfeatures within a kernel with a sparse set of weights that are estimated in\nconjunction with the original classification or regression problem. The\niterative algorithm, KNIFE, alternates between finding the coefficients of the\noriginal problem and finding the feature weights through kernel linearization.\nIn addition, a slight modification of KNIFE yields an efficient algorithm for\nfinding feature regularization paths, or the paths of each feature's weight.\nSimulation results demonstrate the utility of KNIFE for both kernel regression\nand support vector machines with a variety of kernels. Feature path\nrealizations also reveal important non-linear correlations among features that\nprove useful in determining a subset of significant variables. Results on vowel\nrecognition data, Parkinson's disease data, and microarray data are also given.\n","vector":null,"chunk_id":"a6fae2fb888db54993403465314fbd80"}
{"title":"Fuzzy Logic Based Method for Improving Text Summarization","authors":"Ladda Suanmali, Naomie Salim and Mohammed Salem Binwahlan","category":"cs.IR","abstract":"  Text summarization can be classified into two approaches: extraction and\nabstraction. This paper focuses on extraction approach. The goal of text\nsummarization based on extraction approach is sentence selection. One of the\nmethods to obtain the suitable sentences is to assign some numerical measure of\na sentence for the summary called sentence weighting and then select the best\nones. The first step in summarization by extraction is the identification of\nimportant features. In our experiment, we used 125 test documents in DUC2002\ndata set. Each document is prepared by preprocessing process: sentence\nsegmentation, tokenization, removing stop word, and word stemming. Then, we use\n8 important features and calculate their score for each sentence. We propose\ntext summarization based on fuzzy logic to improve the quality of the summary\ncreated by the general statistic method. We compare our results with the\nbaseline summarizer and Microsoft Word 2007 summarizers. The results show that\nthe best average precision, recall, and f-measure for the summaries were\nobtained by fuzzy method.\n","text":"Title:Fuzzy Logic Based Method for Improving Text Summarization\nAbstract:  Text summarization can be classified into two approaches: extraction and\nabstraction. This paper focuses on extraction approach. The goal of text\nsummarization based on extraction approach is sentence selection. One of the\nmethods to obtain the suitable sentences is to assign some numerical measure of\na sentence for the summary called sentence weighting and then select the best\nones. The first step in summarization by extraction is the identification of\nimportant features. In our experiment, we used 125 test documents in DUC2002\ndata set. Each document is prepared by preprocessing process: sentence\nsegmentation, tokenization, removing stop word, and word stemming. Then, we use\n8 important features and calculate their score for each sentence. We propose\ntext summarization based on fuzzy logic to improve the quality of the summary\ncreated by the general statistic method. We compare our results with the\nbaseline summarizer and Microsoft Word 2007 summarizers. The results show that\nthe best average precision, recall, and f-measure for the summaries were\nobtained by fuzzy method.\n","vector":null,"chunk_id":"346fa7c231e003b7b4797e2eff3303ea"}
{"title":"Collaborative filtering with diffusion-based similarity on tripartite\n  graphs","authors":"Ming-Sheng Shang, Zi-Ke Zhang, Tao Zhou, Yi-Cheng Zhang","category":"cs.IR","abstract":"  Collaborative tags are playing more and more important role for the\norganization of information systems. In this paper, we study a personalized\nrecommendation model making use of the ternary relations among users, objects\nand tags. We propose a measure of user similarity based on his preference and\ntagging information. Two kinds of similarities between users are calculated by\nusing a diffusion-based process, which are then integrated for recommendation.\nWe test the proposed method in a standard collaborative filtering framework\nwith three metrics: ranking score, Recall and Precision, and demonstrate that\nit performs better than the commonly used cosine similarity.\n","text":"Title:Collaborative filtering with diffusion-based similarity on tripartite\n  graphs\nAbstract:  Collaborative tags are playing more and more important role for the\norganization of information systems. In this paper, we study a personalized\nrecommendation model making use of the ternary relations among users, objects\nand tags. We propose a measure of user similarity based on his preference and\ntagging information. Two kinds of similarities between users are calculated by\nusing a diffusion-based process, which are then integrated for recommendation.\nWe test the proposed method in a standard collaborative filtering framework\nwith three metrics: ranking score, Recall and Precision, and demonstrate that\nit performs better than the commonly used cosine similarity.\n","vector":null,"chunk_id":"c40cbfb45a6268fb293c18b23e9750c3"}
{"title":"Effective Focused Crawling Based on Content and Link Structure Analysis","authors":"Anshika Pal, Deepak Singh Tomar, S.C. Shrivastava","category":"cs.IR","abstract":"  A focused crawler traverses the web selecting out relevant pages to a\npredefined topic and neglecting those out of concern. While surfing the\ninternet it is difficult to deal with irrelevant pages and to predict which\nlinks lead to quality pages. In this paper a technique of effective focused\ncrawling is implemented to improve the quality of web navigation. To check the\nsimilarity of web pages w.r.t. topic keywords a similarity function is used and\nthe priorities of extracted out links are also calculated based on meta data\nand resultant pages generated from focused crawler. The proposed work also uses\na method for traversing the irrelevant pages that met during crawling to\nimprove the coverage of a specific topic.\n","text":"Title:Effective Focused Crawling Based on Content and Link Structure Analysis\nAbstract:  A focused crawler traverses the web selecting out relevant pages to a\npredefined topic and neglecting those out of concern. While surfing the\ninternet it is difficult to deal with irrelevant pages and to predict which\nlinks lead to quality pages. In this paper a technique of effective focused\ncrawling is implemented to improve the quality of web navigation. To check the\nsimilarity of web pages w.r.t. topic keywords a similarity function is used and\nthe priorities of extracted out links are also calculated based on meta data\nand resultant pages generated from focused crawler. The proposed work also uses\na method for traversing the irrelevant pages that met during crawling to\nimprove the coverage of a specific topic.\n","vector":null,"chunk_id":"6871c285c1d4950c81493a55e0d9e28f"}
{"title":"Putting Recommendations on the Map -- Visualizing Clusters and Relations","authors":"Emden Gansner, Yifan Hu, Stephen Kobourov, Chris Volinsky","category":"cs.IR","abstract":"  For users, recommendations can sometimes seem odd or counterintuitive.\nVisualizing recommendations can remove some of this mystery, showing how a\nrecommendation is grouped with other choices. A drawing can also lead a user's\neye to other options. Traditional 2D-embeddings of points can be used to create\na basic layout, but these methods, by themselves, do not illustrate clusters\nand neighborhoods very well. In this paper, we propose the use of geographic\nmaps to enhance the definition of clusters and neighborhoods, and consider the\neffectiveness of this approach in visualizing similarities and recommendations\narising from TV shows and music selections. All the maps referenced in this\npaper can be found in http://www.research.att.com/~volinsky/maps\n","text":"Title:Putting Recommendations on the Map -- Visualizing Clusters and Relations\nAbstract:  For users, recommendations can sometimes seem odd or counterintuitive.\nVisualizing recommendations can remove some of this mystery, showing how a\nrecommendation is grouped with other choices. A drawing can also lead a user's\neye to other options. Traditional 2D-embeddings of points can be used to create\na basic layout, but these methods, by themselves, do not illustrate clusters\nand neighborhoods very well. In this paper, we propose the use of geographic\nmaps to enhance the definition of clusters and neighborhoods, and consider the\neffectiveness of this approach in visualizing similarities and recommendations\narising from TV shows and music selections. All the maps referenced in this\npaper can be found in http://www.research.att.com/~volinsky/maps\n","vector":null,"chunk_id":"ef4594d63712caa074518f6efe2aa6d3"}
{"title":"Bayesian Agglomerative Clustering with Coalescents","authors":"Yee Whye Teh and Hal Daum\\'e III and Daniel Roy","category":"stat.ML","abstract":"  We introduce a new Bayesian model for hierarchical clustering based on a\nprior over trees called Kingman's coalescent. We develop novel greedy and\nsequential Monte Carlo inferences which operate in a bottom-up agglomerative\nfashion. We show experimentally the superiority of our algorithms over others,\nand demonstrate our approach in document clustering and phylolinguistics.\n","text":"Title:Bayesian Agglomerative Clustering with Coalescents\nAbstract:  We introduce a new Bayesian model for hierarchical clustering based on a\nprior over trees called Kingman's coalescent. We develop novel greedy and\nsequential Monte Carlo inferences which operate in a bottom-up agglomerative\nfashion. We show experimentally the superiority of our algorithms over others,\nand demonstrate our approach in document clustering and phylolinguistics.\n","vector":null,"chunk_id":"baccf8bb8e623501b2b9a0786737716c"}
{"title":"Visualizing Topics with Multi-Word Expressions","authors":"David M. Blei and John D. Lafferty","category":"stat.ML","abstract":"  We describe a new method for visualizing topics, the distributions over terms\nthat are automatically extracted from large text corpora using latent variable\nmodels. Our method finds significant $n$-grams related to a topic, which are\nthen used to help understand and interpret the underlying distribution.\nCompared with the usual visualization, which simply lists the most probable\ntopical terms, the multi-word expressions provide a better intuitive impression\nfor what a topic is \"about.\" Our approach is based on a language model of\narbitrary length expressions, for which we develop a new methodology based on\nnested permutation tests to find significant phrases. We show that this method\noutperforms the more standard use of $\\chi^2$ and likelihood ratio tests. We\nillustrate the topic presentations on corpora of scientific abstracts and news\narticles.\n","text":"Title:Visualizing Topics with Multi-Word Expressions\nAbstract:  We describe a new method for visualizing topics, the distributions over terms\nthat are automatically extracted from large text corpora using latent variable\nmodels. Our method finds significant $n$-grams related to a topic, which are\nthen used to help understand and interpret the underlying distribution.\nCompared with the usual visualization, which simply lists the most probable\ntopical terms, the multi-word expressions provide a better intuitive impression\nfor what a topic is \"about.\" Our approach is based on a language model of\narbitrary length expressions, for which we develop a new methodology based on\nnested permutation tests to find significant phrases. We show that this method\noutperforms the more standard use of $\\chi^2$ and likelihood ratio tests. We\nillustrate the topic presentations on corpora of scientific abstracts and news\narticles.\n","vector":null,"chunk_id":"476b6783e0d37a0c1a714b8fe9f4009b"}
{"title":"Role of Weak Ties in Link Prediction of Complex Networks","authors":"Linyuan Lu and Tao Zhou","category":"cs.IR","abstract":"  Plenty of algorithms for link prediction have been proposed and were applied\nto various real networks. Among these works, the weights of links are rarely\ntaken into account. In this paper, we use local similarity indices to estimate\nthe likelihood of the existence of links in weighted networks, including Common\nNeighbor, Adamic-Adar Index, Resource Allocation Index, and their weighted\nversions. In both the unweighted and weighted cases, the resource allocation\nindex performs the best. To our surprise, the weighted indices perform worse,\nwhich reminds us of the well-known Weak Tie Theory. Further extensive\nexperimental study shows that the weak ties play a significant role in the link\nprediction problem, and to emphasize the contribution of weak ties can\nremarkably enhance the predicting accuracy.\n","text":"Title:Role of Weak Ties in Link Prediction of Complex Networks\nAbstract:  Plenty of algorithms for link prediction have been proposed and were applied\nto various real networks. Among these works, the weights of links are rarely\ntaken into account. In this paper, we use local similarity indices to estimate\nthe likelihood of the existence of links in weighted networks, including Common\nNeighbor, Adamic-Adar Index, Resource Allocation Index, and their weighted\nversions. In both the unweighted and weighted cases, the resource allocation\nindex performs the best. To our surprise, the weighted indices perform worse,\nwhich reminds us of the well-known Weak Tie Theory. Further extensive\nexperimental study shows that the weak ties play a significant role in the link\nprediction problem, and to emphasize the contribution of weak ties can\nremarkably enhance the predicting accuracy.\n","vector":null,"chunk_id":"809cca2b979eafa6d5b24341ed89b74a"}
{"title":"Related terms search based on WordNet / Wiktionary and its application\n  in Ontology Matching","authors":"A. A. Krizhanovsky, Feiyu Lin","category":"cs.IR","abstract":"  A set of ontology matching algorithms (for finding correspondences between\nconcepts) is based on a thesaurus that provides the source data for the\nsemantic distance calculations. In this wiki era, new resources may spring up\nand improve this kind of semantic search. In the paper a solution of this task\nbased on Russian Wiktionary is compared to WordNet based algorithms. Metrics\nare estimated using the test collection, containing 353 English word pairs with\na relatedness score assigned by human evaluators. The experiment shows that the\nproposed method is capable in principle of calculating a semantic distance\nbetween pair of words in any language presented in Russian Wiktionary. The\ncalculation of Wiktionary based metric had required the development of the\nopen-source Wiktionary parser software.\n","text":"Title:Related terms search based on WordNet / Wiktionary and its application\n  in Ontology Matching\nAbstract:  A set of ontology matching algorithms (for finding correspondences between\nconcepts) is based on a thesaurus that provides the source data for the\nsemantic distance calculations. In this wiki era, new resources may spring up\nand improve this kind of semantic search. In the paper a solution of this task\nbased on Russian Wiktionary is compared to WordNet based algorithms. Metrics\nare estimated using the test collection, containing 353 English word pairs with\na relatedness score assigned by human evaluators. The experiment shows that the\nproposed method is capable in principle of calculating a semantic distance\nbetween pair of words in any language presented in Russian Wiktionary. The\ncalculation of Wiktionary based metric had required the development of the\nopen-source Wiktionary parser software.\n","vector":null,"chunk_id":"79def80371ca5dda71e290156eeb5c60"}
{"title":"Sparsistent Estimation of Time-Varying Discrete Markov Random Fields","authors":"Mladen Kolar, Eric P. Xing","category":"stat.ML","abstract":"  Network models have been popular for modeling and representing complex\nrelationships and dependencies between observed variables. When data comes from\na dynamic stochastic process, a single static network model cannot adequately\ncapture transient dependencies, such as, gene regulatory dependencies\nthroughout a developmental cycle of an organism. Kolar et al (2010b) proposed a\nmethod based on kernel-smoothing l1-penalized logistic regression for\nestimating time-varying networks from nodal observations collected from a\ntime-series of observational data. In this paper, we establish conditions under\nwhich the proposed method consistently recovers the structure of a time-varying\nnetwork. This work complements previous empirical findings by providing sound\ntheoretical guarantees for the proposed estimation procedure. For completeness,\nwe include numerical simulations in the paper.\n","text":"Title:Sparsistent Estimation of Time-Varying Discrete Markov Random Fields\nAbstract:  Network models have been popular for modeling and representing complex\nrelationships and dependencies between observed variables. When data comes from\na dynamic stochastic process, a single static network model cannot adequately\ncapture transient dependencies, such as, gene regulatory dependencies\nthroughout a developmental cycle of an organism. Kolar et al (2010b) proposed a\nmethod based on kernel-smoothing l1-penalized logistic regression for\nestimating time-varying networks from nodal observations collected from a\ntime-series of observational data. In this paper, we establish conditions under\nwhich the proposed method consistently recovers the structure of a time-varying\nnetwork. This work complements previous empirical findings by providing sound\ntheoretical guarantees for the proposed estimation procedure. For completeness,\nwe include numerical simulations in the paper.\n","vector":null,"chunk_id":"89606bcbc1c50a8f0eb6bf786297e742"}
{"title":"Effective Personalized Recommendation in Collaborative Tagging Systems","authors":"Zi-Ke Zhang, Tao Zhou","category":"cs.IR","abstract":"  Recently, collaborative tagging systems have attracted more and more\nattention and have been widely applied in web systems. Tags provide highly\nabstracted information about personal preferences and item content, and are\ntherefore potential to help in improving better personalized recommendations.\nIn this paper, we propose a tag-based recommendation algorithm considering the\npersonal vocabulary and evaluate it in a real-world dataset: Del.icio.us.\nExperimental results demonstrate that the usage of tag information can\nsignificantly improve the accuracy of personalized recommendations.\n","text":"Title:Effective Personalized Recommendation in Collaborative Tagging Systems\nAbstract:  Recently, collaborative tagging systems have attracted more and more\nattention and have been widely applied in web systems. Tags provide highly\nabstracted information about personal preferences and item content, and are\ntherefore potential to help in improving better personalized recommendations.\nIn this paper, we propose a tag-based recommendation algorithm considering the\npersonal vocabulary and evaluate it in a real-world dataset: Del.icio.us.\nExperimental results demonstrate that the usage of tag information can\nsignificantly improve the accuracy of personalized recommendations.\n","vector":null,"chunk_id":"53085ecdf1435ec9fa5b62ebe0044897"}
{"title":"Empirical Bernstein Bounds and Sample Variance Penalization","authors":"Andreas Maurer and Massimiliano Pontil","category":"stat.ML","abstract":"  We give improved constants for data dependent and variance sensitive\nconfidence bounds, called empirical Bernstein bounds, and extend these\ninequalities to hold uniformly over classes of functionswhose growth function\nis polynomial in the sample size n. The bounds lead us to consider sample\nvariance penalization, a novel learning method which takes into account the\nempirical variance of the loss function. We give conditions under which sample\nvariance penalization is effective. In particular, we present a bound on the\nexcess risk incurred by the method. Using this, we argue that there are\nsituations in which the excess risk of our method is of order 1/n, while the\nexcess risk of empirical risk minimization is of order 1/sqrt/{n}. We show some\nexperimental results, which confirm the theory. Finally, we discuss the\npotential application of our results to sample compression schemes.\n","text":"Title:Empirical Bernstein Bounds and Sample Variance Penalization\nAbstract:  We give improved constants for data dependent and variance sensitive\nconfidence bounds, called empirical Bernstein bounds, and extend these\ninequalities to hold uniformly over classes of functionswhose growth function\nis polynomial in the sample size n. The bounds lead us to consider sample\nvariance penalization, a novel learning method which takes into account the\nempirical variance of the loss function. We give conditions under which sample\nvariance penalization is effective. In particular, we present a bound on the\nexcess risk incurred by the method. Using this, we argue that there are\nsituations in which the excess risk of our method is of order 1/n, while the\nexcess risk of empirical risk minimization is of order 1/sqrt/{n}. We show some\nexperimental results, which confirm the theory. Finally, we discuss the\npotential application of our results to sample compression schemes.\n","vector":null,"chunk_id":"aec17c0a5d3c04e0702c8e19b1d97a96"}
{"title":"USUM: Update Summary Generation System","authors":"C Ravindranath Chowdary, P Sreenivasa Kumar","category":"cs.IR","abstract":"  Huge amount of information is present in the World Wide Web and a large\namount is being added to it frequently. A query-specific summary of multiple\ndocuments is very helpful to the user in this context. Currently, few systems\nhave been proposed for query-specific, extractive multi-document summarization.\nIf a summary is available for a set of documents on a given query and if a new\ndocument is added to the corpus, generating an updated summary from the scratch\nis time consuming and many a times it is not practical/possible. In this paper\nwe propose a solution to this problem. This is especially useful in a scenario\nwhere the source documents are not accessible. We cleverly embed the sentences\nof the current summary into the new document and then perform query-specific\nsummary generation on that document. Our experimental results show that the\nperformance of the proposed approach is good in terms of both quality and\nefficiency.\n","text":"Title:USUM: Update Summary Generation System\nAbstract:  Huge amount of information is present in the World Wide Web and a large\namount is being added to it frequently. A query-specific summary of multiple\ndocuments is very helpful to the user in this context. Currently, few systems\nhave been proposed for query-specific, extractive multi-document summarization.\nIf a summary is available for a set of documents on a given query and if a new\ndocument is added to the corpus, generating an updated summary from the scratch\nis time consuming and many a times it is not practical/possible. In this paper\nwe propose a solution to this problem. This is especially useful in a scenario\nwhere the source documents are not accessible. We cleverly embed the sentences\nof the current summary into the new document and then perform query-specific\nsummary generation on that document. Our experimental results show that the\nperformance of the proposed approach is good in terms of both quality and\nefficiency.\n","vector":null,"chunk_id":"835c9a424f9a2e30b7b866d9e3d03e7d"}
{"title":"Mean-Field Theory of Meta-Learning","authors":"Dariusz Plewczynski","category":"stat.ML","abstract":"  We discuss here the mean-field theory for a cellular automata model of\nmeta-learning. The meta-learning is the process of combining outcomes of\nindividual learning procedures in order to determine the final decision with\nhigher accuracy than any single learning method. Our method is constructed from\nan ensemble of interacting, learning agents, that acquire and process incoming\ninformation using various types, or different versions of machine learning\nalgorithms. The abstract learning space, where all agents are located, is\nconstructed here using a fully connected model that couples all agents with\nrandom strength values. The cellular automata network simulates the higher\nlevel integration of information acquired from the independent learning trials.\nThe final classification of incoming input data is therefore defined as the\nstationary state of the meta-learning system using simple majority rule, yet\nthe minority clusters that share opposite classification outcome can be\nobserved in the system. Therefore, the probability of selecting proper class\nfor a given input data, can be estimated even without the prior knowledge of\nits affiliation. The fuzzy logic can be easily introduced into the system, even\nif learning agents are build from simple binary classification machine learning\nalgorithms by calculating the percentage of agreeing agents.\n","text":"Title:Mean-Field Theory of Meta-Learning\nAbstract:  We discuss here the mean-field theory for a cellular automata model of\nmeta-learning. The meta-learning is the process of combining outcomes of\nindividual learning procedures in order to determine the final decision with\nhigher accuracy than any single learning method. Our method is constructed from\nan ensemble of interacting, learning agents, that acquire and process incoming\ninformation using various types, or different versions of machine learning\nalgorithms. The abstract learning space, where all agents are located, is\nconstructed here using a fully connected model that couples all agents with\nrandom strength values. The cellular automata network simulates the higher\nlevel integration of information acquired from the independent learning trials.\nThe final classification of incoming input data is therefore defined as the\nstationary state of the meta-learning system using simple majority rule, yet\nthe minority clusters that share opposite classification outcome can be\nobserved in the system. Therefore, the probability of selecting proper class\nfor a given input data, can be estimated even without the prior knowledge of\nits affiliation. The fuzzy logic can be easily introduced into the system, even\nif learning agents are build from simple binary classification machine learning\nalgorithms by calculating the percentage of agreeing agents.\n","vector":null,"chunk_id":"ed4d2616efcd71df78d74defe52a4471"}
{"title":"How the initialization affects the stability of the k-means algorithm","authors":"Sebastien Bubeck, Marina Meila, Ulrike von Luxburg","category":"stat.ML","abstract":"  We investigate the role of the initialization for the stability of the\nk-means clustering algorithm. As opposed to other papers, we consider the\nactual k-means algorithm and do not ignore its property of getting stuck in\nlocal optima. We are interested in the actual clustering, not only in the costs\nof the solution. We analyze when different initializations lead to the same\nlocal optimum, and when they lead to different local optima. This enables us to\nprove that it is reasonable to select the number of clusters based on stability\nscores.\n","text":"Title:How the initialization affects the stability of the k-means algorithm\nAbstract:  We investigate the role of the initialization for the stability of the\nk-means clustering algorithm. As opposed to other papers, we consider the\nactual k-means algorithm and do not ignore its property of getting stuck in\nlocal optima. We are interested in the actual clustering, not only in the costs\nof the solution. We analyze when different initializations lead to the same\nlocal optimum, and when they lead to different local optima. This enables us to\nprove that it is reasonable to select the number of clusters based on stability\nscores.\n","vector":null,"chunk_id":"0dde981de8a3ed35e36c48fcdf57df3d"}
{"title":"Evaluation of Coordination Techniques in Synchronous Collaborative\n  Information Retrieval","authors":"Colum Foley and Alan F. Smeaton","category":"cs.IR","abstract":"  Traditional Information Retrieval (IR) research has focussed on a single user\ninteraction modality, where a user searches to satisfy an information need.\nRecent advances in web technologies and computer hardware have enabled multiple\nusers to collaborate on many computer-supported tasks, therefore there is an\nincreasing opportunity to support two or more users searching together at the\nsame time in order to satisfy a shared information need, which we refer to as\nSynchronous Collaborative Information Retrieval (SCIR). SCIR systems represent\na significant paradigmatic shift from traditional IR systems. In order to\nsupport effective SCIR, new techniques are required to coordinate users'\nactivities. In addition, the novel domain of SCIR presents challenges for\neffective evaluations of these systems. In this paper we will propose an\neffective and re-usable evaluation methodology based on simulating users\nsearching together. We will outline how we have used this evaluation in\nempirical studies of the effects of different division of labour and sharing of\nknowledge techniques for SCIR.\n","text":"Title:Evaluation of Coordination Techniques in Synchronous Collaborative\n  Information Retrieval\nAbstract:  Traditional Information Retrieval (IR) research has focussed on a single user\ninteraction modality, where a user searches to satisfy an information need.\nRecent advances in web technologies and computer hardware have enabled multiple\nusers to collaborate on many computer-supported tasks, therefore there is an\nincreasing opportunity to support two or more users searching together at the\nsame time in order to satisfy a shared information need, which we refer to as\nSynchronous Collaborative Information Retrieval (SCIR). SCIR systems represent\na significant paradigmatic shift from traditional IR systems. In order to\nsupport effective SCIR, new techniques are required to coordinate users'\nactivities. In addition, the novel domain of SCIR presents challenges for\neffective evaluations of these systems. In this paper we will propose an\neffective and re-usable evaluation methodology based on simulating users\nsearching together. We will outline how we have used this evaluation in\nempirical studies of the effects of different division of labour and sharing of\nknowledge techniques for SCIR.\n","vector":null,"chunk_id":"4edd33c6edd3175c600942b24ce0e546"}
{"title":"Collaborative Search Trails for Video Search","authors":"Frank Hopfgartner, David Vallet, Martin Halvey and Joemon Jose","category":"cs.IR","abstract":"  In this paper we present an approach for supporting users in the difficult\ntask of searching for video. We use collaborative feedback mined from the\ninteractions of earlier users of a video search system to help users in their\ncurrent search tasks. Our objective is to improve the quality of the results\nthat users find, and in doing so also assist users to explore a large and\ncomplex information space. It is hoped that this will lead to them considering\nsearch options that they may not have considered otherwise. We performed a user\ncentred evaluation. The results of our evaluation indicate that we achieved our\ngoals, the performance of the users in finding relevant video clips was\nenhanced with our system; users were able to explore the collection of video\nclips more and users demonstrated a preference for our system that provided\nrecommendations.\n","text":"Title:Collaborative Search Trails for Video Search\nAbstract:  In this paper we present an approach for supporting users in the difficult\ntask of searching for video. We use collaborative feedback mined from the\ninteractions of earlier users of a video search system to help users in their\ncurrent search tasks. Our objective is to improve the quality of the results\nthat users find, and in doing so also assist users to explore a large and\ncomplex information space. It is hoped that this will lead to them considering\nsearch options that they may not have considered otherwise. We performed a user\ncentred evaluation. The results of our evaluation indicate that we achieved our\ngoals, the performance of the users in finding relevant video clips was\nenhanced with our system; users were able to explore the collection of video\nclips more and users demonstrated a preference for our system that provided\nrecommendations.\n","vector":null,"chunk_id":"724f2d07060b55416d8c3f41bc8cc3ce"}
{"title":"Classification by Set Cover: The Prototype Vector Machine","authors":"Jacob Bien, Robert Tibshirani","category":"stat.ML","abstract":"  We introduce a new nearest-prototype classifier, the prototype vector machine\n(PVM). It arises from a combinatorial optimization problem which we cast as a\nvariant of the set cover problem. We propose two algorithms for approximating\nits solution. The PVM selects a relatively small number of representative\npoints which can then be used for classification. It contains 1-NN as a special\ncase. The method is compatible with any dissimilarity measure, making it\namenable to situations in which the data are not embedded in an underlying\nfeature space or in which using a non-Euclidean metric is desirable. Indeed, we\ndemonstrate on the much studied ZIP code data how the PVM can reap the benefits\nof a problem-specific metric. In this example, the PVM outperforms the highly\nsuccessful 1-NN with tangent distance, and does so retaining fewer than half of\nthe data points. This example highlights the strengths of the PVM in yielding a\nlow-error, highly interpretable model. Additionally, we apply the PVM to a\nprotein classification problem in which a kernel-based distance is used.\n","text":"Title:Classification by Set Cover: The Prototype Vector Machine\nAbstract:  We introduce a new nearest-prototype classifier, the prototype vector machine\n(PVM). It arises from a combinatorial optimization problem which we cast as a\nvariant of the set cover problem. We propose two algorithms for approximating\nits solution. The PVM selects a relatively small number of representative\npoints which can then be used for classification. It contains 1-NN as a special\ncase. The method is compatible with any dissimilarity measure, making it\namenable to situations in which the data are not embedded in an underlying\nfeature space or in which using a non-Euclidean metric is desirable. Indeed, we\ndemonstrate on the much studied ZIP code data how the PVM can reap the benefits\nof a problem-specific metric. In this example, the PVM outperforms the highly\nsuccessful 1-NN with tangent distance, and does so retaining fewer than half of\nthe data points. This example highlights the strengths of the PVM in yielding a\nlow-error, highly interpretable model. Additionally, we apply the PVM to a\nprotein classification problem in which a kernel-based distance is used.\n","vector":null,"chunk_id":"a84a3b62845ab32d527923064ff9034a"}
{"title":"Convex Multiview Fisher Discriminant Analysis","authors":"Tom Diethe, John Shawe-Taylor","category":"stat.ML","abstract":"  Section 1.3 was incorrect, and 2.1 will be removed from further submissions.\nA rewritten version will be posted in the future.\n","text":"Title:Convex Multiview Fisher Discriminant Analysis\nAbstract:  Section 1.3 was incorrect, and 2.1 will be removed from further submissions.\nA rewritten version will be posted in the future.\n","vector":null,"chunk_id":"b89fa720da6e27b8b9c2e682c782dc07"}
{"title":"Relative Expected Improvement in Kriging Based Optimization","authors":"{\\L}ukasz {\\L}aniewski-Wo{\\l}{\\l}k","category":"stat.ML","abstract":"  We propose an extension of the concept of Expected Improvement criterion\ncommonly used in Kriging based optimization. We extend it for more complex\nKriging models, e.g. models using derivatives. The target field of application\nare CFD problems, where objective function are extremely expensive to evaluate,\nbut the theory can be also used in other fields.\n","text":"Title:Relative Expected Improvement in Kriging Based Optimization\nAbstract:  We propose an extension of the concept of Expected Improvement criterion\ncommonly used in Kriging based optimization. We extend it for more complex\nKriging models, e.g. models using derivatives. The target field of application\nare CFD problems, where objective function are extremely expensive to evaluate,\nbut the theory can be also used in other fields.\n","vector":null,"chunk_id":"e9b046c3b317ad3dbbe7c4e85d037c3a"}
{"title":"Learning Bayesian Networks with the bnlearn R Package","authors":"Marco Scutari","category":"stat.ML","abstract":"  bnlearn is an R package which includes several algorithms for learning the\nstructure of Bayesian networks with either discrete or continuous variables.\nBoth constraint-based and score-based algorithms are implemented, and can use\nthe functionality provided by the snow package to improve their performance via\nparallel computing. Several network scores and conditional independence\nalgorithms are available for both the learning algorithms and independent use.\nAdvanced plotting options are provided by the Rgraphviz package.\n","text":"Title:Learning Bayesian Networks with the bnlearn R Package\nAbstract:  bnlearn is an R package which includes several algorithms for learning the\nstructure of Bayesian networks with either discrete or continuous variables.\nBoth constraint-based and score-based algorithms are implemented, and can use\nthe functionality provided by the snow package to improve their performance via\nparallel computing. Several network scores and conditional independence\nalgorithms are available for both the learning algorithms and independent use.\nAdvanced plotting options are provided by the Rgraphviz package.\n","vector":null,"chunk_id":"bebaaed9041563112456aefe3db1a0d8"}
{"title":"A Method for Accelerating the HITS Algorithm","authors":"Andri Mirzal and Masashi Furukawa","category":"cs.IR","abstract":"  We present a new method to accelerate the HITS algorithm by exploiting\nhyperlink structure of the web graph. The proposed algorithm extends the idea\nof authority and hub scores from HITS by introducing two diagonal matrices\nwhich contain constants that act as weights to make authority pages more\nauthoritative and hub pages more hubby. This method works because in the web\ngraph good authorities are pointed to by good hubs and good hubs point to good\nauthorities. Consequently, these pages will collect their scores faster under\nthe proposed algorithm than under the standard HITS. We show that the authority\nand hub vectors of the proposed algorithm exist but are not necessarily be\nunique, and then give a treatment to ensure the uniqueness property of the\nvectors. The experimental results show that the proposed algorithm can improve\nHITS computations, especially for back button datasets.\n","text":"Title:A Method for Accelerating the HITS Algorithm\nAbstract:  We present a new method to accelerate the HITS algorithm by exploiting\nhyperlink structure of the web graph. The proposed algorithm extends the idea\nof authority and hub scores from HITS by introducing two diagonal matrices\nwhich contain constants that act as weights to make authority pages more\nauthoritative and hub pages more hubby. This method works because in the web\ngraph good authorities are pointed to by good hubs and good hubs point to good\nauthorities. Consequently, these pages will collect their scores faster under\nthe proposed algorithm than under the standard HITS. We show that the authority\nand hub vectors of the proposed algorithm exist but are not necessarily be\nunique, and then give a treatment to ensure the uniqueness property of the\nvectors. The experimental results show that the proposed algorithm can improve\nHITS computations, especially for back button datasets.\n","vector":null,"chunk_id":"c858ab967b4696dbc168ee87589c5490"}
{"title":"Kernels for Measures Defined on the Gram Matrix of their Support","authors":"Marco Cuturi","category":"stat.ML","abstract":"  We present in this work a new family of kernels to compare positive measures\non arbitrary spaces $\\Xcal$ endowed with a positive kernel $\\kappa$, which\ntranslates naturally into kernels between histograms or clouds of points. We\nfirst cover the case where $\\Xcal$ is Euclidian, and focus on kernels which\ntake into account the variance matrix of the mixture of two measures to compute\ntheir similarity. The kernels we define are semigroup kernels in the sense that\nthey only use the sum of two measures to compare them, and spectral in the\nsense that they only use the eigenspectrum of the variance matrix of this\nmixture. We show that such a family of kernels has close bonds with the laplace\ntransforms of nonnegative-valued functions defined on the cone of positive\nsemidefinite matrices, and we present some closed formulas that can be derived\nas special cases of such integral expressions. By focusing further on functions\nwhich are invariant to the addition of a null eigenvalue to the spectrum of the\nvariance matrix, we can define kernels between atomic measures on arbitrary\nspaces $\\Xcal$ endowed with a kernel $\\kappa$ by using directly the eigenvalues\nof the centered Gram matrix of the joined support of the compared measures. We\nprovide explicit formulas suited for applications and present preliminary\nexperiments to illustrate the interest of the approach.\n","text":"Title:Kernels for Measures Defined on the Gram Matrix of their Support\nAbstract:  We present in this work a new family of kernels to compare positive measures\non arbitrary spaces $\\Xcal$ endowed with a positive kernel $\\kappa$, which\ntranslates naturally into kernels between histograms or clouds of points. We\nfirst cover the case where $\\Xcal$ is Euclidian, and focus on kernels which\ntake into account the variance matrix of the mixture of two measures to compute\ntheir similarity. The kernels we define are semigroup kernels in the sense that\nthey only use the sum of two measures to compare them, and spectral in the\nsense that they only use the eigenspectrum of the variance matrix of this\nmixture. We show that such a family of kernels has close bonds with the laplace\ntransforms of nonnegative-valued functions defined on the cone of positive\nsemidefinite matrices, and we present some closed formulas that can be derived\nas special cases of such integral expressions. By focusing further on functions\nwhich are invariant to the addition of a null eigenvalue to the spectrum of the\nvariance matrix, we can define kernels between atomic measures on arbitrary\nspaces $\\Xcal$ endowed with a kernel $\\kappa$ by using directly the eigenvalues\nof the centered Gram matrix of the joined support of the compared measures. We\nprovide explicit formulas suited for applications and present preliminary\nexperiments to illustrate the interest of the approach.\n","vector":null,"chunk_id":"034916d6da5b3ea0bde659765dfd8fbe"}
{"title":"Structured Sparse Principal Component Analysis","authors":"Rodolphe Jenatton (INRIA Rocquencourt), Guillaume Obozinski (INRIA\n  Rocquencourt), Francis Bach (INRIA Rocquencourt)","category":"stat.ML","abstract":"  We present an extension of sparse PCA, or sparse dictionary learning, where\nthe sparsity patterns of all dictionary elements are structured and constrained\nto belong to a prespecified set of shapes. This \\emph{structured sparse PCA} is\nbased on a structured regularization recently introduced by [1]. While\nclassical sparse priors only deal with \\textit{cardinality}, the regularization\nwe use encodes higher-order information about the data. We propose an efficient\nand simple optimization procedure to solve this problem. Experiments with two\npractical tasks, face recognition and the study of the dynamics of a protein\ncomplex, demonstrate the benefits of the proposed structured approach over\nunstructured approaches.\n","text":"Title:Structured Sparse Principal Component Analysis\nAbstract:  We present an extension of sparse PCA, or sparse dictionary learning, where\nthe sparsity patterns of all dictionary elements are structured and constrained\nto belong to a prespecified set of shapes. This \\emph{structured sparse PCA} is\nbased on a structured regularization recently introduced by [1]. While\nclassical sparse priors only deal with \\textit{cardinality}, the regularization\nwe use encodes higher-order information about the data. We propose an efficient\nand simple optimization procedure to solve this problem. Experiments with two\npractical tasks, face recognition and the study of the dynamics of a protein\ncomplex, demonstrate the benefits of the proposed structured approach over\nunstructured approaches.\n","vector":null,"chunk_id":"b1cebbb5a10c834e8a823cd952c26497"}
{"title":"Weblog Clustering in Multilinear Algebra Perspective","authors":"Andri Mirzal","category":"cs.IR","abstract":"  This paper describes a clustering method to group the most similar and\nimportant weblogs with their descriptive shared words by using a technique from\nmultilinear algebra known as PARAFAC tensor decomposition. The proposed method\nfirst creates labeled-link network representation of the weblog datasets, where\nthe nodes are the blogs and the labels are the shared words. Then, 3-way\nadjacency tensor is extracted from the network and the PARAFAC decomposition is\napplied to the tensor to get pairs of node lists and label lists with scores\nattached to each list as the indication of the degree of importance. The\nclustering is done by sorting the lists in decreasing order and taking the\npairs of top ranked blogs and words. Thus, unlike standard co-clustering\nmethods, this method not only groups the similar blogs with their descriptive\nwords but also tends to produce clusters of important blogs and descriptive\nwords.\n","text":"Title:Weblog Clustering in Multilinear Algebra Perspective\nAbstract:  This paper describes a clustering method to group the most similar and\nimportant weblogs with their descriptive shared words by using a technique from\nmultilinear algebra known as PARAFAC tensor decomposition. The proposed method\nfirst creates labeled-link network representation of the weblog datasets, where\nthe nodes are the blogs and the labels are the shared words. Then, 3-way\nadjacency tensor is extracted from the network and the PARAFAC decomposition is\napplied to the tensor to get pairs of node lists and label lists with scores\nattached to each list as the indication of the degree of importance. The\nclustering is done by sorting the lists in decreasing order and taking the\npairs of top ranked blogs and words. Thus, unlike standard co-clustering\nmethods, this method not only groups the similar blogs with their descriptive\nwords but also tends to produce clusters of important blogs and descriptive\nwords.\n","vector":null,"chunk_id":"0e68635f43cca3863e6f3941abe661e5"}
{"title":"PrisCrawler: A Relevance Based Crawler for Automated Data Classification\n  from Bulletin Board","authors":"Pu Yang, Jun Guo, Weiran Xu","category":"cs.IR","abstract":"  Nowadays people realize that it is difficult to find information simply and\nquickly on the bulletin boards. In order to solve this problem, people propose\nthe concept of bulletin board search engine. This paper describes the\npriscrawler system, a subsystem of the bulletin board search engine, which can\nautomatically crawl and add the relevance to the classified attachments of the\nbulletin board. Priscrawler utilizes Attachrank algorithm to generate the\nrelevance between webpages and attachments and then turns bulletin board into\nclear classified and associated databases, making the search for attachments\ngreatly simplified. Moreover, it can effectively reduce the complexity of\npretreatment subsystem and retrieval subsystem and improve the search\nprecision. We provide experimental results to demonstrate the efficacy of the\npriscrawler.\n","text":"Title:PrisCrawler: A Relevance Based Crawler for Automated Data Classification\n  from Bulletin Board\nAbstract:  Nowadays people realize that it is difficult to find information simply and\nquickly on the bulletin boards. In order to solve this problem, people propose\nthe concept of bulletin board search engine. This paper describes the\npriscrawler system, a subsystem of the bulletin board search engine, which can\nautomatically crawl and add the relevance to the classified attachments of the\nbulletin board. Priscrawler utilizes Attachrank algorithm to generate the\nrelevance between webpages and attachments and then turns bulletin board into\nclear classified and associated databases, making the search for attachments\ngreatly simplified. Moreover, it can effectively reduce the complexity of\npretreatment subsystem and retrieval subsystem and improve the search\nprecision. We provide experimental results to demonstrate the efficacy of the\npriscrawler.\n","vector":null,"chunk_id":"0230a50b4c82a535e151c9e7ea04f38e"}
{"title":"Pavideoge: A Metadata Markup Video Structure in Video Search Engine","authors":"Pu Yang, Jun Guo, Guang Chen","category":"cs.IR","abstract":"  In this paper, we study the problems of video processing in video search\nengine. Video has now become a very important kind of data in Internet; while\nsearching for video is still a challenging task due to the inner properties of\nvideo: requiring enormous storage space, being independent, expressing\ninformation hiddenly. To handle the properties of video more effectively, in\nthis paper, we propose a new video processing method in video search engine. In\ndetail, the core of the new video processing method is creating pavideoge--a\nnew data type, which contains the video advantages and webpage advantages. The\npavideoge has four attributes: real link, videorank, text information and\nplaynum. Each of them combines video's properties with webpage's. Video search\nengine based on the pavideoge can retrieve video more effectively. The\nexperiment results show the encouraging performance of our approach. Based on\nthe pavideoge, our video search engine can retrieve more precise videos in\ncomparsion with previous related work.\n","text":"Title:Pavideoge: A Metadata Markup Video Structure in Video Search Engine\nAbstract:  In this paper, we study the problems of video processing in video search\nengine. Video has now become a very important kind of data in Internet; while\nsearching for video is still a challenging task due to the inner properties of\nvideo: requiring enormous storage space, being independent, expressing\ninformation hiddenly. To handle the properties of video more effectively, in\nthis paper, we propose a new video processing method in video search engine. In\ndetail, the core of the new video processing method is creating pavideoge--a\nnew data type, which contains the video advantages and webpage advantages. The\npavideoge has four attributes: real link, videorank, text information and\nplaynum. Each of them combines video's properties with webpage's. Video search\nengine based on the pavideoge can retrieve video more effectively. The\nexperiment results show the encouraging performance of our approach. Based on\nthe pavideoge, our video search engine can retrieve more precise videos in\ncomparsion with previous related work.\n","vector":null,"chunk_id":"297d88e2ba4d7fedaf0454768b2244fa"}
{"title":"The Universal Recommender","authors":"J\\'er\\^ome Kunegis, Alan Said, Winfried Umbrath","category":"cs.IR","abstract":"  We describe the Universal Recommender, a recommender system for semantic\ndatasets that generalizes domain-specific recommenders such as content-based,\ncollaborative, social, bibliographic, lexicographic, hybrid and other\nrecommenders. In contrast to existing recommender systems, the Universal\nRecommender applies to any dataset that allows a semantic representation. We\ndescribe the scalable three-stage architecture of the Universal Recommender and\nits application to Internet Protocol Television (IPTV). To achieve good\nrecommendation accuracy, several novel machine learning and optimization\nproblems are identified. We finally give a brief argument supporting the need\nfor machine learning recommenders.\n","text":"Title:The Universal Recommender\nAbstract:  We describe the Universal Recommender, a recommender system for semantic\ndatasets that generalizes domain-specific recommenders such as content-based,\ncollaborative, social, bibliographic, lexicographic, hybrid and other\nrecommenders. In contrast to existing recommender systems, the Universal\nRecommender applies to any dataset that allows a semantic representation. We\ndescribe the scalable three-stage architecture of the Universal Recommender and\nits application to Internet Protocol Television (IPTV). To achieve good\nrecommendation accuracy, several novel machine learning and optimization\nproblems are identified. We finally give a brief argument supporting the need\nfor machine learning recommenders.\n","vector":null,"chunk_id":"ff154948b9b54e1908ef562a64139703"}
{"title":"Telling cause from effect based on high-dimensional observations","authors":"Dominik Janzing, Patrik O. Hoyer, Bernhard Schoelkopf","category":"stat.ML","abstract":"  We describe a method for inferring linear causal relations among\nmulti-dimensional variables. The idea is to use an asymmetry between the\ndistributions of cause and effect that occurs if both the covariance matrix of\nthe cause and the structure matrix mapping cause to the effect are\nindependently chosen. The method works for both stochastic and deterministic\ncausal relations, provided that the dimensionality is sufficiently high (in\nsome experiments, 5 was enough). It is applicable to Gaussian as well as\nnon-Gaussian data.\n","text":"Title:Telling cause from effect based on high-dimensional observations\nAbstract:  We describe a method for inferring linear causal relations among\nmulti-dimensional variables. The idea is to use an asymmetry between the\ndistributions of cause and effect that occurs if both the covariance matrix of\nthe cause and the structure matrix mapping cause to the effect are\nindependently chosen. The method works for both stochastic and deterministic\ncausal relations, provided that the dimensionality is sufficiently high (in\nsome experiments, 5 was enough). It is applicable to Gaussian as well as\nnon-Gaussian data.\n","vector":null,"chunk_id":"25eae3bdc18b88eb482be745c950201e"}
{"title":"Initialization Free Graph Based Clustering","authors":"Laurent Galluccio, Olivier J.J. Michel (GIPSA-lab), Pierre Comon, Eric\n  Slezak (CASSIOPEE), Alfred O. Hero","category":"stat.ML","abstract":"  This paper proposes an original approach to cluster multi-component data\nsets, including an estimation of the number of clusters. From the construction\nof a minimal spanning tree with Prim's algorithm, and the assumption that the\nvertices are approximately distributed according to a Poisson distribution, the\nnumber of clusters is estimated by thresholding the Prim's trajectory. The\ncorresponding cluster centroids are then computed in order to initialize the\ngeneralized Lloyd's algorithm, also known as $K$-means, which allows to\ncircumvent initialization problems. Some results are derived for evaluating the\nfalse positive rate of our cluster detection algorithm, with the help of\napproximations relevant in Euclidean spaces. Metrics used for measuring\nsimilarity between multi-dimensional data points are based on symmetrical\ndivergences. The use of these informational divergences together with the\nproposed method leads to better results, compared to other clustering methods\nfor the problem of astrophysical data processing. Some applications of this\nmethod in the multi/hyper-spectral imagery domain to a satellite view of Paris\nand to an image of the Mars planet are also presented. In order to demonstrate\nthe usefulness of divergences in our problem, the method with informational\ndivergence as similarity measure is compared with the same method using\nclassical metrics. In the astrophysics application, we also compare the method\nwith the spectral clustering algorithms.\n","text":"Title:Initialization Free Graph Based Clustering\nAbstract:  This paper proposes an original approach to cluster multi-component data\nsets, including an estimation of the number of clusters. From the construction\nof a minimal spanning tree with Prim's algorithm, and the assumption that the\nvertices are approximately distributed according to a Poisson distribution, the\nnumber of clusters is estimated by thresholding the Prim's trajectory. The\ncorresponding cluster centroids are then computed in order to initialize the\ngeneralized Lloyd's algorithm, also known as $K$-means, which allows to\ncircumvent initialization problems. Some results are derived for evaluating the\nfalse positive rate of our cluster detection algorithm, with the help of\napproximations relevant in Euclidean spaces. Metrics used for measuring\nsimilarity between multi-dimensional data points are based on symmetrical\ndivergences. The use of these informational divergences together with the\nproposed method leads to better results, compared to other clustering methods\nfor the problem of astrophysical data processing. Some applications of this\nmethod in the multi/hyper-spectral imagery domain to a satellite view of Paris\nand to an image of the Mars planet are also presented. In order to demonstrate\nthe usefulness of divergences in our problem, the method with informational\ndivergence as similarity measure is compared with the same method using\nclassical metrics. In the astrophysics application, we also compare the method\nwith the spectral clustering algorithms.\n","vector":null,"chunk_id":"195a54b79153923f1ad2ff02fbdf46a0"}
{"title":"A baseline for content-based blog classification","authors":"Olof Gornerup, Magnus Boman","category":"cs.IR","abstract":"  A content-based network representation of web logs (blogs) using a basic\nword-overlap similarity measure is presented. Due to a strong signal in blog\ndata the approach is sufficient for accurately classifying blogs. Using Swedish\nblog data we demonstrate that blogs that treat similar subjects are organized\nin clusters that, in turn, are hierarchically organized in higher-order\nclusters. The simplicity of the representation renders it both computationally\ntractable and transparent. We therefore argue that the approach is suitable as\na baseline when developing and analyzing more advanced content-based\nrepresentations of the blogosphere.\n","text":"Title:A baseline for content-based blog classification\nAbstract:  A content-based network representation of web logs (blogs) using a basic\nword-overlap similarity measure is presented. Due to a strong signal in blog\ndata the approach is sufficient for accurately classifying blogs. Using Swedish\nblog data we demonstrate that blogs that treat similar subjects are organized\nin clusters that, in turn, are hierarchically organized in higher-order\nclusters. The simplicity of the representation renders it both computationally\ntractable and transparent. We therefore argue that the approach is suitable as\na baseline when developing and analyzing more advanced content-based\nrepresentations of the blogosphere.\n","vector":null,"chunk_id":"43c30b729843e29c08ce139560afe396"}
{"title":"Dirichlet Process Mixtures of Generalized Linear Models","authors":"Lauren A. Hannah, David M. Blei, Warren B. Powell","category":"stat.ML","abstract":"  We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM),\na new method of nonparametric regression that accommodates continuous and\ncategorical inputs, and responses that can be modeled by a generalized linear\nmodel. We prove conditions for the asymptotic unbiasedness of the DP-GLM\nregression mean function estimate. We also give examples for when those\nconditions hold, including models for compactly supported continuous\ndistributions and a model with continuous covariates and categorical response.\nWe empirically analyze the properties of the DP-GLM and why it provides better\nresults than existing Dirichlet process mixture regression models. We evaluate\nDP-GLM on several data sets, comparing it to modern methods of nonparametric\nregression like CART, Bayesian trees and Gaussian processes. Compared to\nexisting techniques, the DP-GLM provides a single model (and corresponding\ninference algorithms) that performs well in many regression settings.\n","text":"Title:Dirichlet Process Mixtures of Generalized Linear Models\nAbstract:  We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM),\na new method of nonparametric regression that accommodates continuous and\ncategorical inputs, and responses that can be modeled by a generalized linear\nmodel. We prove conditions for the asymptotic unbiasedness of the DP-GLM\nregression mean function estimate. We also give examples for when those\nconditions hold, including models for compactly supported continuous\ndistributions and a model with continuous covariates and categorical response.\nWe empirically analyze the properties of the DP-GLM and why it provides better\nresults than existing Dirichlet process mixture regression models. We evaluate\nDP-GLM on several data sets, comparing it to modern methods of nonparametric\nregression like CART, Bayesian trees and Gaussian processes. Compared to\nexisting techniques, the DP-GLM provides a single model (and corresponding\ninference algorithms) that performs well in many regression settings.\n","vector":null,"chunk_id":"9d1038972cd8d33e2ae39a2c54be42b3"}
{"title":"Laplacian Support Vector Machines Trained in the Primal","authors":"Stefano Melacci, Mikhail Belkin","category":"stat.ML","abstract":"  In the last few years, due to the growing ubiquity of unlabeled data, much\neffort has been spent by the machine learning community to develop better\nunderstanding and improve the quality of classifiers exploiting unlabeled data.\nFollowing the manifold regularization approach, Laplacian Support Vector\nMachines (LapSVMs) have shown the state of the art performance in\nsemi--supervised classification. In this paper we present two strategies to\nsolve the primal LapSVM problem, in order to overcome some issues of the\noriginal dual formulation. Whereas training a LapSVM in the dual requires two\nsteps, using the primal form allows us to collapse training to a single step.\nMoreover, the computational complexity of the training algorithm is reduced\nfrom O(n^3) to O(n^2) using preconditioned conjugate gradient, where n is the\ncombined number of labeled and unlabeled examples. We speed up training by\nusing an early stopping strategy based on the prediction on unlabeled data or,\nif available, on labeled validation examples. This allows the algorithm to\nquickly compute approximate solutions with roughly the same classification\naccuracy as the optimal ones, considerably reducing the training time. Due to\nits simplicity, training LapSVM in the primal can be the starting point for\nadditional enhancements of the original LapSVM formulation, such as those for\ndealing with large datasets. We present an extensive experimental evaluation on\nreal world data showing the benefits of the proposed approach.\n","text":"Title:Laplacian Support Vector Machines Trained in the Primal\nAbstract:  In the last few years, due to the growing ubiquity of unlabeled data, much\neffort has been spent by the machine learning community to develop better\nunderstanding and improve the quality of classifiers exploiting unlabeled data.\nFollowing the manifold regularization approach, Laplacian Support Vector\nMachines (LapSVMs) have shown the state of the art performance in\nsemi--supervised classification. In this paper we present two strategies to\nsolve the primal LapSVM problem, in order to overcome some issues of the\noriginal dual formulation. Whereas training a LapSVM in the dual requires two\nsteps, using the primal form allows us to collapse training to a single step.\nMoreover, the computational complexity of the training algorithm is reduced\nfrom O(n^3) to O(n^2) using preconditioned conjugate gradient, where n is the\ncombined number of labeled and unlabeled examples. We speed up training by\nusing an early stopping strategy based on the prediction on unlabeled data or,\nif available, on labeled validation examples. This allows the algorithm to\nquickly compute approximate solutions with roughly the same classification\naccuracy as the optimal ones, considerably reducing the training time. Due to\nits simplicity, training LapSVM in the primal can be the starting point for\nadditional enhancements of the original LapSVM formulation, such as those for\ndealing with large datasets. We present an extensive experimental evaluation on\nreal world data showing the benefits of the proposed approach.\n","vector":null,"chunk_id":"f49a51a3e09365498f188c63732a1a29"}
{"title":"Expectation Propagation on the Maximum of Correlated Normal Variables","authors":"Philipp Hennig","category":"stat.ML","abstract":"  Many inference problems involving questions of optimality ask for the maximum\nor the minimum of a finite set of unknown quantities. This technical report\nderives the first two posterior moments of the maximum of two correlated\nGaussian variables and the first two posterior moments of the two generating\nvariables (corresponding to Gaussian approximations minimizing relative\nentropy). It is shown how this can be used to build a heuristic approximation\nto the maximum relationship over a finite set of Gaussian variables, allowing\napproximate inference by Expectation Propagation on such quantities.\n","text":"Title:Expectation Propagation on the Maximum of Correlated Normal Variables\nAbstract:  Many inference problems involving questions of optimality ask for the maximum\nor the minimum of a finite set of unknown quantities. This technical report\nderives the first two posterior moments of the maximum of two correlated\nGaussian variables and the first two posterior moments of the two generating\nvariables (corresponding to Gaussian approximations minimizing relative\nentropy). It is shown how this can be used to build a heuristic approximation\nto the maximum relationship over a finite set of Gaussian variables, allowing\napproximate inference by Expectation Propagation on such quantities.\n","vector":null,"chunk_id":"2a8cbdb668340261eacd7717f0416c2e"}
{"title":"Functional learning through kernels","authors":"Stephane Canu (LITIS), Xavier Mary, Alain Rakotomamonjy (LITIS)","category":"stat.ML","abstract":"  This paper reviews the functional aspects of statistical learning theory. The\nmain point under consideration is the nature of the hypothesis set when no\nprior information is available but data. Within this framework we first discuss\nabout the hypothesis set: it is a vectorial space, it is a set of pointwise\ndefined functions, and the evaluation functional on this set is a continuous\nmapping. Based on these principles an original theory is developed generalizing\nthe notion of reproduction kernel Hilbert space to non hilbertian sets. Then it\nis shown that the hypothesis set of any learning machine has to be a\ngeneralized reproducing set. Therefore, thanks to a general \"representer\ntheorem\", the solution of the learning problem is still a linear combination of\na kernel. Furthermore, a way to design these kernels is given. To illustrate\nthis framework some examples of such reproducing sets and kernels are given.\n","text":"Title:Functional learning through kernels\nAbstract:  This paper reviews the functional aspects of statistical learning theory. The\nmain point under consideration is the nature of the hypothesis set when no\nprior information is available but data. Within this framework we first discuss\nabout the hypothesis set: it is a vectorial space, it is a set of pointwise\ndefined functions, and the evaluation functional on this set is a continuous\nmapping. Based on these principles an original theory is developed generalizing\nthe notion of reproduction kernel Hilbert space to non hilbertian sets. Then it\nis shown that the hypothesis set of any learning machine has to be a\ngeneralized reproducing set. Therefore, thanks to a general \"representer\ntheorem\", the solution of the learning problem is still a linear combination of\na kernel. Furthermore, a way to design these kernels is given. To illustrate\nthis framework some examples of such reproducing sets and kernels are given.\n","vector":null,"chunk_id":"805020cd6d6ed6c9936061cabdb8c410"}
{"title":"Management Of Volatile Information In Incremental Web Crawler","authors":"Ravita Chahar, Komal Hooda and Annu Dhankhar","category":"cs.IR","abstract":"  Paper has been withdrawn.\n","text":"Title:Management Of Volatile Information In Incremental Web Crawler\nAbstract:  Paper has been withdrawn.\n","vector":null,"chunk_id":"142d69f2349fcca7c4aac046240431f7"}
{"title":"Information Retrieval via Truncated Hilbert-Space Expansions","authors":"Patricio Galeas, Ralph Kretschmer and Bernd Freisleben","category":"cs.IR","abstract":"  In addition to the frequency of terms in a document collection, the\ndistribution of terms plays an important role in determining the relevance of\ndocuments. In this paper, a new approach for representing term positions in\ndocuments is presented. The approach allows an efficient evaluation of\nterm-positional information at query evaluation time. Three applications are\ninvestigated: a function-based ranking optimization representing a user-defined\ndocument region, a query expansion technique based on overlapping the term\ndistributions in the top-ranked documents, and cluster analysis of terms in\ndocuments. Experimental results demonstrate the effectiveness of the proposed\napproach.\n","text":"Title:Information Retrieval via Truncated Hilbert-Space Expansions\nAbstract:  In addition to the frequency of terms in a document collection, the\ndistribution of terms plays an important role in determining the relevance of\ndocuments. In this paper, a new approach for representing term positions in\ndocuments is presented. The approach allows an efficient evaluation of\nterm-positional information at query evaluation time. Three applications are\ninvestigated: a function-based ranking optimization representing a user-defined\ndocument region, a query expansion technique based on overlapping the term\ndistributions in the top-ranked documents, and cluster analysis of terms in\ndocuments. Experimental results demonstrate the effectiveness of the proposed\napproach.\n","vector":null,"chunk_id":"82800d5b4bbc558757a847ac78924f6f"}
{"title":"Sparsification and feature selection by compressive linear regression","authors":"Florin Popescu, Daniel Renz","category":"stat.ML","abstract":"  The Minimum Description Length (MDL) principle states that the optimal model\nfor a given data set is that which compresses it best. Due to practial\nlimitations the model can be restricted to a class such as linear regression\nmodels, which we address in this study. As in other formulations such as the\nLASSO and forward step-wise regression we are interested in sparsifying the\nfeature set while preserving generalization ability. We derive a\nwell-principled set of codes for both parameters and error residuals along with\nsmooth approximations to lengths of these codes as to allow gradient descent\noptimization of description length, and go on to show that sparsification and\nfeature selection using our approach is faster than the LASSO on several\ndatasets from the UCI and StatLib repositories, with favorable generalization\naccuracy, while being fully automatic, requiring neither cross-validation nor\ntuning of regularization hyper-parameters, allowing even for a nonlinear\nexpansion of the feature set followed by sparsification.\n","text":"Title:Sparsification and feature selection by compressive linear regression\nAbstract:  The Minimum Description Length (MDL) principle states that the optimal model\nfor a given data set is that which compresses it best. Due to practial\nlimitations the model can be restricted to a class such as linear regression\nmodels, which we address in this study. As in other formulations such as the\nLASSO and forward step-wise regression we are interested in sparsifying the\nfeature set while preserving generalization ability. We derive a\nwell-principled set of codes for both parameters and error residuals along with\nsmooth approximations to lengths of these codes as to allow gradient descent\noptimization of description length, and go on to show that sparsification and\nfeature selection using our approach is faster than the LASSO on several\ndatasets from the UCI and StatLib repositories, with favorable generalization\naccuracy, while being fully automatic, requiring neither cross-validation nor\ntuning of regularization hyper-parameters, allowing even for a nonlinear\nexpansion of the feature set followed by sparsification.\n","vector":null,"chunk_id":"63c77df8f8a9467b6bac96e7187db829"}
{"title":"Enrichissement des contenus par la r\\'eindexation des usagers : un\n  \\'etat de l'art sur la probl\\'ematique","authors":"Azza Harbaoui (ENSI-Riadi-GDL), Malek Ghenima (ENSI-Riadi-GDL), Sahbi\n  Sidhom (LORIA, Loria)","category":"cs.IR","abstract":"  Information retrieval (IR) is a user approach to obtain relevant information\nwhich meets needs with the help of a IR system (IRS). However, the IRS shows\ncertain differences between user relevance and system relevance. These gaps are\nessentially related to the imperfection of the indexing process (as approach\nrelated to the IR), to problems related to the misunderstanding of the natural\nlanguage and the non correspondence between the real needs of the user and the\nresults of his query. As idea is to think about an ?intellectual? indexing that\ntakes into account the point of view of the user. By consulting the document,\nuser can build information as added-value on the existing content: new\ninformation which grows contents and allows the semantic visibility or\nfacilitates the reading by the annotations, by links to other content, by new\ndescriptors, specific new abstracts of users: it is the reindexing of the\ncontents by the contribution or the vote of the uses\n","text":"Title:Enrichissement des contenus par la r\\'eindexation des usagers : un\n  \\'etat de l'art sur la probl\\'ematique\nAbstract:  Information retrieval (IR) is a user approach to obtain relevant information\nwhich meets needs with the help of a IR system (IRS). However, the IRS shows\ncertain differences between user relevance and system relevance. These gaps are\nessentially related to the imperfection of the indexing process (as approach\nrelated to the IR), to problems related to the misunderstanding of the natural\nlanguage and the non correspondence between the real needs of the user and the\nresults of his query. As idea is to think about an ?intellectual? indexing that\ntakes into account the point of view of the user. By consulting the document,\nuser can build information as added-value on the existing content: new\ninformation which grows contents and allows the semantic visibility or\nfacilitates the reading by the annotations, by links to other content, by new\ndescriptors, specific new abstracts of users: it is the reindexing of the\ncontents by the contribution or the vote of the uses\n","vector":null,"chunk_id":"91f7701c735c77291765e0f3c05d874c"}
{"title":"Distinguishing Cause and Effect via Second Order Exponential Models","authors":"Dominik Janzing, Xiaohai Sun, Bernhard Schoelkopf","category":"stat.ML","abstract":"  We propose a method to infer causal structures containing both discrete and\ncontinuous variables. The idea is to select causal hypotheses for which the\nconditional density of every variable, given its causes, becomes smooth. We\ndefine a family of smooth densities and conditional densities by second order\nexponential models, i.e., by maximizing conditional entropy subject to first\nand second statistical moments. If some of the variables take only values in\nproper subsets of R^n, these conditionals can induce different families of\njoint distributions even for Markov-equivalent graphs.\n  We consider the case of one binary and one real-valued variable where the\nmethod can distinguish between cause and effect. Using this example, we\ndescribe that sometimes a causal hypothesis must be rejected because\nP(effect|cause) and P(cause) share algorithmic information (which is untypical\nif they are chosen independently). This way, our method is in the same spirit\nas faithfulness-based causal inference because it also rejects non-generic\nmutual adjustments among DAG-parameters.\n","text":"Title:Distinguishing Cause and Effect via Second Order Exponential Models\nAbstract:  We propose a method to infer causal structures containing both discrete and\ncontinuous variables. The idea is to select causal hypotheses for which the\nconditional density of every variable, given its causes, becomes smooth. We\ndefine a family of smooth densities and conditional densities by second order\nexponential models, i.e., by maximizing conditional entropy subject to first\nand second statistical moments. If some of the variables take only values in\nproper subsets of R^n, these conditionals can induce different families of\njoint distributions even for Markov-equivalent graphs.\n  We consider the case of one binary and one real-valued variable where the\nmethod can distinguish between cause and effect. Using this example, we\ndescribe that sometimes a causal hypothesis must be rejected because\nP(effect|cause) and P(cause) share algorithmic information (which is untypical\nif they are chosen independently). This way, our method is in the same spirit\nas faithfulness-based causal inference because it also rejects non-generic\nmutual adjustments among DAG-parameters.\n","vector":null,"chunk_id":"2b273c1e0ab986afb732dad43a434ede"}
{"title":"Causal Inference on Discrete Data using Additive Noise Models","authors":"Jonas Peters, Dominik Janzing, Bernhard Sch\\\"olkopf","category":"stat.ML","abstract":"  Inferring the causal structure of a set of random variables from a finite\nsample of the joint distribution is an important problem in science. Recently,\nmethods using additive noise models have been suggested to approach the case of\ncontinuous variables. In many situations, however, the variables of interest\nare discrete or even have only finitely many states. In this work we extend the\nnotion of additive noise models to these cases. We prove that whenever the\njoint distribution $\\prob^{(X,Y)}$ admits such a model in one direction, e.g.\n$Y=f(X)+N, N \\independent X$, it does not admit the reversed model\n$X=g(Y)+\\tilde N, \\tilde N \\independent Y$ as long as the model is chosen in a\ngeneric way. Based on these deliberations we propose an efficient new algorithm\nthat is able to distinguish between cause and effect for a finite sample of\ndiscrete variables. In an extensive experimental study we show that this\nalgorithm works both on synthetic and real data sets.\n","text":"Title:Causal Inference on Discrete Data using Additive Noise Models\nAbstract:  Inferring the causal structure of a set of random variables from a finite\nsample of the joint distribution is an important problem in science. Recently,\nmethods using additive noise models have been suggested to approach the case of\ncontinuous variables. In many situations, however, the variables of interest\nare discrete or even have only finitely many states. In this work we extend the\nnotion of additive noise models to these cases. We prove that whenever the\njoint distribution $\\prob^{(X,Y)}$ admits such a model in one direction, e.g.\n$Y=f(X)+N, N \\independent X$, it does not admit the reversed model\n$X=g(Y)+\\tilde N, \\tilde N \\independent Y$ as long as the model is chosen in a\ngeneric way. Based on these deliberations we propose an efficient new algorithm\nthat is able to distinguish between cause and effect for a finite sample of\ndiscrete variables. In an extensive experimental study we show that this\nalgorithm works both on synthetic and real data sets.\n","vector":null,"chunk_id":"2b098a7bf9fca58674ff89fc92599e02"}
{"title":"Enhanced Trustworthy and High-Quality Information Retrieval System for\n  Web Search Engines","authors":"Sumalatha Ramachandran, Sujaya Paulraj, Sharon Joseph and Vetriselvi\n  Ramaraj","category":"cs.IR","abstract":"  The WWW is the most important source of information. But, there is no\nguarantee for information correctness and lots of conflicting information is\nretrieved by the search engines and the quality of provided information also\nvaries from low quality to high quality. We provide enhanced trustworthiness in\nboth specific (entity) and broad (content) queries in web searching. The\nfiltering of trustworthiness is based on 5 factors: Provenance, Authority, Age,\nPopularity, and Related Links. The trustworthiness is calculated based on these\n5 factors and it is stored thereby increasing the performance in retrieving\ntrustworthy websites. The calculated trustworthiness is stored only for static\nwebsites. Quality is provided based on policies selected by the user. Quality\nbased ranking of retrieved trusted information is provided using WIQA (Web\nInformation Quality Assessment) Framework.\n","text":"Title:Enhanced Trustworthy and High-Quality Information Retrieval System for\n  Web Search Engines\nAbstract:  The WWW is the most important source of information. But, there is no\nguarantee for information correctness and lots of conflicting information is\nretrieved by the search engines and the quality of provided information also\nvaries from low quality to high quality. We provide enhanced trustworthiness in\nboth specific (entity) and broad (content) queries in web searching. The\nfiltering of trustworthiness is based on 5 factors: Provenance, Authority, Age,\nPopularity, and Related Links. The trustworthiness is calculated based on these\n5 factors and it is stored thereby increasing the performance in retrieving\ntrustworthy websites. The calculated trustworthiness is stored only for static\nwebsites. Quality is provided based on policies selected by the user. Quality\nbased ranking of retrieved trusted information is provided using WIQA (Web\nInformation Quality Assessment) Framework.\n","vector":null,"chunk_id":"939672dda02efb497b27f4da89dd092b"}
{"title":"How slow is slow? SFA detects signals that are slower than the driving\n  force","authors":"Wolfgang Konen, Patrick Koch","category":"stat.ML","abstract":"  Slow feature analysis (SFA) is a method for extracting slowly varying driving\nforces from quickly varying nonstationary time series. We show here that it is\npossible for SFA to detect a component which is even slower than the driving\nforce itself (e.g. the envelope of a modulated sine wave). It is shown that it\ndepends on circumstances like the embedding dimension, the time series\npredictability, or the base frequency, whether the driving force itself or a\nslower subcomponent is detected. We observe a phase transition from one regime\nto the other and it is the purpose of this work to quantify the influence of\nvarious parameters on this phase transition. We conclude that what is percieved\nas slow by SFA varies and that a more or less fast switching from one regime to\nthe other occurs, perhaps showing some similarity to human perception.\n","text":"Title:How slow is slow? SFA detects signals that are slower than the driving\n  force\nAbstract:  Slow feature analysis (SFA) is a method for extracting slowly varying driving\nforces from quickly varying nonstationary time series. We show here that it is\npossible for SFA to detect a component which is even slower than the driving\nforce itself (e.g. the envelope of a modulated sine wave). It is shown that it\ndepends on circumstances like the embedding dimension, the time series\npredictability, or the base frequency, whether the driving force itself or a\nslower subcomponent is detected. We observe a phase transition from one regime\nto the other and it is the purpose of this work to quantify the influence of\nvarious parameters on this phase transition. We conclude that what is percieved\nas slow by SFA varies and that a more or less fast switching from one regime to\nthe other occurs, perhaps showing some similarity to human perception.\n","vector":null,"chunk_id":"ac96021f1afa5b8097ac4f24488f8db1"}
{"title":"Integrating the Probabilistic Models BM25/BM25F into Lucene","authors":"Joaqu\\'in P\\'erez-Iglesias, Jos\\'e R. P\\'erez-Ag\\\"uera, V\\'ictor\n  Fresno and Yuval Z. Feinstein","category":"cs.IR","abstract":"  This document describes the BM25 and BM25F implementation using the Lucene\nJava Framework. Both models have stood out at TREC by their performance and are\nconsidered as state-of-the-art in the IR community. BM25 is applied to\nretrieval on plain text documents, that is for documents that do not contain\nfields, while BM25F is applied to documents with structure.\n","text":"Title:Integrating the Probabilistic Models BM25/BM25F into Lucene\nAbstract:  This document describes the BM25 and BM25F implementation using the Lucene\nJava Framework. Both models have stood out at TREC by their performance and are\nconsidered as state-of-the-art in the IR community. BM25 is applied to\nretrieval on plain text documents, that is for documents that do not contain\nfields, while BM25F is applied to documents with structure.\n","vector":null,"chunk_id":"1fec6ee52953377d9474471414178864"}
{"title":"Sparse Convolved Multiple Output Gaussian Processes","authors":"Mauricio A. \\'Alvarez and Neil D. Lawrence","category":"stat.ML","abstract":"  Recently there has been an increasing interest in methods that deal with\nmultiple outputs. This has been motivated partly by frameworks like multitask\nlearning, multisensor networks or structured output data. From a Gaussian\nprocesses perspective, the problem reduces to specifying an appropriate\ncovariance function that, whilst being positive semi-definite, captures the\ndependencies between all the data points and across all the outputs. One\napproach to account for non-trivial correlations between outputs employs\nconvolution processes. Under a latent function interpretation of the\nconvolution transform we establish dependencies between output variables. The\nmain drawbacks of this approach are the associated computational and storage\ndemands. In this paper we address these issues. We present different sparse\napproximations for dependent output Gaussian processes constructed through the\nconvolution formalism. We exploit the conditional independencies present\nnaturally in the model. This leads to a form of the covariance similar in\nspirit to the so called PITC and FITC approximations for a single output. We\nshow experimental results with synthetic and real data, in particular, we show\nresults in pollution prediction, school exams score prediction and gene\nexpression data.\n","text":"Title:Sparse Convolved Multiple Output Gaussian Processes\nAbstract:  Recently there has been an increasing interest in methods that deal with\nmultiple outputs. This has been motivated partly by frameworks like multitask\nlearning, multisensor networks or structured output data. From a Gaussian\nprocesses perspective, the problem reduces to specifying an appropriate\ncovariance function that, whilst being positive semi-definite, captures the\ndependencies between all the data points and across all the outputs. One\napproach to account for non-trivial correlations between outputs employs\nconvolution processes. Under a latent function interpretation of the\nconvolution transform we establish dependencies between output variables. The\nmain drawbacks of this approach are the associated computational and storage\ndemands. In this paper we address these issues. We present different sparse\napproximations for dependent output Gaussian processes constructed through the\nconvolution formalism. We exploit the conditional independencies present\nnaturally in the model. This leads to a form of the covariance similar in\nspirit to the so called PITC and FITC approximations for a single output. We\nshow experimental results with synthetic and real data, in particular, we show\nresults in pollution prediction, school exams score prediction and gene\nexpression data.\n","vector":null,"chunk_id":"5b4f4f8aa9c01411b1f4ffc1457484df"}
{"title":"Positive Definite Kernels in Machine Learning","authors":"Marco Cuturi","category":"stat.ML","abstract":"  This survey is an introduction to positive definite kernels and the set of\nmethods they have inspired in the machine learning literature, namely kernel\nmethods. We first discuss some properties of positive definite kernels as well\nas reproducing kernel Hibert spaces, the natural extension of the set of\nfunctions $\\{k(x,\\cdot),x\\in\\mathcal{X}\\}$ associated with a kernel $k$ defined\non a space $\\mathcal{X}$. We discuss at length the construction of kernel\nfunctions that take advantage of well-known statistical models. We provide an\noverview of numerous data-analysis methods which take advantage of reproducing\nkernel Hilbert spaces and discuss the idea of combining several kernels to\nimprove the performance on certain tasks. We also provide a short cookbook of\ndifferent kernels which are particularly useful for certain data-types such as\nimages, graphs or speech segments.\n","text":"Title:Positive Definite Kernels in Machine Learning\nAbstract:  This survey is an introduction to positive definite kernels and the set of\nmethods they have inspired in the machine learning literature, namely kernel\nmethods. We first discuss some properties of positive definite kernels as well\nas reproducing kernel Hibert spaces, the natural extension of the set of\nfunctions $\\{k(x,\\cdot),x\\in\\mathcal{X}\\}$ associated with a kernel $k$ defined\non a space $\\mathcal{X}$. We discuss at length the construction of kernel\nfunctions that take advantage of well-known statistical models. We provide an\noverview of numerous data-analysis methods which take advantage of reproducing\nkernel Hilbert spaces and discuss the idea of combining several kernels to\nimprove the performance on certain tasks. We also provide a short cookbook of\ndifferent kernels which are particularly useful for certain data-types such as\nimages, graphs or speech segments.\n","vector":null,"chunk_id":"248268d3f3c5a474ee61344609eebe9f"}
{"title":"De la recherche sociale d'information \\`a la recherche collaborative\n  d'information","authors":"Victor Odumuyiwa (LORIA)","category":"cs.IR","abstract":"  In this paper, we explain social information retrieval (SIR) and\ncollaborative information retrieval (CIR). We see SIR as a way of knowing who\nto collaborate with in resolving an information problem while CIR entails the\nprocess of mutual understanding and solving of an information problem among\ncollaborators. We are interested in the transition from SIR to CIR hence we\ndeveloped a communication model to facilitate knowledge sharing during CIR.\n","text":"Title:De la recherche sociale d'information \\`a la recherche collaborative\n  d'information\nAbstract:  In this paper, we explain social information retrieval (SIR) and\ncollaborative information retrieval (CIR). We see SIR as a way of knowing who\nto collaborate with in resolving an information problem while CIR entails the\nprocess of mutual understanding and solving of an information problem among\ncollaborators. We are interested in the transition from SIR to CIR hence we\ndeveloped a communication model to facilitate knowledge sharing during CIR.\n","vector":null,"chunk_id":"2eef22bc634e32c250aaf17950a39044"}
{"title":"Under-determined reverberant audio source separation using a full-rank\n  spatial covariance model","authors":"Ngoc Duong (INRIA - Irisa), Emmanuel Vincent (INRIA - Irisa), Remi\n  Gribonval (INRIA - Irisa)","category":"stat.ML","abstract":"  This article addresses the modeling of reverberant recording environments in\nthe context of under-determined convolutive blind source separation. We model\nthe contribution of each source to all mixture channels in the time-frequency\ndomain as a zero-mean Gaussian random variable whose covariance encodes the\nspatial characteristics of the source. We then consider four specific\ncovariance models, including a full-rank unconstrained model. We derive a\nfamily of iterative expectationmaximization (EM) algorithms to estimate the\nparameters of each model and propose suitable procedures to initialize the\nparameters and to align the order of the estimated sources across all frequency\nbins based on their estimated directions of arrival (DOA). Experimental results\nover reverberant synthetic mixtures and live recordings of speech data show the\neffectiveness of the proposed approach.\n","text":"Title:Under-determined reverberant audio source separation using a full-rank\n  spatial covariance model\nAbstract:  This article addresses the modeling of reverberant recording environments in\nthe context of under-determined convolutive blind source separation. We model\nthe contribution of each source to all mixture channels in the time-frequency\ndomain as a zero-mean Gaussian random variable whose covariance encodes the\nspatial characteristics of the source. We then consider four specific\ncovariance models, including a full-rank unconstrained model. We derive a\nfamily of iterative expectationmaximization (EM) algorithms to estimate the\nparameters of each model and propose suitable procedures to initialize the\nparameters and to align the order of the estimated sources across all frequency\nbins based on their estimated directions of arrival (DOA). Experimental results\nover reverberant synthetic mixtures and live recordings of speech data show the\neffectiveness of the proposed approach.\n","vector":null,"chunk_id":"1741d699f64df0dd30e708973ad3d19e"}
{"title":"Web Document Analysis for Companies Listed in Bursa Malaysia","authors":"Mohd Shahizan Othman, Lizawati Mi Yusuf, Juhana Salim","category":"cs.IR","abstract":"  This paper discusses a research on web document analysis for companies listed\non Bursa Malaysia which is the forerunner of financial and investment center in\nMalaysia. Data set used in this research are from the company web documents\nlisted in the Main Board and Second Board on Bursa Malaysia. This research has\nused the Web Resources Extraction System which was developed by the research\ngroup mainly to extract information for the web documents involved. Our\nresearch findings have shown that the level of website usage among the\ncompanies on Bursa Malaysia is still minimal. Furthermore, research has also\nfound that 60.02 percent of the image files are utilized making it the most\nused type of file in creating websites.\n","text":"Title:Web Document Analysis for Companies Listed in Bursa Malaysia\nAbstract:  This paper discusses a research on web document analysis for companies listed\non Bursa Malaysia which is the forerunner of financial and investment center in\nMalaysia. Data set used in this research are from the company web documents\nlisted in the Main Board and Second Board on Bursa Malaysia. This research has\nused the Web Resources Extraction System which was developed by the research\ngroup mainly to extract information for the web documents involved. Our\nresearch findings have shown that the level of website usage among the\ncompanies on Bursa Malaysia is still minimal. Furthermore, research has also\nfound that 60.02 percent of the image files are utilized making it the most\nused type of file in creating websites.\n","vector":null,"chunk_id":"35e4bf562fd6a9fa0212b57520e5d5aa"}
{"title":"Conception d'un outil d'aide \\`a l'indexation de ressources\n  p\\'edagogiques - Extraction automatique des th\\'ematiques et des mots-clefs\n  de documents UNIT","authors":"Carlo Abi Chahine (LITIS), Jean-Philippe Kotowicz (LITIS), Nathalie\n  Chaignaud (LITIS), Jean-Pierre P\\'ecuchet (LITIS)","category":"cs.IR","abstract":"  Indexing learning documents using the Learning Object Metadata (LOM) is often\ncarried out manually by archivists. Filling out the LOM fields is a long and\ndifficult task, requiring a complete reading and a full knowledge on the topic\ndealt within the document. In this paper, we present an innovative model and\nmethod to assist the archivists in finding the important concepts and keywords\nof a learning document. The application is performed using wikipedia's category\nlinks.\n","text":"Title:Conception d'un outil d'aide \\`a l'indexation de ressources\n  p\\'edagogiques - Extraction automatique des th\\'ematiques et des mots-clefs\n  de documents UNIT\nAbstract:  Indexing learning documents using the Learning Object Metadata (LOM) is often\ncarried out manually by archivists. Filling out the LOM fields is a long and\ndifficult task, requiring a complete reading and a full knowledge on the topic\ndealt within the document. In this paper, we present an innovative model and\nmethod to assist the archivists in finding the important concepts and keywords\nof a learning document. The application is performed using wikipedia's category\nlinks.\n","vector":null,"chunk_id":"0096d02948bdf994ad00247e3339a753"}
{"title":"Context and Keyword Extraction in Plain Text Using a Graph\n  Representation","authors":"Carlo Abi Chahine (LITIS), Nathalie Chaignaud (LITIS), Jean-Philippe\n  Kotowicz (LITIS), Jean-Pierre P\\'ecuchet (LITIS)","category":"cs.IR","abstract":"  Document indexation is an essential task achieved by archivists or automatic\nindexing tools. To retrieve relevant documents to a query, keywords describing\nthis document have to be carefully chosen. Archivists have to find out the\nright topic of a document before starting to extract the keywords. For an\narchivist indexing specialized documents, experience plays an important role.\nBut indexing documents on different topics is much harder. This article\nproposes an innovative method for an indexing support system. This system takes\nas input an ontology and a plain text document and provides as output\ncontextualized keywords of the document. The method has been evaluated by\nexploiting Wikipedia's category links as a termino-ontological resources.\n","text":"Title:Context and Keyword Extraction in Plain Text Using a Graph\n  Representation\nAbstract:  Document indexation is an essential task achieved by archivists or automatic\nindexing tools. To retrieve relevant documents to a query, keywords describing\nthis document have to be carefully chosen. Archivists have to find out the\nright topic of a document before starting to extract the keywords. For an\narchivist indexing specialized documents, experience plays an important role.\nBut indexing documents on different topics is much harder. This article\nproposes an innovative method for an indexing support system. This system takes\nas input an ontology and a plain text document and provides as output\ncontextualized keywords of the document. The method has been evaluated by\nexploiting Wikipedia's category links as a termino-ontological resources.\n","vector":null,"chunk_id":"50f78a0dd17649affe0d8237771804c8"}
{"title":"Hyper-sparse optimal aggregation","authors":"St\\'ephane Ga\\\"iffas, Guillaume Lecu\\'e","category":"stat.ML","abstract":"  In this paper, we consider the problem of \"hyper-sparse aggregation\". Namely,\ngiven a dictionary $F = \\{f_1, ..., f_M \\}$ of functions, we look for an\noptimal aggregation algorithm that writes $\\tilde f = \\sum_{j=1}^M \\theta_j\nf_j$ with as many zero coefficients $\\theta_j$ as possible. This problem is of\nparticular interest when $F$ contains many irrelevant functions that should not\nappear in $\\tilde{f}$. We provide an exact oracle inequality for $\\tilde f$,\nwhere only two coefficients are non-zero, that entails $\\tilde f$ to be an\noptimal aggregation algorithm. Since selectors are suboptimal aggregation\nprocedures, this proves that 2 is the minimal number of elements of $F$\nrequired for the construction of an optimal aggregation procedures in every\nsituations. A simulated example of this algorithm is proposed on a dictionary\nobtained using LARS, for the problem of selection of the regularization\nparameter of the LASSO. We also give an example of use of aggregation to\nachieve minimax adaptation over anisotropic Besov spaces, which was not\npreviously known in minimax theory (in regression on a random design).\n","text":"Title:Hyper-sparse optimal aggregation\nAbstract:  In this paper, we consider the problem of \"hyper-sparse aggregation\". Namely,\ngiven a dictionary $F = \\{f_1, ..., f_M \\}$ of functions, we look for an\noptimal aggregation algorithm that writes $\\tilde f = \\sum_{j=1}^M \\theta_j\nf_j$ with as many zero coefficients $\\theta_j$ as possible. This problem is of\nparticular interest when $F$ contains many irrelevant functions that should not\nappear in $\\tilde{f}$. We provide an exact oracle inequality for $\\tilde f$,\nwhere only two coefficients are non-zero, that entails $\\tilde f$ to be an\noptimal aggregation algorithm. Since selectors are suboptimal aggregation\nprocedures, this proves that 2 is the minimal number of elements of $F$\nrequired for the construction of an optimal aggregation procedures in every\nsituations. A simulated example of this algorithm is proposed on a dictionary\nobtained using LARS, for the problem of selection of the regularization\nparameter of the LASSO. We also give an example of use of aggregation to\nachieve minimax adaptation over anisotropic Besov spaces, which was not\npreviously known in minimax theory (in regression on a random design).\n","vector":null,"chunk_id":"49da4e4314d6179452c57085f22c503d"}
{"title":"Multi-Way, Multi-View Learning","authors":"Ilkka Huopaniemi, Tommi Suvitaival, Janne Nikkil\\\"a, Matej\n  Ore\\v{s}i\\v{c}, Samuel Kaski","category":"stat.ML","abstract":"  We extend multi-way, multivariate ANOVA-type analysis to cases where one\ncovariate is the view, with features of each view coming from different,\nhigh-dimensional domains. The different views are assumed to be connected by\nhaving paired samples; this is a common setup in recent bioinformatics\nexperiments, of which we analyze metabolite profiles in different conditions\n(disease vs. control and treatment vs. untreated) in different tissues (views).\nWe introduce a multi-way latent variable model for this new task, by extending\nthe generative model of Bayesian canonical correlation analysis (CCA) both to\ntake multi-way covariate information into account as population priors, and by\nreducing the dimensionality by an integrated factor analysis that assumes the\nmetabolites to come in correlated groups.\n","text":"Title:Multi-Way, Multi-View Learning\nAbstract:  We extend multi-way, multivariate ANOVA-type analysis to cases where one\ncovariate is the view, with features of each view coming from different,\nhigh-dimensional domains. The different views are assumed to be connected by\nhaving paired samples; this is a common setup in recent bioinformatics\nexperiments, of which we analyze metabolite profiles in different conditions\n(disease vs. control and treatment vs. untreated) in different tissues (views).\nWe introduce a multi-way latent variable model for this new task, by extending\nthe generative model of Bayesian canonical correlation analysis (CCA) both to\ntake multi-way covariate information into account as population priors, and by\nreducing the dimensionality by an integrated factor analysis that assumes the\nmetabolites to come in correlated groups.\n","vector":null,"chunk_id":"a3a31ef44908df1861bb60873318189d"}
{"title":"Variational Inducing Kernels for Sparse Convolved Multiple Output\n  Gaussian Processes","authors":"Mauricio A. \\'Alvarez, David Luengo, Michalis K. Titsias, Neil D.\n  Lawrence","category":"stat.ML","abstract":"  Interest in multioutput kernel methods is increasing, whether under the guise\nof multitask learning, multisensor networks or structured output data. From the\nGaussian process perspective a multioutput Mercer kernel is a covariance\nfunction over correlated output functions. One way of constructing such kernels\nis based on convolution processes (CP). A key problem for this approach is\nefficient inference. Alvarez and Lawrence (2009) recently presented a sparse\napproximation for CPs that enabled efficient inference. In this paper, we\nextend this work in two directions: we introduce the concept of variational\ninducing functions to handle potential non-smooth functions involved in the\nkernel CP construction and we consider an alternative approach to approximate\ninference based on variational methods, extending the work by Titsias (2009) to\nthe multiple output case. We demonstrate our approaches on prediction of school\nmarks, compiler performance and financial time series.\n","text":"Title:Variational Inducing Kernels for Sparse Convolved Multiple Output\n  Gaussian Processes\nAbstract:  Interest in multioutput kernel methods is increasing, whether under the guise\nof multitask learning, multisensor networks or structured output data. From the\nGaussian process perspective a multioutput Mercer kernel is a covariance\nfunction over correlated output functions. One way of constructing such kernels\nis based on convolution processes (CP). A key problem for this approach is\nefficient inference. Alvarez and Lawrence (2009) recently presented a sparse\napproximation for CPs that enabled efficient inference. In this paper, we\nextend this work in two directions: we introduce the concept of variational\ninducing functions to handle potential non-smooth functions involved in the\nkernel CP construction and we consider an alternative approach to approximate\ninference based on variational methods, extending the work by Titsias (2009) to\nthe multiple output case. We demonstrate our approaches on prediction of school\nmarks, compiler performance and financial time series.\n","vector":null,"chunk_id":"b996ebd68711f8cd1a353ffc959e97d1"}
{"title":"Composite Binary Losses","authors":"Mark D. Reid, Robert C. Williamson","category":"stat.ML","abstract":"  We study losses for binary classification and class probability estimation\nand extend the understanding of them from margin losses to general composite\nlosses which are the composition of a proper loss with a link function. We\ncharacterise when margin losses can be proper composite losses, explicitly show\nhow to determine a symmetric loss in full from half of one of its partial\nlosses, introduce an intrinsic parametrisation of composite binary losses and\ngive a complete characterisation of the relationship between proper losses and\n``classification calibrated'' losses. We also consider the question of the\n``best'' surrogate binary loss. We introduce a precise notion of ``best'' and\nshow there exist situations where two convex surrogate losses are\nincommensurable. We provide a complete explicit characterisation of the\nconvexity of composite binary losses in terms of the link function and the\nweight function associated with the proper loss which make up the composite\nloss. This characterisation suggests new ways of ``surrogate tuning''. Finally,\nin an appendix we present some new algorithm-independent results on the\nrelationship between properness, convexity and robustness to misclassification\nnoise for binary losses and show that all convex proper losses are non-robust\nto misclassification noise.\n","text":"Title:Composite Binary Losses\nAbstract:  We study losses for binary classification and class probability estimation\nand extend the understanding of them from margin losses to general composite\nlosses which are the composition of a proper loss with a link function. We\ncharacterise when margin losses can be proper composite losses, explicitly show\nhow to determine a symmetric loss in full from half of one of its partial\nlosses, introduce an intrinsic parametrisation of composite binary losses and\ngive a complete characterisation of the relationship between proper losses and\n``classification calibrated'' losses. We also consider the question of the\n``best'' surrogate binary loss. We introduce a precise notion of ``best'' and\nshow there exist situations where two convex surrogate losses are\nincommensurable. We provide a complete explicit characterisation of the\nconvexity of composite binary losses in terms of the link function and the\nweight function associated with the proper loss which make up the composite\nloss. This characterisation suggests new ways of ``surrogate tuning''. Finally,\nin an appendix we present some new algorithm-independent results on the\nrelationship between properness, convexity and robustness to misclassification\nnoise for binary losses and show that all convex proper losses are non-robust\nto misclassification noise.\n","vector":null,"chunk_id":"3bffe9dddb458ea5e9dd2ae0c0a6886d"}
{"title":"A Geometric Proof of Calibration","authors":"Shie Mannor (EE-Technion), Gilles Stoltz (DMA, GREGH)","category":"stat.ML","abstract":"  We provide yet another proof of the existence of calibrated forecasters; it\nhas two merits. First, it is valid for an arbitrary finite number of outcomes.\nSecond, it is short and simple and it follows from a direct application of\nBlackwell's approachability theorem to carefully chosen vector-valued payoff\nfunction and convex target set. Our proof captures the essence of existing\nproofs based on approachability (e.g., the proof by Foster, 1999 in case of\nbinary outcomes) and highlights the intrinsic connection between\napproachability and calibration.\n","text":"Title:A Geometric Proof of Calibration\nAbstract:  We provide yet another proof of the existence of calibrated forecasters; it\nhas two merits. First, it is valid for an arbitrary finite number of outcomes.\nSecond, it is short and simple and it follows from a direct application of\nBlackwell's approachability theorem to carefully chosen vector-valued payoff\nfunction and convex target set. Our proof captures the essence of existing\nproofs based on approachability (e.g., the proof by Foster, 1999 in case of\nbinary outcomes) and highlights the intrinsic connection between\napproachability and calibration.\n","vector":null,"chunk_id":"722f8f161db0be3a6215fb25d3ec2a4c"}
{"title":"Realization of Semantic Atom Blog","authors":"Dhiren R. Patel, Sidheshwar A. Khuba","category":"cs.IR","abstract":"  Web blog is used as a collaborative platform to publish and share\ninformation. The information accumulated in the blog intrinsically contains the\nknowledge. The knowledge shared by the community of people has intangible value\nproposition. The blog is viewed as a multimedia information resource available\non the Internet. In a blog, information in the form of text, image, audio and\nvideo builds up exponentially. The multimedia information contained in an Atom\nblog does not have the capability, which is required by the software processes\nso that Atom blog content can be accessed, processed and reused over the\nInternet. This shortcoming is addressed by exploring OWL knowledge modeling,\nsemantic annotation and semantic categorization techniques in an Atom blog\nsphere. By adopting these techniques, futuristic Atom blogs can be created and\ndeployed over the Internet.\n","text":"Title:Realization of Semantic Atom Blog\nAbstract:  Web blog is used as a collaborative platform to publish and share\ninformation. The information accumulated in the blog intrinsically contains the\nknowledge. The knowledge shared by the community of people has intangible value\nproposition. The blog is viewed as a multimedia information resource available\non the Internet. In a blog, information in the form of text, image, audio and\nvideo builds up exponentially. The multimedia information contained in an Atom\nblog does not have the capability, which is required by the software processes\nso that Atom blog content can be accessed, processed and reused over the\nInternet. This shortcoming is addressed by exploring OWL knowledge modeling,\nsemantic annotation and semantic categorization techniques in an Atom blog\nsphere. By adopting these techniques, futuristic Atom blogs can be created and\ndeployed over the Internet.\n","vector":null,"chunk_id":"8211df91b1a5d765c92dd1c008d2fe8a"}
{"title":"Learning the Structure of Deep Sparse Graphical Models","authors":"Ryan Prescott Adams, Hanna M. Wallach, Zoubin Ghahramani","category":"stat.ML","abstract":"  Deep belief networks are a powerful way to model complex probability\ndistributions. However, learning the structure of a belief network,\nparticularly one with hidden units, is difficult. The Indian buffet process has\nbeen used as a nonparametric Bayesian prior on the directed structure of a\nbelief network with a single infinitely wide hidden layer. In this paper, we\nintroduce the cascading Indian buffet process (CIBP), which provides a\nnonparametric prior on the structure of a layered, directed belief network that\nis unbounded in both depth and width, yet allows tractable inference. We use\nthe CIBP prior with the nonlinear Gaussian belief network so each unit can\nadditionally vary its behavior between discrete and continuous representations.\nWe provide Markov chain Monte Carlo algorithms for inference in these belief\nnetworks and explore the structures learned on several image data sets.\n","text":"Title:Learning the Structure of Deep Sparse Graphical Models\nAbstract:  Deep belief networks are a powerful way to model complex probability\ndistributions. However, learning the structure of a belief network,\nparticularly one with hidden units, is difficult. The Indian buffet process has\nbeen used as a nonparametric Bayesian prior on the directed structure of a\nbelief network with a single infinitely wide hidden layer. In this paper, we\nintroduce the cascading Indian buffet process (CIBP), which provides a\nnonparametric prior on the structure of a layered, directed belief network that\nis unbounded in both depth and width, yet allows tractable inference. We use\nthe CIBP prior with the nonlinear Gaussian belief network so each unit can\nadditionally vary its behavior between discrete and continuous representations.\nWe provide Markov chain Monte Carlo algorithms for inference in these belief\nnetworks and explore the structures learned on several image data sets.\n","vector":null,"chunk_id":"ecd55fcfed0d34729e00259b316166b3"}
{"title":"Forest Density Estimation","authors":"Han Liu, Min Xu, Haijie Gu, Anupam Gupta, John Lafferty, Larry\n  Wasserman","category":"stat.ML","abstract":"  We study graph estimation and density estimation in high dimensions, using a\nfamily of density estimators based on forest structured undirected graphical\nmodels. For density estimation, we do not assume the true distribution\ncorresponds to a forest; rather, we form kernel density estimates of the\nbivariate and univariate marginals, and apply Kruskal's algorithm to estimate\nthe optimal forest on held out data. We prove an oracle inequality on the\nexcess risk of the resulting estimator relative to the risk of the best forest.\nFor graph estimation, we consider the problem of estimating forests with\nrestricted tree sizes. We prove that finding a maximum weight spanning forest\nwith restricted tree size is NP-hard, and develop an approximation algorithm\nfor this problem. Viewing the tree size as a complexity parameter, we then\nselect a forest using data splitting, and prove bounds on excess risk and\nstructure selection consistency of the procedure. Experiments with simulated\ndata and microarray data indicate that the methods are a practical alternative\nto Gaussian graphical models.\n","text":"Title:Forest Density Estimation\nAbstract:  We study graph estimation and density estimation in high dimensions, using a\nfamily of density estimators based on forest structured undirected graphical\nmodels. For density estimation, we do not assume the true distribution\ncorresponds to a forest; rather, we form kernel density estimates of the\nbivariate and univariate marginals, and apply Kruskal's algorithm to estimate\nthe optimal forest on held out data. We prove an oracle inequality on the\nexcess risk of the resulting estimator relative to the risk of the best forest.\nFor graph estimation, we consider the problem of estimating forests with\nrestricted tree sizes. We prove that finding a maximum weight spanning forest\nwith restricted tree size is NP-hard, and develop an approximation algorithm\nfor this problem. Viewing the tree size as a complexity parameter, we then\nselect a forest using data splitting, and prove bounds on excess risk and\nstructure selection consistency of the procedure. Experiments with simulated\ndata and microarray data indicate that the methods are a practical alternative\nto Gaussian graphical models.\n","vector":null,"chunk_id":"fb78853b343c838180036d22f126fc6e"}
{"title":"On Utilization and Importance of Perl Status Reporter (SRr) in Text\n  Mining","authors":"Sugam Sharma, Tzusheng Pei, and Hari Cohly","category":"cs.IR","abstract":"  In Bioinformatics, text mining and text data mining sometimes interchangeably\nused is a process to derive high-quality information from text. Perl Status\nReporter (SRr) is a data fetching tool from a flat text file and in this\nresearch paper we illustrate the use of SRr in text or data mining. SRr needs a\nflat text input file where the mining process to be performed. SRr reads input\nfile and derives the high quality information from it. Typically text mining\ntasks are text categorization, text clustering, concept and entity extraction,\nand document summarization. SRr can be utilized for any of these tasks with\nlittle or none customizing efforts. In our implementation we perform text\ncategorization mining operation on input file. The input file has two\nparameters of interest (firstKey and secondKey). The composition of these two\nparameters describes the uniqueness of entries in that file in the similar\nmanner as done by composite key in database. SRr reads the input file line by\nline and extracts the parameters of interest and form a composite key by\njoining them together. It subsequently generates an output file consisting of\nthe name as firstKey secondKey. SRr reads the input file and tracks the\ncomposite key. It further stores all that data lines, having the same composite\nkey, in output file generated by SRr based on that composite key.\n","text":"Title:On Utilization and Importance of Perl Status Reporter (SRr) in Text\n  Mining\nAbstract:  In Bioinformatics, text mining and text data mining sometimes interchangeably\nused is a process to derive high-quality information from text. Perl Status\nReporter (SRr) is a data fetching tool from a flat text file and in this\nresearch paper we illustrate the use of SRr in text or data mining. SRr needs a\nflat text input file where the mining process to be performed. SRr reads input\nfile and derives the high quality information from it. Typically text mining\ntasks are text categorization, text clustering, concept and entity extraction,\nand document summarization. SRr can be utilized for any of these tasks with\nlittle or none customizing efforts. In our implementation we perform text\ncategorization mining operation on input file. The input file has two\nparameters of interest (firstKey and secondKey). The composition of these two\nparameters describes the uniqueness of entries in that file in the similar\nmanner as done by composite key in database. SRr reads the input file line by\nline and extracts the parameters of interest and form a composite key by\njoining them together. It subsequently generates an output file consisting of\nthe name as firstKey secondKey. SRr reads the input file and tracks the\ncomposite key. It further stores all that data lines, having the same composite\nkey, in output file generated by SRr based on that composite key.\n","vector":null,"chunk_id":"ed3173062b1e81f6eb4b5ab1fbcefdf7"}
{"title":"Learning to Blend by Relevance","authors":"Jiang Chen, Wei Chu, Zhenzhen Kou, Zhaohui Zheng","category":"cs.IR","abstract":"  Emergence of various vertical search engines highlights the fact that a\nsingle ranking technology cannot deal with the complexity and scale of search\nproblems. For example, technology behind video and image search is very\ndifferent from general web search. Their ranking functions share few features.\nQuestion answering websites (e.g., Yahoo! Answer) can make use of text matching\nand click features developed for general web, but they have unique page\nstructures and rich user feedback, e.g., thumbs up and thumbs down ratings in\nYahoo! answer, which greatly benefit their own ranking. Even for those features\nshared by answer and general web, the correlation between features and\nrelevance could be very different. Therefore, dedicated functions are needed in\norder to better rank documents within individual domains. These dedicated\nfunctions are defined on distinct feature spaces. However, having one search\nbox for each domain, is neither efficient nor scalable. Rather than typing the\nsame query two times into both Yahoo! Search and Yahoo! Answer and retrieving\ntwo ranking lists, we would prefer putting it only once but receiving a\ncomprehensive list of documents from both domains on the subject. This\nsituation calls for new technology that blends documents from different sources\ninto a single ranking list. Despite the content richness of the blended list,\nit has to be sorted by relevance none the less. We call such technology\nblending, which is the main subject of this paper.\n","text":"Title:Learning to Blend by Relevance\nAbstract:  Emergence of various vertical search engines highlights the fact that a\nsingle ranking technology cannot deal with the complexity and scale of search\nproblems. For example, technology behind video and image search is very\ndifferent from general web search. Their ranking functions share few features.\nQuestion answering websites (e.g., Yahoo! Answer) can make use of text matching\nand click features developed for general web, but they have unique page\nstructures and rich user feedback, e.g., thumbs up and thumbs down ratings in\nYahoo! answer, which greatly benefit their own ranking. Even for those features\nshared by answer and general web, the correlation between features and\nrelevance could be very different. Therefore, dedicated functions are needed in\norder to better rank documents within individual domains. These dedicated\nfunctions are defined on distinct feature spaces. However, having one search\nbox for each domain, is neither efficient nor scalable. Rather than typing the\nsame query two times into both Yahoo! Search and Yahoo! Answer and retrieving\ntwo ranking lists, we would prefer putting it only once but receiving a\ncomprehensive list of documents from both domains on the subject. This\nsituation calls for new technology that blends documents from different sources\ninto a single ranking list. Despite the content richness of the blended list,\nit has to be sorted by relevance none the less. We call such technology\nblending, which is the main subject of this paper.\n","vector":null,"chunk_id":"65f9643270c465f5c250e14856371866"}
{"title":"Extraction de termes, reconnaissance et labellisation de relations dans\n  un th\\'esaurus","authors":"Marie-No\\\"elle Bessagnet (LIUPPA), Eric Kergosien (LIUPPA), Mauro Gaio\n  (LIUPPA)","category":"cs.IR","abstract":"  Within the documentary system domain, the integration of thesauri for\nindexing and retrieval information steps is usual. In libraries, documents own\nrich descriptive information made by librarians, under descriptive notice based\non Rameau thesaurus. We exploit two kinds of information in order to create a\nfirst semantic structure. A step of conceptualization allows us to define the\nvarious modules used to automatically build the semantic structure of the\nindexation work. Our current work focuses on an approach that aims to define an\nontology based on a thesaurus. We hope to integrate new knowledge\ncharacterizing the territory of our structure (adding \"toponyms\" and links\nbetween concepts) thanks to a geographic information system (GIS).\n","text":"Title:Extraction de termes, reconnaissance et labellisation de relations dans\n  un th\\'esaurus\nAbstract:  Within the documentary system domain, the integration of thesauri for\nindexing and retrieval information steps is usual. In libraries, documents own\nrich descriptive information made by librarians, under descriptive notice based\non Rameau thesaurus. We exploit two kinds of information in order to create a\nfirst semantic structure. A step of conceptualization allows us to define the\nvarious modules used to automatically build the semantic structure of the\nindexation work. Our current work focuses on an approach that aims to define an\nontology based on a thesaurus. We hope to integrate new knowledge\ncharacterizing the territory of our structure (adding \"toponyms\" and links\nbetween concepts) thanks to a geographic information system (GIS).\n","vector":null,"chunk_id":"811133eaea53c42d2479eda275171998"}
{"title":"Construction et enrichissement automatique d'ontologie \\`a partir de\n  ressources externes","authors":"Eric Kergosien (LIUPPA), Mouna Kamel (IRIT), Christian Sallaberry\n  (LIUPPA), Marie-No\\\"elle Bessagnet (LIUPPA), Nathalie Aussenac- Gilles\n  (IRIT), Mauro Gaio (LIUPPA)","category":"cs.IR","abstract":"  Automatic construction of ontologies from text is generally based on\nretrieving text content. For a much more rich ontology we extend these\napproaches by taking into account the document structure and some external\nresources (like thesaurus of indexing terms of near domain). In this paper we\ndescribe how these external resources are at first analyzed and then exploited.\nThis method has been applied on a geographical domain and the benefit has been\nevaluated.\n","text":"Title:Construction et enrichissement automatique d'ontologie \\`a partir de\n  ressources externes\nAbstract:  Automatic construction of ontologies from text is generally based on\nretrieving text content. For a much more rich ontology we extend these\napproaches by taking into account the document structure and some external\nresources (like thesaurus of indexing terms of near domain). In this paper we\ndescribe how these external resources are at first analyzed and then exploited.\nThis method has been applied on a geographical domain and the benefit has been\nevaluated.\n","vector":null,"chunk_id":"5e62763f85d711d54095174267bd2424"}
{"title":"Recherche de relations spatio-temporelles : une m\\'ethode bas\\'ee sur\n  l'analyse de corpus textuels","authors":"Tien Nguyen Van (LIUPPA), Mauro Gaio (LIUPPA), Christian Sallaberry\n  (LIUPPA)","category":"cs.IR","abstract":"  This paper presents a work package realized for the G\\'eOnto project. A new\nmethod is proposed for an enrichment of a first geographical ontology developed\nbeforehand. This method relies on text analysis by lexico-syntactic patterns.\n  From the retrieve of n-ary relations the method automatically detect those\ninvolved in a spatial and/or temporal relation in a context of a description of\njourneys.\n","text":"Title:Recherche de relations spatio-temporelles : une m\\'ethode bas\\'ee sur\n  l'analyse de corpus textuels\nAbstract:  This paper presents a work package realized for the G\\'eOnto project. A new\nmethod is proposed for an enrichment of a first geographical ontology developed\nbeforehand. This method relies on text analysis by lexico-syntactic patterns.\n  From the retrieve of n-ary relations the method automatically detect those\ninvolved in a spatial and/or temporal relation in a context of a description of\njourneys.\n","vector":null,"chunk_id":"bf3aea3fad93a131b1a31ba76d1a2ab9"}
{"title":"Probabilistic Recovery of Multiple Subspaces in Point Clouds by\n  Geometric lp Minimization","authors":"Gilad Lerman and Teng Zhang","category":"stat.ML","abstract":"  We assume data independently sampled from a mixture distribution on the unit\nball of the D-dimensional Euclidean space with K+1 components: the first\ncomponent is a uniform distribution on that ball representing outliers and the\nother K components are uniform distributions along K d-dimensional linear\nsubspaces restricted to that ball. We study both the simultaneous recovery of\nall K underlying subspaces and the recovery of the best l0 subspace (i.e., with\nlargest number of points) by minimizing the lp-averaged distances of data\npoints from d-dimensional subspaces of the D-dimensional space. Unlike other lp\nminimization problems, this minimization is non-convex for all p>0 and thus\nrequires different methods for its analysis. We show that if 0<p <= 1, then\nboth all underlying subspaces and the best l0 subspace can be precisely\nrecovered by lp minimization with overwhelming probability. This result extends\nto additive homoscedastic uniform noise around the subspaces (i.e., uniform\ndistribution in a strip around them) and near recovery with an error\nproportional to the noise level. On the other hand, if K>1 and p>1, then we\nshow that both all underlying subspaces and the best l0 subspace cannot be\nrecovered and even nearly recovered. Further relaxations are also discussed. We\nuse the results of this paper for partially justifying recent effective\nalgorithms for modeling data by mixtures of multiple subspaces as well as for\ndiscussing the effect of using variants of lp minimizations in RANSAC-type\nstrategies for single subspace recovery.\n","text":"Title:Probabilistic Recovery of Multiple Subspaces in Point Clouds by\n  Geometric lp Minimization\nAbstract:  We assume data independently sampled from a mixture distribution on the unit\nball of the D-dimensional Euclidean space with K+1 components: the first\ncomponent is a uniform distribution on that ball representing outliers and the\nother K components are uniform distributions along K d-dimensional linear\nsubspaces restricted to that ball. We study both the simultaneous recovery of\nall K underlying subspaces and the recovery of the best l0 subspace (i.e., with\nlargest number of points) by minimizing the lp-averaged distances of data\npoints from d-dimensional subspaces of the D-dimensional space. Unlike other lp\nminimization problems, this minimization is non-convex for all p>0 and thus\nrequires different methods for its analysis. We show that if 0<p <= 1, then\nboth all underlying subspaces and the best l0 subspace can be precisely\nrecovered by lp minimization with overwhelming probability. This result extends\nto additive homoscedastic uniform noise around the subspaces (i.e., uniform\ndistribution in a strip around them) and near recovery with an error\nproportional to the noise level. On the other hand, if K>1 and p>1, then we\nshow that both all underlying subspaces and the best l0 subspace cannot be\nrecovered and even nearly recovered. Further relaxations are also discussed. We\nuse the results of this paper for partially justifying recent effective\nalgorithms for modeling data by mixtures of multiple subspaces as well as for\ndiscussing the effect of using variants of lp minimizations in RANSAC-type\nstrategies for single subspace recovery.\n","vector":null,"chunk_id":"5394e07e45d24a35a095e8b6fa3030c1"}
{"title":"Using Web Page Titles to Rediscover Lost Web Pages","authors":"Jeffery L. Shipman, Martin Klein, Michael L. Nelson","category":"cs.IR","abstract":"  Titles are denoted by the TITLE element within a web page. We queried the\ntitle against the the Yahoo search engine to determine the page's status\n(found, not found). We conducted several tests based on elements of the title.\nThese tests were used to discern whether we could predict a pages status based\non the title. Our results increase our ability to determine bad titles but not\nour ability to determine good titles.\n","text":"Title:Using Web Page Titles to Rediscover Lost Web Pages\nAbstract:  Titles are denoted by the TITLE element within a web page. We queried the\ntitle against the the Yahoo search engine to determine the page's status\n(found, not found). We conducted several tests based on elements of the title.\nThese tests were used to discern whether we could predict a pages status based\non the title. Our results increase our ability to determine bad titles but not\nour ability to determine good titles.\n","vector":null,"chunk_id":"9be6ed7af06ec4fbb1972259029f5b7e"}
{"title":"Exploring a Multidimensional Representation of Documents and Queries\n  (extended version)","authors":"Benjamin Piwowarski and Ingo Frommholz and Mounia Lalmas and Keith van\n  Rijsbergen","category":"cs.IR","abstract":"  In Information Retrieval (IR), whether implicitly or explicitly, queries and\ndocuments are often represented as vectors. However, it may be more beneficial\nto consider documents and/or queries as multidimensional objects. Our belief is\nthis would allow building \"truly\" interactive IR systems, i.e., where\ninteraction is fully incorporated in the IR framework.\n  The probabilistic formalism of quantum physics represents events and\ndensities as multidimensional objects. This paper presents our first step\ntowards building an interactive IR framework upon this formalism, by stating\nhow the first interaction of the retrieval process, when the user types a\nquery, can be formalised. Our framework depends on a number of parameters\naffecting the final document ranking. In this paper we experimentally\ninvestigate the effect of these parameters, showing that the proposed\nrepresentation of documents and queries as multidimensional objects can compete\nwith standard approaches, with the additional prospect to be applied to\ninteractive retrieval.\n","text":"Title:Exploring a Multidimensional Representation of Documents and Queries\n  (extended version)\nAbstract:  In Information Retrieval (IR), whether implicitly or explicitly, queries and\ndocuments are often represented as vectors. However, it may be more beneficial\nto consider documents and/or queries as multidimensional objects. Our belief is\nthis would allow building \"truly\" interactive IR systems, i.e., where\ninteraction is fully incorporated in the IR framework.\n  The probabilistic formalism of quantum physics represents events and\ndensities as multidimensional objects. This paper presents our first step\ntowards building an interactive IR framework upon this formalism, by stating\nhow the first interaction of the retrieval process, when the user types a\nquery, can be formalised. Our framework depends on a number of parameters\naffecting the final document ranking. In this paper we experimentally\ninvestigate the effect of these parameters, showing that the proposed\nrepresentation of documents and queries as multidimensional objects can compete\nwith standard approaches, with the additional prospect to be applied to\ninteractive retrieval.\n","vector":null,"chunk_id":"6f78cadc6da0ddea6fc746f556cbf37e"}
{"title":"Spectral properties of the Google matrix of the World Wide Web and other\n  directed networks","authors":"B. Georgeot, O. Giraud and D.L. Shepelyansky","category":"cs.IR","abstract":"  We study numerically the spectrum and eigenstate properties of the Google\nmatrix of various examples of directed networks such as vocabulary networks of\ndictionaries and university World Wide Web networks. The spectra have gapless\nstructure in the vicinity of the maximal eigenvalue for Google damping\nparameter $\\alpha$ equal to unity. The vocabulary networks have relatively\nhomogeneous spectral density, while university networks have pronounced\nspectral structures which change from one university to another, reflecting\nspecific properties of the networks. We also determine specific properties of\neigenstates of the Google matrix, including the PageRank. The fidelity of the\nPageRank is proposed as a new characterization of its stability.\n","text":"Title:Spectral properties of the Google matrix of the World Wide Web and other\n  directed networks\nAbstract:  We study numerically the spectrum and eigenstate properties of the Google\nmatrix of various examples of directed networks such as vocabulary networks of\ndictionaries and university World Wide Web networks. The spectra have gapless\nstructure in the vicinity of the maximal eigenvalue for Google damping\nparameter $\\alpha$ equal to unity. The vocabulary networks have relatively\nhomogeneous spectral density, while university networks have pronounced\nspectral structures which change from one university to another, reflecting\nspecific properties of the networks. We also determine specific properties of\neigenstates of the Google matrix, including the PageRank. The fidelity of the\nPageRank is proposed as a new characterization of its stability.\n","vector":null,"chunk_id":"5ce2d833a0a7ed493f2c6a873209180b"}
{"title":"Robust Independent Component Analysis by Iterative Maximization of the\n  Kurtosis Contrast with Algebraic Optimal Step Size","authors":"Vicente Zarzoso, Pierre Comon","category":"stat.ML","abstract":"  Independent component analysis (ICA) aims at decomposing an observed random\nvector into statistically independent variables. Deflation-based\nimplementations, such as the popular one-unit FastICA algorithm and its\nvariants, extract the independent components one after another. A novel method\nfor deflationary ICA, referred to as RobustICA, is put forward in this paper.\nThis simple technique consists of performing exact line search optimization of\nthe kurtosis contrast function. The step size leading to the global maximum of\nthe contrast along the search direction is found among the roots of a\nfourth-degree polynomial. This polynomial rooting can be performed\nalgebraically, and thus at low cost, at each iteration. Among other practical\nbenefits, RobustICA can avoid prewhitening and deals with real- and\ncomplex-valued mixtures of possibly noncircular sources alike. The absence of\nprewhitening improves asymptotic performance. The algorithm is robust to local\nextrema and shows a very high convergence speed in terms of the computational\ncost required to reach a given source extraction quality, particularly for\nshort data records. These features are demonstrated by a comparative numerical\nanalysis on synthetic data. RobustICA's capabilities in processing real-world\ndata involving noncircular complex strongly super-Gaussian sources are\nillustrated by the biomedical problem of atrial activity (AA) extraction in\natrial fibrillation (AF) electrocardiograms (ECGs), where it outperforms an\nalternative ICA-based technique.\n","text":"Title:Robust Independent Component Analysis by Iterative Maximization of the\n  Kurtosis Contrast with Algebraic Optimal Step Size\nAbstract:  Independent component analysis (ICA) aims at decomposing an observed random\nvector into statistically independent variables. Deflation-based\nimplementations, such as the popular one-unit FastICA algorithm and its\nvariants, extract the independent components one after another. A novel method\nfor deflationary ICA, referred to as RobustICA, is put forward in this paper.\nThis simple technique consists of performing exact line search optimization of\nthe kurtosis contrast function. The step size leading to the global maximum of\nthe contrast along the search direction is found among the roots of a\nfourth-degree polynomial. This polynomial rooting can be performed\nalgebraically, and thus at low cost, at each iteration. Among other practical\nbenefits, RobustICA can avoid prewhitening and deals with real- and\ncomplex-valued mixtures of possibly noncircular sources alike. The absence of\nprewhitening improves asymptotic performance. The algorithm is robust to local\nextrema and shows a very high convergence speed in terms of the computational\ncost required to reach a given source extraction quality, particularly for\nshort data records. These features are demonstrated by a comparative numerical\nanalysis on synthetic data. RobustICA's capabilities in processing real-world\ndata involving noncircular complex strongly super-Gaussian sources are\nillustrated by the biomedical problem of atrial activity (AA) extraction in\natrial fibrillation (AF) electrocardiograms (ECGs), where it outperforms an\nalternative ICA-based technique.\n","vector":null,"chunk_id":"d3bd68af5d65426f6b27525882d11230"}
{"title":"Improving Term Extraction Using Particle Swarm Optimization Techniques","authors":"Mohammad Syafrullah, Naomie Salim","category":"cs.IR","abstract":"  Term extraction is one of the layers in the ontology development process\nwhich has the task to extract all the terms contained in the input document\nautomatically. The purpose of this process is to generate list of terms that\nare relevant to the domain of the input document. In the literature there are\nmany approaches, techniques and algorithms used for term extraction. In this\npaper we propose a new approach using particle swarm optimization techniques in\norder to improve the accuracy of term extraction results. We choose five\nfeatures to represent the term score. The approach has been applied to the\ndomain of religious document. We compare our term extraction method precision\nwith TFIDF, Weirdness, GlossaryExtraction and TermExtractor. The experimental\nresults show that our propose approach achieve better precision than those four\nalgorithm.\n","text":"Title:Improving Term Extraction Using Particle Swarm Optimization Techniques\nAbstract:  Term extraction is one of the layers in the ontology development process\nwhich has the task to extract all the terms contained in the input document\nautomatically. The purpose of this process is to generate list of terms that\nare relevant to the domain of the input document. In the literature there are\nmany approaches, techniques and algorithms used for term extraction. In this\npaper we propose a new approach using particle swarm optimization techniques in\norder to improve the accuracy of term extraction results. We choose five\nfeatures to represent the term score. The approach has been applied to the\ndomain of religious document. We compare our term extraction method precision\nwith TFIDF, Weirdness, GlossaryExtraction and TermExtractor. The experimental\nresults show that our propose approach achieve better precision than those four\nalgorithm.\n","vector":null,"chunk_id":"f55f7a75496091f7bb2a268ba6cddd8d"}
{"title":"A Hough Transform based Technique for Text Segmentation","authors":"Satadal Saha, Subhadip Basu, Mita Nasipuri, Dipak Kr. Basu","category":"cs.IR","abstract":"  Text segmentation is an inherent part of an OCR system irrespective of the\ndomain of application of it. The OCR system contains a segmentation module\nwhere the text lines, words and ultimately the characters must be segmented\nproperly for its successful recognition. The present work implements a Hough\ntransform based technique for line and word segmentation from digitized images.\nThe proposed technique is applied not only on the document image dataset but\nalso on dataset for business card reader system and license plate recognition\nsystem. For standardization of the performance of the system the technique is\nalso applied on public domain dataset published in the website by CMATER,\nJadavpur University. The document images consist of multi-script printed and\nhand written text lines with variety in script and line spacing in single\ndocument image. The technique performs quite satisfactorily when applied on\nmobile camera captured business card images with low resolution. The usefulness\nof the technique is verified by applying it in a commercial project for\nlocalization of license plate of vehicles from surveillance camera images by\nthe process of segmentation itself. The accuracy of the technique for word\nsegmentation, as verified experimentally, is 85.7% for document images, 94.6%\nfor business card images and 88% for surveillance camera images.\n","text":"Title:A Hough Transform based Technique for Text Segmentation\nAbstract:  Text segmentation is an inherent part of an OCR system irrespective of the\ndomain of application of it. The OCR system contains a segmentation module\nwhere the text lines, words and ultimately the characters must be segmented\nproperly for its successful recognition. The present work implements a Hough\ntransform based technique for line and word segmentation from digitized images.\nThe proposed technique is applied not only on the document image dataset but\nalso on dataset for business card reader system and license plate recognition\nsystem. For standardization of the performance of the system the technique is\nalso applied on public domain dataset published in the website by CMATER,\nJadavpur University. The document images consist of multi-script printed and\nhand written text lines with variety in script and line spacing in single\ndocument image. The technique performs quite satisfactorily when applied on\nmobile camera captured business card images with low resolution. The usefulness\nof the technique is verified by applying it in a commercial project for\nlocalization of license plate of vehicles from surveillance camera images by\nthe process of segmentation itself. The accuracy of the technique for word\nsegmentation, as verified experimentally, is 85.7% for document images, 94.6%\nfor business card images and 88% for surveillance camera images.\n","vector":null,"chunk_id":"6928c0ecec19942ceee943eecb2ce4ab"}
{"title":"Security Analysis of Online Centroid Anomaly Detection","authors":"Marius Kloft and Pavel Laskov","category":"stat.ML","abstract":"  Security issues are crucial in a number of machine learning applications,\nespecially in scenarios dealing with human activity rather than natural\nphenomena (e.g., information ranking, spam detection, malware detection, etc.).\nIt is to be expected in such cases that learning algorithms will have to deal\nwith manipulated data aimed at hampering decision making. Although some\nprevious work addressed the handling of malicious data in the context of\nsupervised learning, very little is known about the behavior of anomaly\ndetection methods in such scenarios. In this contribution we analyze the\nperformance of a particular method -- online centroid anomaly detection -- in\nthe presence of adversarial noise. Our analysis addresses the following\nsecurity-related issues: formalization of learning and attack processes,\nderivation of an optimal attack, analysis of its efficiency and constraints. We\nderive bounds on the effectiveness of a poisoning attack against centroid\nanomaly under different conditions: bounded and unbounded percentage of\ntraffic, and bounded false positive rate. Our bounds show that whereas a\npoisoning attack can be effectively staged in the unconstrained case, it can be\nmade arbitrarily difficult (a strict upper bound on the attacker's gain) if\nexternal constraints are properly used. Our experimental evaluation carried out\non real HTTP and exploit traces confirms the tightness of our theoretical\nbounds and practicality of our protection mechanisms.\n","text":"Title:Security Analysis of Online Centroid Anomaly Detection\nAbstract:  Security issues are crucial in a number of machine learning applications,\nespecially in scenarios dealing with human activity rather than natural\nphenomena (e.g., information ranking, spam detection, malware detection, etc.).\nIt is to be expected in such cases that learning algorithms will have to deal\nwith manipulated data aimed at hampering decision making. Although some\nprevious work addressed the handling of malicious data in the context of\nsupervised learning, very little is known about the behavior of anomaly\ndetection methods in such scenarios. In this contribution we analyze the\nperformance of a particular method -- online centroid anomaly detection -- in\nthe presence of adversarial noise. Our analysis addresses the following\nsecurity-related issues: formalization of learning and attack processes,\nderivation of an optimal attack, analysis of its efficiency and constraints. We\nderive bounds on the effectiveness of a poisoning attack against centroid\nanomaly under different conditions: bounded and unbounded percentage of\ntraffic, and bounded false positive rate. Our bounds show that whereas a\npoisoning attack can be effectively staged in the unconstrained case, it can be\nmade arbitrarily difficult (a strict upper bound on the attacker's gain) if\nexternal constraints are properly used. Our experimental evaluation carried out\non real HTTP and exploit traces confirms the tightness of our theoretical\nbounds and practicality of our protection mechanisms.\n","vector":null,"chunk_id":"5c60c12da266b0e0fa98937f4ddb4739"}
{"title":"Supervised Topic Models","authors":"David M. Blei and Jon D. McAuliffe","category":"stat.ML","abstract":"  We introduce supervised latent Dirichlet allocation (sLDA), a statistical\nmodel of labelled documents. The model accommodates a variety of response\ntypes. We derive an approximate maximum-likelihood procedure for parameter\nestimation, which relies on variational methods to handle intractable posterior\nexpectations. Prediction problems motivate this research: we use the fitted\nmodel to predict response values for new documents. We test sLDA on two\nreal-world problems: movie ratings predicted from reviews, and the political\ntone of amendments in the U.S. Senate based on the amendment text. We\nillustrate the benefits of sLDA versus modern regularized regression, as well\nas versus an unsupervised LDA analysis followed by a separate regression.\n","text":"Title:Supervised Topic Models\nAbstract:  We introduce supervised latent Dirichlet allocation (sLDA), a statistical\nmodel of labelled documents. The model accommodates a variety of response\ntypes. We derive an approximate maximum-likelihood procedure for parameter\nestimation, which relies on variational methods to handle intractable posterior\nexpectations. Prediction problems motivate this research: we use the fitted\nmodel to predict response values for new documents. We test sLDA on two\nreal-world problems: movie ratings predicted from reviews, and the political\ntone of amendments in the U.S. Senate based on the amendment text. We\nillustrate the benefits of sLDA versus modern regularized regression, as well\nas versus an unsupervised LDA analysis followed by a separate regression.\n","vector":null,"chunk_id":"6002f32637aa44714befeafab5e32a4e"}
{"title":"Tag Clusters as Information Retrieval Interfaces","authors":"Kathrin Knautz, Simone Soubusta, Wolfgang G. Stock","category":"cs.IR","abstract":"  The paper presents our design of a next generation information retrieval\nsystem based on tag co-occurrences and subsequent clustering. We help users\ngetting access to digital data through information visualization in the form of\ntag clusters. Current problems like the absence of interactivity and semantics\nbetween tags or the difficulty of adding additional search arguments are\nsolved. In the evaluation, based upon SERVQUAL and IT systems quality\nindicators, we found out that tag clusters are perceived as more useful than\ntag clouds, are much more trustworthy, and are more enjoyable to use.\n","text":"Title:Tag Clusters as Information Retrieval Interfaces\nAbstract:  The paper presents our design of a next generation information retrieval\nsystem based on tag co-occurrences and subsequent clustering. We help users\ngetting access to digital data through information visualization in the form of\ntag clusters. Current problems like the absence of interactivity and semantics\nbetween tags or the difficulty of adding additional search arguments are\nsolved. In the evaluation, based upon SERVQUAL and IT systems quality\nindicators, we found out that tag clusters are perceived as more useful than\ntag clouds, are much more trustworthy, and are more enjoyable to use.\n","vector":null,"chunk_id":"229f56b78cea49c366c709006fe61dbc"}
{"title":"Ontology Based Query Expansion Using Word Sense Disambiguation","authors":"M. Barathi, S. Valli","category":"cs.IR","abstract":"  The existing information retrieval techniques do not consider the context of\nthe keywords present in the user's queries. Therefore, the search engines\nsometimes do not provide sufficient information to the users. New methods based\non the semantics of user keywords must be developed to search in the vast web\nspace without incurring loss of information. The semantic based information\nretrieval techniques need to understand the meaning of the concepts in the user\nqueries. This will improve the precision-recall of the search results.\nTherefore, this approach focuses on the concept based semantic information\nretrieval. This work is based on Word sense disambiguation, thesaurus WordNet\nand ontology of any domain for retrieving information in order to capture the\ncontext of particular concept(s) and discover semantic relationships between\nthem.\n","text":"Title:Ontology Based Query Expansion Using Word Sense Disambiguation\nAbstract:  The existing information retrieval techniques do not consider the context of\nthe keywords present in the user's queries. Therefore, the search engines\nsometimes do not provide sufficient information to the users. New methods based\non the semantics of user keywords must be developed to search in the vast web\nspace without incurring loss of information. The semantic based information\nretrieval techniques need to understand the meaning of the concepts in the user\nqueries. This will improve the precision-recall of the search results.\nTherefore, this approach focuses on the concept based semantic information\nretrieval. This work is based on Word sense disambiguation, thesaurus WordNet\nand ontology of any domain for retrieving information in order to capture the\ncontext of particular concept(s) and discover semantic relationships between\nthem.\n","vector":null,"chunk_id":"899694720472606c97dd4a9a5d1f4a81"}
{"title":"Formal Concept Analysis for Information Retrieval","authors":"Abderrahim El Qadi, Driss Aboutajedine, Yassine Ennouary","category":"cs.IR","abstract":"  In this paper we describe a mechanism to improve Information Retrieval (IR)\non the web. The method is based on Formal Concepts Analysis (FCA) that it is\nmakes semantical relations during the queries, and allows a reorganizing, in\nthe shape of a lattice of concepts, the answers provided by a search engine. We\nproposed for the IR an incremental algorithm based on Galois lattice. This\nalgorithm allows a formal clustering of the data sources, and the results which\nit turns over are classified by order of relevance. The control of relevance is\nexploited in clustering, we improved the result by using ontology in field of\nimage processing, and reformulating the user queries which make it possible to\ngive more relevant documents.\n","text":"Title:Formal Concept Analysis for Information Retrieval\nAbstract:  In this paper we describe a mechanism to improve Information Retrieval (IR)\non the web. The method is based on Formal Concepts Analysis (FCA) that it is\nmakes semantical relations during the queries, and allows a reorganizing, in\nthe shape of a lattice of concepts, the answers provided by a search engine. We\nproposed for the IR an incremental algorithm based on Galois lattice. This\nalgorithm allows a formal clustering of the data sources, and the results which\nit turns over are classified by order of relevance. The control of relevance is\nexploited in clustering, we improved the result by using ontology in field of\nimage processing, and reformulating the user queries which make it possible to\ngive more relevant documents.\n","vector":null,"chunk_id":"63ab862dbf263782c89cf02a6037cb75"}
{"title":"An Analytical Approach to Document Clustering Based on Internal\n  Criterion Function","authors":"Alok Ranjan, Harish Verma, Eatesh Kandpal, Joydip Dhar","category":"cs.IR","abstract":"  Fast and high quality document clustering is an important task in organizing\ninformation, search engine results obtaining from user query, enhancing web\ncrawling and information retrieval. With the large amount of data available and\nwith a goal of creating good quality clusters, a variety of algorithms have\nbeen developed having quality-complexity trade-offs. Among these, some\nalgorithms seek to minimize the computational complexity using certain\ncriterion functions which are defined for the whole set of clustering solution.\nIn this paper, we are proposing a novel document clustering algorithm based on\nan internal criterion function. Most commonly used partitioning clustering\nalgorithms (e.g. k-means) have some drawbacks as they suffer from local optimum\nsolutions and creation of empty clusters as a clustering solution. The proposed\nalgorithm usually does not suffer from these problems and converge to a global\noptimum, its performance enhances with the increase in number of clusters. We\nhave checked our algorithm against three different datasets for four different\nvalues of k (required number of clusters).\n","text":"Title:An Analytical Approach to Document Clustering Based on Internal\n  Criterion Function\nAbstract:  Fast and high quality document clustering is an important task in organizing\ninformation, search engine results obtaining from user query, enhancing web\ncrawling and information retrieval. With the large amount of data available and\nwith a goal of creating good quality clusters, a variety of algorithms have\nbeen developed having quality-complexity trade-offs. Among these, some\nalgorithms seek to minimize the computational complexity using certain\ncriterion functions which are defined for the whole set of clustering solution.\nIn this paper, we are proposing a novel document clustering algorithm based on\nan internal criterion function. Most commonly used partitioning clustering\nalgorithms (e.g. k-means) have some drawbacks as they suffer from local optimum\nsolutions and creation of empty clusters as a clustering solution. The proposed\nalgorithm usually does not suffer from these problems and converge to a global\noptimum, its performance enhances with the increase in number of clusters. We\nhave checked our algorithm against three different datasets for four different\nvalues of k (required number of clusters).\n","vector":null,"chunk_id":"7d16745544b54435e90969f8392aac91"}
{"title":"Optimal Allocation Strategies for the Dark Pool Problem","authors":"Alekh Agarwal and Peter Bartlett and Max Dama","category":"stat.ML","abstract":"  We study the problem of allocating stocks to dark pools. We propose and\nanalyze an optimal approach for allocations, if continuous-valued allocations\nare allowed. We also propose a modification for the case when only\ninteger-valued allocations are possible. We extend the previous work on this\nproblem to adversarial scenarios, while also improving on their results in the\niid setup. The resulting algorithms are efficient, and perform well in\nsimulations under stochastic and adversarial inputs.\n","text":"Title:Optimal Allocation Strategies for the Dark Pool Problem\nAbstract:  We study the problem of allocating stocks to dark pools. We propose and\nanalyze an optimal approach for allocations, if continuous-valued allocations\nare allowed. We also propose a modification for the case when only\ninteger-valued allocations are possible. We extend the previous work on this\nproblem to adversarial scenarios, while also improving on their results in the\niid setup. The resulting algorithms are efficient, and perform well in\nsimulations under stochastic and adversarial inputs.\n","vector":null,"chunk_id":"2f32bc3fef5ae409e6e75b54c0ec97fa"}
{"title":"Revisiting the Examination Hypothesis with Query Specific Position Bias","authors":"Sreenivas Gollapudi and Rina Panigrahy","category":"cs.IR","abstract":"  Click through rates (CTR) offer useful user feedback that can be used to\ninfer the relevance of search results for queries. However it is not very\nmeaningful to look at the raw click through rate of a search result because the\nlikelihood of a result being clicked depends not only on its relevance but also\nthe position in which it is displayed. One model of the browsing behavior, the\n{\\em Examination Hypothesis} \\cite{RDR07,Craswell08,DP08}, states that each\nposition has a certain probability of being examined and is then clicked based\non the relevance of the search snippets. This is based on eye tracking studies\n\\cite{Claypool01, GJG04} which suggest that users are less likely to view\nresults in lower positions. Such a position dependent variation in the\nprobability of examining a document is referred to as {\\em position bias}. Our\nmain observation in this study is that the position bias tends to differ with\nthe kind of information the user is looking for. This makes the position bias\n{\\em query specific}. In this study, we present a model for analyzing a query\nspecific position bias from the click data and use these biases to derive\nposition independent relevance values of search results. Our model is based on\nthe assumption that for a given query, the positional click through rate of a\ndocument is proportional to the product of its relevance and a {\\em query\nspecific} position bias. We compare our model with the vanilla examination\nhypothesis model (EH) on a set of queries obtained from search logs of a\ncommercial search engine. We also compare it with the User Browsing Model (UBM)\n\\cite{DP08} which extends the cascade model of Craswell et al\\cite{Craswell08}\nby incorporating multiple clicks in a query session. We show that the our\nmodel, although much simpler to implement, consistently outperforms both EH and\nUBM on well-used measures such as relative error and cross entropy.\n","text":"Title:Revisiting the Examination Hypothesis with Query Specific Position Bias\nAbstract:  Click through rates (CTR) offer useful user feedback that can be used to\ninfer the relevance of search results for queries. However it is not very\nmeaningful to look at the raw click through rate of a search result because the\nlikelihood of a result being clicked depends not only on its relevance but also\nthe position in which it is displayed. One model of the browsing behavior, the\n{\\em Examination Hypothesis} \\cite{RDR07,Craswell08,DP08}, states that each\nposition has a certain probability of being examined and is then clicked based\non the relevance of the search snippets. This is based on eye tracking studies\n\\cite{Claypool01, GJG04} which suggest that users are less likely to view\nresults in lower positions. Such a position dependent variation in the\nprobability of examining a document is referred to as {\\em position bias}. Our\nmain observation in this study is that the position bias tends to differ with\nthe kind of information the user is looking for. This makes the position bias\n{\\em query specific}. In this study, we present a model for analyzing a query\nspecific position bias from the click data and use these biases to derive\nposition independent relevance values of search results. Our model is based on\nthe assumption that for a given query, the positional click through rate of a\ndocument is proportional to the product of its relevance and a {\\em query\nspecific} position bias. We compare our model with the vanilla examination\nhypothesis model (EH) on a set of queries obtained from search logs of a\ncommercial search engine. We also compare it with the User Browsing Model (UBM)\n\\cite{DP08} which extends the cascade model of Craswell et al\\cite{Craswell08}\nby incorporating multiple clicks in a query session. We show that the our\nmodel, although much simpler to implement, consistently outperforms both EH and\nUBM on well-used measures such as relative error and cross entropy.\n","vector":null,"chunk_id":"90a696b9ff5ce9d51cf563217646bb7d"}
{"title":"Classified Ads Harvesting Agent and Notification System","authors":"Razvi Doomun, Lollmahamod N., Auleear Nadeem, Mozafar Aukin","category":"cs.IR","abstract":"  The shift from an information society to a knowledge society require rapid\ninformation harvesting, reliable search and instantaneous on demand delivery.\nInformation extraction agents are used to explore and collect data available\nfrom Web, in order to effectively exploit such data for business purposes, such\nas automatic news filtering, advertisement or product searching and price\ncomparing. In this paper, we develop a real-time automatic harvesting agent for\nadverts posted on Servihoo web portal and an SMS-based notification system. It\nuses the URL of the web portal and the object model, i.e., the fields of\ninterests and a set of rules written using the HTML parsing functions to\nextract latest adverts information. The extraction engine executes the\nextraction rules and stores the information in a database to be processed for\nautomatic notification. This intelligent system helps to tremendously save\ntime. It also enables users or potential product buyers to react more quickly\nto changes and newly posted sales adverts, paving the way to real-time best buy\ndeals.\n","text":"Title:Classified Ads Harvesting Agent and Notification System\nAbstract:  The shift from an information society to a knowledge society require rapid\ninformation harvesting, reliable search and instantaneous on demand delivery.\nInformation extraction agents are used to explore and collect data available\nfrom Web, in order to effectively exploit such data for business purposes, such\nas automatic news filtering, advertisement or product searching and price\ncomparing. In this paper, we develop a real-time automatic harvesting agent for\nadverts posted on Servihoo web portal and an SMS-based notification system. It\nuses the URL of the web portal and the object model, i.e., the fields of\ninterests and a set of rules written using the HTML parsing functions to\nextract latest adverts information. The extraction engine executes the\nextraction rules and stores the information in a database to be processed for\nautomatic notification. This intelligent system helps to tremendously save\ntime. It also enables users or potential product buyers to react more quickly\nto changes and newly posted sales adverts, paving the way to real-time best buy\ndeals.\n","vector":null,"chunk_id":"c7f76a6e5b9a792877375f12d9ebde5c"}
{"title":"Linear Time Feature Selection for Regularized Least-Squares","authors":"Tapio Pahikkala and Antti Airola and Tapio Salakoski","category":"stat.ML","abstract":"  We propose a novel algorithm for greedy forward feature selection for\nregularized least-squares (RLS) regression and classification, also known as\nthe least-squares support vector machine or ridge regression. The algorithm,\nwhich we call greedy RLS, starts from the empty feature set, and on each\niteration adds the feature whose addition provides the best leave-one-out\ncross-validation performance. Our method is considerably faster than the\npreviously proposed ones, since its time complexity is linear in the number of\ntraining examples, the number of features in the original data set, and the\ndesired size of the set of selected features. Therefore, as a side effect we\nobtain a new training algorithm for learning sparse linear RLS predictors which\ncan be used for large scale learning. This speed is possible due to matrix\ncalculus based short-cuts for leave-one-out and feature addition. We\nexperimentally demonstrate the scalability of our algorithm and its ability to\nfind good quality feature sets.\n","text":"Title:Linear Time Feature Selection for Regularized Least-Squares\nAbstract:  We propose a novel algorithm for greedy forward feature selection for\nregularized least-squares (RLS) regression and classification, also known as\nthe least-squares support vector machine or ridge regression. The algorithm,\nwhich we call greedy RLS, starts from the empty feature set, and on each\niteration adds the feature whose addition provides the best leave-one-out\ncross-validation performance. Our method is considerably faster than the\npreviously proposed ones, since its time complexity is linear in the number of\ntraining examples, the number of features in the original data set, and the\ndesired size of the set of selected features. Therefore, as a side effect we\nobtain a new training algorithm for learning sparse linear RLS predictors which\ncan be used for large scale learning. This speed is possible due to matrix\ncalculus based short-cuts for leave-one-out and feature addition. We\nexperimentally demonstrate the scalability of our algorithm and its ability to\nfind good quality feature sets.\n","vector":null,"chunk_id":"790614ef1196db331ed115d392a0d207"}
{"title":"Computation of Reducts Using Topology and Measure of Significance of\n  Attributes","authors":"P. G. JansiRani, R. Bhaskaran","category":"cs.IR","abstract":"  Data generated in the fields of science, technology, business and in many\nother fields of research are increasing in an exponential rate. The way to\nextract knowledge from a huge set of data is a challenging task. This paper\naims to propose a hybrid and viable method to deal with an information system\nin data mining, using topological techniques and the significance of the\nattributes measured using rough set theory, to compute the reduct, This will\nreduce the randomness in the process of elimination of redundant attributes,\nwhich, in turn, will reduce the complexity of the computation of reducts of an\ninformation system where a large amount of data have to be processed.\n","text":"Title:Computation of Reducts Using Topology and Measure of Significance of\n  Attributes\nAbstract:  Data generated in the fields of science, technology, business and in many\nother fields of research are increasing in an exponential rate. The way to\nextract knowledge from a huge set of data is a challenging task. This paper\naims to propose a hybrid and viable method to deal with an information system\nin data mining, using topological techniques and the significance of the\nattributes measured using rough set theory, to compute the reduct, This will\nreduce the randomness in the process of elimination of redundant attributes,\nwhich, in turn, will reduce the complexity of the computation of reducts of an\ninformation system where a large amount of data have to be processed.\n","vector":null,"chunk_id":"ab147854ac2e3085359d4a65760aaa1f"}
{"title":"Local Popularity based Page Link Analysis","authors":"C Ravindranath Chowdary","category":"cs.IR","abstract":"  In this paper we introduce the concept of dynamic link pages. A web site/page\ncontains a number of links to other pages. All the links are not equally\nimportant. Few links are more frequently visited and few rarely visited. In\nthis scenario, identifying the frequently used links and placing them in the\ntop left corner of the page will increase the user's satisfaction. This process\nwill reduce the time spent by a visitor on the page, as most of the times, the\npopular links are presented in the visible part of the screen itself. Also, a\nsite can be indexed based on the popular links in that page. This will increase\nthe efficiency of the retrieval system. We presented a model to display the\npopular links, and also proposed a method to increase the quality of retrieval\nsystem.\n","text":"Title:Local Popularity based Page Link Analysis\nAbstract:  In this paper we introduce the concept of dynamic link pages. A web site/page\ncontains a number of links to other pages. All the links are not equally\nimportant. Few links are more frequently visited and few rarely visited. In\nthis scenario, identifying the frequently used links and placing them in the\ntop left corner of the page will increase the user's satisfaction. This process\nwill reduce the time spent by a visitor on the page, as most of the times, the\npopular links are presented in the visible part of the screen itself. Also, a\nsite can be indexed based on the popular links in that page. This will increase\nthe efficiency of the retrieval system. We presented a model to display the\npopular links, and also proposed a method to increase the quality of retrieval\nsystem.\n","vector":null,"chunk_id":"dbda8e6a7885cfae874886554c42dfa6"}
{"title":"On the Schoenberg Transformations in Data Analysis: Theory and\n  Illustrations","authors":"Fran\\c{c}ois Bavaud","category":"stat.ML","abstract":"  The class of Schoenberg transformations, embedding Euclidean distances into\nhigher dimensional Euclidean spaces, is presented, and derived from theorems on\npositive definite and conditionally negative definite matrices. Original\nresults on the arc lengths, angles and curvature of the transformations are\nproposed, and visualized on artificial data sets by classical multidimensional\nscaling. A simple distance-based discriminant algorithm illustrates the theory,\nintimately connected to the Gaussian kernels of Machine Learning.\n","text":"Title:On the Schoenberg Transformations in Data Analysis: Theory and\n  Illustrations\nAbstract:  The class of Schoenberg transformations, embedding Euclidean distances into\nhigher dimensional Euclidean spaces, is presented, and derived from theorems on\npositive definite and conditionally negative definite matrices. Original\nresults on the arc lengths, angles and curvature of the transformations are\nproposed, and visualized on artificial data sets by classical multidimensional\nscaling. A simple distance-based discriminant algorithm illustrates the theory,\nintimately connected to the Gaussian kernels of Machine Learning.\n","vector":null,"chunk_id":"5da5f78e52a5c9cd9294597676af36b4"}
{"title":"Maximal Intersection Queries in Randomized Input Models","authors":"Benjamin Hoffmann, Mikhail Lifshits, Yury Lifshits, Dirk Nowotka","category":"cs.IR","abstract":"  Consider a family of sets and a single set, called the query set. How can one\nquickly find a member of the family which has a maximal intersection with the\nquery set? Time constraints on the query and on a possible preprocessing of the\nset family make this problem challenging. Such maximal intersection queries\narise in a wide range of applications, including web search, recommendation\nsystems, and distributing on-line advertisements. In general, maximal\nintersection queries are computationally expensive. We investigate two\nwell-motivated distributions over all families of sets and propose an algorithm\nfor each of them. We show that with very high probability an almost optimal\nsolution is found in time which is logarithmic in the size of the family.\nMoreover, we point out a threshold phenomenon on the probabilities of\nintersecting sets in each of our two input models which leads to the efficient\nalgorithms mentioned above.\n","text":"Title:Maximal Intersection Queries in Randomized Input Models\nAbstract:  Consider a family of sets and a single set, called the query set. How can one\nquickly find a member of the family which has a maximal intersection with the\nquery set? Time constraints on the query and on a possible preprocessing of the\nset family make this problem challenging. Such maximal intersection queries\narise in a wide range of applications, including web search, recommendation\nsystems, and distributing on-line advertisements. In general, maximal\nintersection queries are computationally expensive. We investigate two\nwell-motivated distributions over all families of sets and propose an algorithm\nfor each of them. We show that with very high probability an almost optimal\nsolution is found in time which is logarithmic in the size of the family.\nMoreover, we point out a threshold phenomenon on the probabilities of\nintersecting sets in each of our two input models which leads to the efficient\nalgorithms mentioned above.\n","vector":null,"chunk_id":"2fed5640b6584ce1987bbe5a3012a20a"}
{"title":"Visualization of Manifold-Valued Elements by Multidimensional Scaling","authors":"Simone Fiori","category":"stat.ML","abstract":"  The present contribution suggests the use of a multidimensional scaling (MDS)\nalgorithm as a visualization tool for manifold-valued elements. A visualization\ntool of this kind is useful in signal processing and machine learning whenever\nlearning/adaptation algorithms insist on high-dimensional parameter manifolds.\n","text":"Title:Visualization of Manifold-Valued Elements by Multidimensional Scaling\nAbstract:  The present contribution suggests the use of a multidimensional scaling (MDS)\nalgorithm as a visualization tool for manifold-valued elements. A visualization\ntool of this kind is useful in signal processing and machine learning whenever\nlearning/adaptation algorithms insist on high-dimensional parameter manifolds.\n","vector":null,"chunk_id":"b6f00ebb076cd7e8001b870f2dade9f9"}
{"title":"Nepotistic Relationships in Twitter and their Impact on Rank Prestige\n  Algorithms","authors":"Daniel Gayo-Avello","category":"cs.IR","abstract":"  Micro-blogging services such as Twitter allow anyone to publish anything,\nanytime. Needless to say, many of the available contents can be diminished as\nbabble or spam. However, given the number and diversity of users, some valuable\npieces of information should arise from the stream of tweets. Thus, such\nservices can develop into valuable sources of up-to-date information (the\nso-called real-time web) provided a way to find the most\nrelevant/trustworthy/authoritative users is available. Hence, this makes a\nhighly pertinent question for which graph centrality methods can provide an\nanswer. In this paper the author offers a comprehensive survey of feasible\nalgorithms for ranking users in social networks, he examines their\nvulnerabilities to linking malpractice in such networks, and suggests an\nobjective criterion against which to compare such algorithms. Additionally, he\nsuggests a first step towards \"desensitizing\" prestige algorithms against\ncheating by spammers and other abusive users.\n","text":"Title:Nepotistic Relationships in Twitter and their Impact on Rank Prestige\n  Algorithms\nAbstract:  Micro-blogging services such as Twitter allow anyone to publish anything,\nanytime. Needless to say, many of the available contents can be diminished as\nbabble or spam. However, given the number and diversity of users, some valuable\npieces of information should arise from the stream of tweets. Thus, such\nservices can develop into valuable sources of up-to-date information (the\nso-called real-time web) provided a way to find the most\nrelevant/trustworthy/authoritative users is available. Hence, this makes a\nhighly pertinent question for which graph centrality methods can provide an\nanswer. In this paper the author offers a comprehensive survey of feasible\nalgorithms for ranking users in social networks, he examines their\nvulnerabilities to linking malpractice in such networks, and suggests an\nobjective criterion against which to compare such algorithms. Additionally, he\nsuggests a first step towards \"desensitizing\" prestige algorithms against\ncheating by spammers and other abusive users.\n","vector":null,"chunk_id":"e5e04aad543297edfe6407d526da4832"}
{"title":"A Survey on Preprocessing Methods for Web Usage Data","authors":"V.Chitraa, Dr. Antony Selvdoss Davamani","category":"cs.IR","abstract":"  World Wide Web is a huge repository of web pages and links. It provides\nabundance of information for the Internet users. The growth of web is\ntremendous as approximately one million pages are added daily. Users' accesses\nare recorded in web logs. Because of the tremendous usage of web, the web log\nfiles are growing at a faster rate and the size is becoming huge. Web data\nmining is the application of data mining techniques in web data. Web Usage\nMining applies mining techniques in log data to extract the behavior of users\nwhich is used in various applications like personalized services, adaptive web\nsites, customer profiling, prefetching, creating attractive web sites etc., Web\nusage mining consists of three phases preprocessing, pattern discovery and\npattern analysis. Web log data is usually noisy and ambiguous and preprocessing\nis an important process before mining. For discovering patterns sessions are to\nbe constructed efficiently. This paper reviews existing work done in the\npreprocessing stage. A brief overview of various data mining techniques for\ndiscovering patterns, and pattern analysis are discussed. Finally a glimpse of\nvarious applications of web usage mining is also presented.\n","text":"Title:A Survey on Preprocessing Methods for Web Usage Data\nAbstract:  World Wide Web is a huge repository of web pages and links. It provides\nabundance of information for the Internet users. The growth of web is\ntremendous as approximately one million pages are added daily. Users' accesses\nare recorded in web logs. Because of the tremendous usage of web, the web log\nfiles are growing at a faster rate and the size is becoming huge. Web data\nmining is the application of data mining techniques in web data. Web Usage\nMining applies mining techniques in log data to extract the behavior of users\nwhich is used in various applications like personalized services, adaptive web\nsites, customer profiling, prefetching, creating attractive web sites etc., Web\nusage mining consists of three phases preprocessing, pattern discovery and\npattern analysis. Web log data is usually noisy and ambiguous and preprocessing\nis an important process before mining. For discovering patterns sessions are to\nbe constructed efficiently. This paper reviews existing work done in the\npreprocessing stage. A brief overview of various data mining techniques for\ndiscovering patterns, and pattern analysis are discussed. Finally a glimpse of\nvarious applications of web usage mining is also presented.\n","vector":null,"chunk_id":"397757500832caae70408bc656188fbe"}
{"title":"Document Clustering using Sequential Information Bottleneck Method","authors":"P.J.Gayathri, S.C. Punitha, M. Punithavalli","category":"cs.IR","abstract":"  This paper illustrates the Principal Direction Divisive Partitioning (PDDP)\nalgorithm and describes its drawbacks and introduces a combinatorial framework\nof the Principal Direction Divisive Partitioning (PDDP) algorithm, then\ndescribes the simplified version of the EM algorithm called the spherical\nGaussian EM (sGEM) algorithm and Information Bottleneck method (IB) is a\ntechnique for finding accuracy, complexity and time space. The PDDP algorithm\nrecursively splits the data samples into two sub clusters using the hyper plane\nnormal to the principal direction derived from the covariance matrix, which is\nthe central logic of the algorithm. However, the PDDP algorithm can yield poor\nresults, especially when clusters are not well separated from one another. To\nimprove the quality of the clustering results problem, it is resolved by\nreallocating new cluster membership using the IB algorithm with different\nsettings. IB Method gives accuracy but time consumption is more. Furthermore,\nbased on the theoretical background of the sGEM algorithm and sequential\nInformation Bottleneck method(sIB), it can be obvious to extend the framework\nto cover the problem of estimating the number of clusters using the Bayesian\nInformation Criterion. Experimental results are given to show the effectiveness\nof the proposed algorithm with comparison to the existing algorithm.\n","text":"Title:Document Clustering using Sequential Information Bottleneck Method\nAbstract:  This paper illustrates the Principal Direction Divisive Partitioning (PDDP)\nalgorithm and describes its drawbacks and introduces a combinatorial framework\nof the Principal Direction Divisive Partitioning (PDDP) algorithm, then\ndescribes the simplified version of the EM algorithm called the spherical\nGaussian EM (sGEM) algorithm and Information Bottleneck method (IB) is a\ntechnique for finding accuracy, complexity and time space. The PDDP algorithm\nrecursively splits the data samples into two sub clusters using the hyper plane\nnormal to the principal direction derived from the covariance matrix, which is\nthe central logic of the algorithm. However, the PDDP algorithm can yield poor\nresults, especially when clusters are not well separated from one another. To\nimprove the quality of the clustering results problem, it is resolved by\nreallocating new cluster membership using the IB algorithm with different\nsettings. IB Method gives accuracy but time consumption is more. Furthermore,\nbased on the theoretical background of the sGEM algorithm and sequential\nInformation Bottleneck method(sIB), it can be obvious to extend the framework\nto cover the problem of estimating the number of clusters using the Bayesian\nInformation Criterion. Experimental results are given to show the effectiveness\nof the proposed algorithm with comparison to the existing algorithm.\n","vector":null,"chunk_id":"40be8393d37cf0f71df4693378236797"}
{"title":"Is This a Good Title?","authors":"Martin Klein, Jeffery Shipman, Michael L. Nelson","category":"cs.IR","abstract":"  Missing web pages, URIs that return the 404 \"Page Not Found\" error or the\nHTTP response code 200 but dereference unexpected content, are ubiquitous in\ntoday's browsing experience. We use Internet search engines to relocate such\nmissing pages and provide means that help automate the rediscovery process. We\npropose querying web pages' titles against search engines. We investigate the\nretrieval performance of titles and compare them to lexical signatures which\nare derived from the pages' content. Since titles naturally represent the\ncontent of a document they intuitively change over time. We measure the edit\ndistance between current titles and titles of copies of the same pages obtained\nfrom the Internet Archive and display their evolution. We further investigate\nthe correlation between title changes and content modifications of a web page\nover time. Lastly we provide a predictive model for the quality of any given\nweb page title in terms of its discovery performance. Our results show that\ntitles return more than 60% URIs top ranked and further relevant content\nreturned in the top 10 results. We show that titles decay slowly but are far\nmore stable than the pages' content. We further distill stop titles than can\nhelp identify insufficiently performing search engine queries.\n","text":"Title:Is This a Good Title?\nAbstract:  Missing web pages, URIs that return the 404 \"Page Not Found\" error or the\nHTTP response code 200 but dereference unexpected content, are ubiquitous in\ntoday's browsing experience. We use Internet search engines to relocate such\nmissing pages and provide means that help automate the rediscovery process. We\npropose querying web pages' titles against search engines. We investigate the\nretrieval performance of titles and compare them to lexical signatures which\nare derived from the pages' content. Since titles naturally represent the\ncontent of a document they intuitively change over time. We measure the edit\ndistance between current titles and titles of copies of the same pages obtained\nfrom the Internet Archive and display their evolution. We further investigate\nthe correlation between title changes and content modifications of a web page\nover time. Lastly we provide a predictive model for the quality of any given\nweb page title in terms of its discovery performance. Our results show that\ntitles return more than 60% URIs top ranked and further relevant content\nreturned in the top 10 results. We show that titles decay slowly but are far\nmore stable than the pages' content. We further distill stop titles than can\nhelp identify insufficiently performing search engine queries.\n","vector":null,"chunk_id":"5bf0660a1f7914a68853702b0365e1c5"}
{"title":"Strong Consistency of Prototype Based Clustering in Probabilistic Space","authors":"Vladimir Nikulin and Geoffrey J. McLachlan","category":"stat.ML","abstract":"  In this paper we formulate in general terms an approach to prove strong\nconsistency of the Empirical Risk Minimisation inductive principle applied to\nthe prototype or distance based clustering. This approach was motivated by the\nDivisive Information-Theoretic Feature Clustering model in probabilistic space\nwith Kullback-Leibler divergence which may be regarded as a special case within\nthe Clustering Minimisation framework. Also, we propose clustering\nregularization restricting creation of additional clusters which are not\nsignificant or are not essentially different comparing with existing clusters.\n","text":"Title:Strong Consistency of Prototype Based Clustering in Probabilistic Space\nAbstract:  In this paper we formulate in general terms an approach to prove strong\nconsistency of the Empirical Risk Minimisation inductive principle applied to\nthe prototype or distance based clustering. This approach was motivated by the\nDivisive Information-Theoretic Feature Clustering model in probabilistic space\nwith Kullback-Leibler divergence which may be regarded as a special case within\nthe Clustering Minimisation framework. Also, we propose clustering\nregularization restricting creation of additional clusters which are not\nsignificant or are not essentially different comparing with existing clusters.\n","vector":null,"chunk_id":"5171086227ea0f71080cea3439dce83a"}
{"title":"A New Approach to Keyphrase Extraction Using Neural Networks","authors":"Kamal Sarkar, Mita Nasipuri, Suranjan Ghose","category":"cs.IR","abstract":"  Keyphrases provide a simple way of describing a document, giving the reader\nsome clues about its contents. Keyphrases can be useful in a various\napplications such as retrieval engines, browsing interfaces, thesaurus\nconstruction, text mining etc.. There are also other tasks for which keyphrases\nare useful, as we discuss in this paper. This paper describes a neural network\nbased approach to keyphrase extraction from scientific articles. Our results\nshow that the proposed method performs better than some state-of-the art\nkeyphrase extraction approaches.\n","text":"Title:A New Approach to Keyphrase Extraction Using Neural Networks\nAbstract:  Keyphrases provide a simple way of describing a document, giving the reader\nsome clues about its contents. Keyphrases can be useful in a various\napplications such as retrieval engines, browsing interfaces, thesaurus\nconstruction, text mining etc.. There are also other tasks for which keyphrases\nare useful, as we discuss in this paper. This paper describes a neural network\nbased approach to keyphrase extraction from scientific articles. Our results\nshow that the proposed method performs better than some state-of-the art\nkeyphrase extraction approaches.\n","vector":null,"chunk_id":"93fdd9b2ba404a94f07191fe2ebf0baf"}
{"title":"Improving Update Summarization by Revisiting the MMR Criterion","authors":"Florian Boudin, Juan-Manuel Torres-Moreno and Marc El-B\\`eze","category":"cs.IR","abstract":"  This paper describes a method for multi-document update summarization that\nrelies on a double maximization criterion. A Maximal Marginal Relevance like\ncriterion, modified and so called Smmr, is used to select sentences that are\nclose to the topic and at the same time, distant from sentences used in already\nread documents. Summaries are then generated by assembling the high ranked\nmaterial and applying some ruled-based linguistic post-processing in order to\nobtain length reduction and maintain coherency. Through a participation to the\nText Analysis Conference (TAC) 2008 evaluation campaign, we have shown that our\nmethod achieves promising results.\n","text":"Title:Improving Update Summarization by Revisiting the MMR Criterion\nAbstract:  This paper describes a method for multi-document update summarization that\nrelies on a double maximization criterion. A Maximal Marginal Relevance like\ncriterion, modified and so called Smmr, is used to select sentences that are\nclose to the topic and at the same time, distant from sentences used in already\nread documents. Summaries are then generated by assembling the high ranked\nmaterial and applying some ruled-based linguistic post-processing in order to\nobtain length reduction and maintain coherency. Through a participation to the\nText Analysis Conference (TAC) 2008 evaluation campaign, we have shown that our\nmethod achieves promising results.\n","vector":null,"chunk_id":"f80f3e4d15b063f3fac767076d582ccb"}
{"title":"Handling Overload Conditions In High Performance Trustworthy Information\n  Retrieval Systems","authors":"Sumalatha Ramachandran, Sharon Joseph, Sujaya Paulraj and Vetriselvi\n  Ramaraj","category":"cs.IR","abstract":"  Web search engines retrieve a vast amount of information for a given search\nquery. But the user needs only trustworthy and high-quality information from\nthis vast retrieved data. The response time of the search engine must be a\nminimum value in order to satisfy the user. An optimum level of response time\nshould be maintained even when the system is overloaded. This paper proposes an\noptimal Load Shedding algorithm which is used to handle overload conditions in\nreal-time data stream applications and is adapted to the Information Retrieval\nSystem of a web search engine. Experiment results show that the proposed\nalgorithm enables a web search engine to provide trustworthy search results to\nthe user within an optimum response time, even during overload conditions.\n","text":"Title:Handling Overload Conditions In High Performance Trustworthy Information\n  Retrieval Systems\nAbstract:  Web search engines retrieve a vast amount of information for a given search\nquery. But the user needs only trustworthy and high-quality information from\nthis vast retrieved data. The response time of the search engine must be a\nminimum value in order to satisfy the user. An optimum level of response time\nshould be maintained even when the system is overloaded. This paper proposes an\noptimal Load Shedding algorithm which is used to handle overload conditions in\nreal-time data stream applications and is adapted to the Information Retrieval\nSystem of a web search engine. Experiment results show that the proposed\nalgorithm enables a web search engine to provide trustworthy search results to\nthe user within an optimum response time, even during overload conditions.\n","vector":null,"chunk_id":"046b559da66ff0a61495ff9e92dfacac"}
{"title":"BiLingual Information Retrieval System for English and Tamil","authors":"S.Saraswathi, Asma Siddhiqaa.M, Kalaimagal.K and Kalaiyarasi.M","category":"cs.IR","abstract":"  This paper addresses the design and implementation of BiLingual Information\nRetrieval system on the domain, Festivals. A generic platform is built for\nBiLingual Information retrieval which can be extended to any foreign or Indian\nlanguage working with the same efficiency. Search for the solution of the query\nis not done in a specific predefined set of standard languages but is chosen\ndynamically on processing the user's query. This paper deals with Indian\nlanguage Tamil apart from English. The task is to retrieve the solution for the\nuser given query in the same language as that of the query. In this process, a\nOntological tree is built for the domain in such a way that there are entries\nin the above listed two languages in every node of the tree. A Part-Of-Speech\n(POS) Tagger is used to determine the keywords from the given query. Based on\nthe context, the keywords are translated to appropriate languages using the\nOntological tree. A search is performed and documents are retrieved based on\nthe keywords. With the use of the Ontological tree, Information Extraction is\ndone. Finally, the solution for the query is translated back to the query\nlanguage (if necessary) and produced to the user.\n","text":"Title:BiLingual Information Retrieval System for English and Tamil\nAbstract:  This paper addresses the design and implementation of BiLingual Information\nRetrieval system on the domain, Festivals. A generic platform is built for\nBiLingual Information retrieval which can be extended to any foreign or Indian\nlanguage working with the same efficiency. Search for the solution of the query\nis not done in a specific predefined set of standard languages but is chosen\ndynamically on processing the user's query. This paper deals with Indian\nlanguage Tamil apart from English. The task is to retrieve the solution for the\nuser given query in the same language as that of the query. In this process, a\nOntological tree is built for the domain in such a way that there are entries\nin the above listed two languages in every node of the tree. A Part-Of-Speech\n(POS) Tagger is used to determine the keywords from the given query. Based on\nthe context, the keywords are translated to appropriate languages using the\nOntological tree. A search is performed and documents are retrieved based on\nthe keywords. With the use of the Ontological tree, Information Extraction is\ndone. Finally, the solution for the query is translated back to the query\nlanguage (if necessary) and produced to the user.\n","vector":null,"chunk_id":"67e6a7f68e9edb816a70bea22f02c563"}
{"title":"MIREX: MapReduce Information Retrieval Experiments","authors":"Djoerd Hiemstra and Claudia Hauff","category":"cs.IR","abstract":"  We propose to use MapReduce to quickly test new retrieval approaches on a\ncluster of machines by sequentially scanning all documents. We present a small\ncase study in which we use a cluster of 15 low cost ma- chines to search a web\ncrawl of 0.5 billion pages showing that sequential scanning is a viable\napproach to running large-scale information retrieval experiments with little\neffort. The code is available to other researchers at:\nhttp://mirex.sourceforge.net\n","text":"Title:MIREX: MapReduce Information Retrieval Experiments\nAbstract:  We propose to use MapReduce to quickly test new retrieval approaches on a\ncluster of machines by sequentially scanning all documents. We present a small\ncase study in which we use a cluster of 15 low cost ma- chines to search a web\ncrawl of 0.5 billion pages showing that sequential scanning is a viable\napproach to running large-scale information retrieval experiments with little\neffort. The code is available to other researchers at:\nhttp://mirex.sourceforge.net\n","vector":null,"chunk_id":"63f550027300d53b5ac9b484db784e54"}
{"title":"Efficient and Effective Spam Filtering and Re-ranking for Large Web\n  Datasets","authors":"Gordon V. Cormack, Mark D. Smucker, and Charles L. A. Clarke","category":"cs.IR","abstract":"  The TREC 2009 web ad hoc and relevance feedback tasks used a new document\ncollection, the ClueWeb09 dataset, which was crawled from the general Web in\nearly 2009. This dataset contains 1 billion web pages, a substantial fraction\nof which are spam --- pages designed to deceive search engines so as to deliver\nan unwanted payload. We examine the effect of spam on the results of the TREC\n2009 web ad hoc and relevance feedback tasks, which used the ClueWeb09 dataset.\nWe show that a simple content-based classifier with minimal training is\nefficient enough to rank the \"spamminess\" of every page in the dataset using a\nstandard personal computer in 48 hours, and effective enough to yield\nsignificant and substantive improvements in the fixed-cutoff precision (estP10)\nas well as rank measures (estR-Precision, StatMAP, MAP) of nearly all submitted\nruns. Moreover, using a set of \"honeypot\" queries the labeling of training data\nmay be reduced to an entirely automatic process. The results of classical\ninformation retrieval methods are particularly enhanced by filtering --- from\namong the worst to among the best.\n","text":"Title:Efficient and Effective Spam Filtering and Re-ranking for Large Web\n  Datasets\nAbstract:  The TREC 2009 web ad hoc and relevance feedback tasks used a new document\ncollection, the ClueWeb09 dataset, which was crawled from the general Web in\nearly 2009. This dataset contains 1 billion web pages, a substantial fraction\nof which are spam --- pages designed to deceive search engines so as to deliver\nan unwanted payload. We examine the effect of spam on the results of the TREC\n2009 web ad hoc and relevance feedback tasks, which used the ClueWeb09 dataset.\nWe show that a simple content-based classifier with minimal training is\nefficient enough to rank the \"spamminess\" of every page in the dataset using a\nstandard personal computer in 48 hours, and effective enough to yield\nsignificant and substantive improvements in the fixed-cutoff precision (estP10)\nas well as rank measures (estR-Precision, StatMAP, MAP) of nearly all submitted\nruns. Moreover, using a set of \"honeypot\" queries the labeling of training data\nmay be reduced to an entirely automatic process. The results of classical\ninformation retrieval methods are particularly enhanced by filtering --- from\namong the worst to among the best.\n","vector":null,"chunk_id":"98589f3fcec12702f99f127bb58e8ffa"}
{"title":"Sparse Linear Identifiable Multivariate Modeling","authors":"Ricardo Henao and Ole Winther","category":"stat.ML","abstract":"  In this paper we consider sparse and identifiable linear latent variable\n(factor) and linear Bayesian network models for parsimonious analysis of\nmultivariate data. We propose a computationally efficient method for joint\nparameter and model inference, and model comparison. It consists of a fully\nBayesian hierarchy for sparse models using slab and spike priors (two-component\ndelta-function and continuous mixtures), non-Gaussian latent factors and a\nstochastic search over the ordering of the variables. The framework, which we\ncall SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and\nbench-marked on artificial and real biological data sets. SLIM is closest in\nspirit to LiNGAM (Shimizu et al., 2006), but differs substantially in\ninference, Bayesian network structure learning and model comparison.\nExperimentally, SLIM performs equally well or better than LiNGAM with\ncomparable computational complexity. We attribute this mainly to the stochastic\nsearch strategy used, and to parsimony (sparsity and identifiability), which is\nan explicit part of the model. We propose two extensions to the basic i.i.d.\nlinear framework: non-linear dependence on observed variables, called SNIM\n(Sparse Non-linear Identifiable Multivariate modeling) and allowing for\ncorrelations between latent variables, called CSLIM (Correlated SLIM), for the\ntemporal and/or spatial data. The source code and scripts are available from\nhttp://cogsys.imm.dtu.dk/slim/.\n","text":"Title:Sparse Linear Identifiable Multivariate Modeling\nAbstract:  In this paper we consider sparse and identifiable linear latent variable\n(factor) and linear Bayesian network models for parsimonious analysis of\nmultivariate data. We propose a computationally efficient method for joint\nparameter and model inference, and model comparison. It consists of a fully\nBayesian hierarchy for sparse models using slab and spike priors (two-component\ndelta-function and continuous mixtures), non-Gaussian latent factors and a\nstochastic search over the ordering of the variables. The framework, which we\ncall SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and\nbench-marked on artificial and real biological data sets. SLIM is closest in\nspirit to LiNGAM (Shimizu et al., 2006), but differs substantially in\ninference, Bayesian network structure learning and model comparison.\nExperimentally, SLIM performs equally well or better than LiNGAM with\ncomparable computational complexity. We attribute this mainly to the stochastic\nsearch strategy used, and to parsimony (sparsity and identifiability), which is\nan explicit part of the model. We propose two extensions to the basic i.i.d.\nlinear framework: non-linear dependence on observed variables, called SNIM\n(Sparse Non-linear Identifiable Multivariate modeling) and allowing for\ncorrelations between latent variables, called CSLIM (Correlated SLIM), for the\ntemporal and/or spatial data. The source code and scripts are available from\nhttp://cogsys.imm.dtu.dk/slim/.\n","vector":null,"chunk_id":"dd41ae70609a0b2855f72948c13eb33e"}
{"title":"Self-Taught Hashing for Fast Similarity Search","authors":"Dell Zhang, Jun Wang, Deng Cai, Jinsong Lu","category":"cs.IR","abstract":"  The ability of fast similarity search at large scale is of great importance\nto many Information Retrieval (IR) applications. A promising way to accelerate\nsimilarity search is semantic hashing which designs compact binary codes for a\nlarge number of documents so that semantically similar documents are mapped to\nsimilar codes (within a short Hamming distance). Although some recently\nproposed techniques are able to generate high-quality codes for documents known\nin advance, obtaining the codes for previously unseen documents remains to be a\nvery challenging problem. In this paper, we emphasise this issue and propose a\nnovel Self-Taught Hashing (STH) approach to semantic hashing: we first find the\noptimal $l$-bit binary codes for all documents in the given corpus via\nunsupervised learning, and then train $l$ classifiers via supervised learning\nto predict the $l$-bit code for any query document unseen before. Our\nexperiments on three real-world text datasets show that the proposed approach\nusing binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine\n(SVM) outperforms state-of-the-art techniques significantly.\n","text":"Title:Self-Taught Hashing for Fast Similarity Search\nAbstract:  The ability of fast similarity search at large scale is of great importance\nto many Information Retrieval (IR) applications. A promising way to accelerate\nsimilarity search is semantic hashing which designs compact binary codes for a\nlarge number of documents so that semantically similar documents are mapped to\nsimilar codes (within a short Hamming distance). Although some recently\nproposed techniques are able to generate high-quality codes for documents known\nin advance, obtaining the codes for previously unseen documents remains to be a\nvery challenging problem. In this paper, we emphasise this issue and propose a\nnovel Self-Taught Hashing (STH) approach to semantic hashing: we first find the\noptimal $l$-bit binary codes for all documents in the given corpus via\nunsupervised learning, and then train $l$ classifiers via supervised learning\nto predict the $l$-bit code for any query document unseen before. Our\nexperiments on three real-world text datasets show that the proposed approach\nusing binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine\n(SVM) outperforms state-of-the-art techniques significantly.\n","vector":null,"chunk_id":"050cb1db478180e79623ce4b8931ae06"}
{"title":"Node-Context Network Clustering using PARAFAC Tensor Decomposition","authors":"Andri Mirzal and Masashi Furukawa","category":"cs.IR","abstract":"  We describe a clustering method for labeled link network (semantic graph)\nthat can be used to group important nodes (highly connected nodes) with their\nrelevant link's labels by using PARAFAC tensor decomposition. In this kind of\nnetwork, the adjacency matrix can not be used to fully describe all information\nabout the network structure. We have to expand the matrix into 3-way adjacency\ntensor, so that not only the information about to which nodes a node connects\nto but by which link's labels is also included. And by applying PARAFAC\ndecomposition on this tensor, we get two lists, nodes and link's labels with\nscores attached to each node and labels, for each decomposition group. So\nclustering process to get the important nodes along with their relevant labels\ncan be done simply by sorting the lists in decreasing order. To test the\nmethod, we construct labeled link network by using blog's dataset, where the\nblogs are the nodes and labeled links are the shared words among them. The\nsimilarity measures between the results and standard measures look promising,\nespecially for two most important tasks, finding the most relevant words to\nblogs query and finding the most similar blogs to blogs query, about 0.87.\n","text":"Title:Node-Context Network Clustering using PARAFAC Tensor Decomposition\nAbstract:  We describe a clustering method for labeled link network (semantic graph)\nthat can be used to group important nodes (highly connected nodes) with their\nrelevant link's labels by using PARAFAC tensor decomposition. In this kind of\nnetwork, the adjacency matrix can not be used to fully describe all information\nabout the network structure. We have to expand the matrix into 3-way adjacency\ntensor, so that not only the information about to which nodes a node connects\nto but by which link's labels is also included. And by applying PARAFAC\ndecomposition on this tensor, we get two lists, nodes and link's labels with\nscores attached to each node and labels, for each decomposition group. So\nclustering process to get the important nodes along with their relevant labels\ncan be done simply by sorting the lists in decreasing order. To test the\nmethod, we construct labeled link network by using blog's dataset, where the\nblogs are the nodes and labeled links are the shared words among them. The\nsimilarity measures between the results and standard measures look promising,\nespecially for two most important tasks, finding the most relevant words to\nblogs query and finding the most similar blogs to blogs query, about 0.87.\n","vector":null,"chunk_id":"d574162c19ada3f790c0054373954b6b"}
{"title":"Training linear ranking SVMs in linearithmic time using red-black trees","authors":"Antti Airola, Tapio Pahikkala, Tapio Salakoski","category":"stat.ML","abstract":"  We introduce an efficient method for training the linear ranking support\nvector machine. The method combines cutting plane optimization with red-black\ntree based approach to subgradient calculations, and has O(m*s+m*log(m)) time\ncomplexity, where m is the number of training examples, and s the average\nnumber of non-zero features per example. Best previously known training\nalgorithms achieve the same efficiency only for restricted special cases,\nwhereas the proposed approach allows any real valued utility scores in the\ntraining data. Experiments demonstrate the superior scalability of the proposed\napproach, when compared to the fastest existing RankSVM implementations.\n","text":"Title:Training linear ranking SVMs in linearithmic time using red-black trees\nAbstract:  We introduce an efficient method for training the linear ranking support\nvector machine. The method combines cutting plane optimization with red-black\ntree based approach to subgradient calculations, and has O(m*s+m*log(m)) time\ncomplexity, where m is the number of training examples, and s the average\nnumber of non-zero features per example. Best previously known training\nalgorithms achieve the same efficiency only for restricted special cases,\nwhereas the proposed approach allows any real valued utility scores in the\ntraining data. Experiments demonstrate the superior scalability of the proposed\napproach, when compared to the fastest existing RankSVM implementations.\n","vector":null,"chunk_id":"e4582513237f22b38bb1706090098883"}
{"title":"Performance Oriented Query Processing In GEO Based Location Search\n  Engines","authors":"M. Umamaheswari, S. Sivasubramanian","category":"cs.IR","abstract":"  Geographic location search engines allow users to constrain and order search\nresults in an intuitive manner by focusing a query on a particular geographic\nregion. Geographic search technology, also called location search, has recently\nreceived significant interest from major search engine companies. Academic\nresearch in this area has focused primarily on techniques for extracting\ngeographic knowledge from the web. In this paper, we study the problem of\nefficient query processing in scalable geographic search engines. Query\nprocessing is a major bottleneck in standard web search engines, and the main\nreason for the thousands of machines used by the major engines. Geographic\nsearch engine query processing is different in that it requires a combination\nof text and spatial data processing techniques. We propose several algorithms\nfor efficient query processing in geographic search engines, integrate them\ninto an existing web search query processor, and evaluate them on large sets of\nreal data and query traces.\n","text":"Title:Performance Oriented Query Processing In GEO Based Location Search\n  Engines\nAbstract:  Geographic location search engines allow users to constrain and order search\nresults in an intuitive manner by focusing a query on a particular geographic\nregion. Geographic search technology, also called location search, has recently\nreceived significant interest from major search engine companies. Academic\nresearch in this area has focused primarily on techniques for extracting\ngeographic knowledge from the web. In this paper, we study the problem of\nefficient query processing in scalable geographic search engines. Query\nprocessing is a major bottleneck in standard web search engines, and the main\nreason for the thousands of machines used by the major engines. Geographic\nsearch engine query processing is different in that it requires a combination\nof text and spatial data processing techniques. We propose several algorithms\nfor efficient query processing in geographic search engines, integrate them\ninto an existing web search query processor, and evaluate them on large sets of\nreal data and query traces.\n","vector":null,"chunk_id":"76b46efea55d76584d8fd584cc77ca18"}
{"title":"Improving the Johnson-Lindenstrauss Lemma","authors":"Javier Rojo and Tuan Nguyen","category":"stat.ML","abstract":"  The Johnson-Lindenstrauss Lemma allows for the projection of $n$ points in\n$p-$dimensional Euclidean space onto a $k-$dimensional Euclidean space, with $k\n\\ge \\frac{24\\ln \\emph{n}}{3\\epsilon^2-2\\epsilon^3}$, so that the pairwise\ndistances are preserved within a factor of $1\\pm\\epsilon$. Here, working\ndirectly with the distributions of the random distances rather than resorting\nto the moment generating function technique, an improvement on the lower bound\nfor $k$ is obtained. The additional reduction in dimension when compared to\nbounds found in the literature, is at least $13\\%$, and, in some cases, up to\n$30\\%$ additional reduction is achieved. Using the moment generating function\ntechnique, we further provide a lower bound for $k$ using pairwise $L_2$\ndistances in the space of points to be projected and pairwise $L_1$ distances\nin the space of the projected points. Comparison with the results obtained in\nthe literature shows that the bound presented here provides an additional\n$36-40\\%$ reduction.\n","text":"Title:Improving the Johnson-Lindenstrauss Lemma\nAbstract:  The Johnson-Lindenstrauss Lemma allows for the projection of $n$ points in\n$p-$dimensional Euclidean space onto a $k-$dimensional Euclidean space, with $k\n\\ge \\frac{24\\ln \\emph{n}}{3\\epsilon^2-2\\epsilon^3}$, so that the pairwise\ndistances are preserved within a factor of $1\\pm\\epsilon$. Here, working\ndirectly with the distributions of the random distances rather than resorting\nto the moment generating function technique, an improvement on the lower bound\nfor $k$ is obtained. The additional reduction in dimension when compared to\nbounds found in the literature, is at least $13\\%$, and, in some cases, up to\n$30\\%$ additional reduction is achieved. Using the moment generating function\ntechnique, we further provide a lower bound for $k$ using pairwise $L_2$\ndistances in the space of points to be projected and pairwise $L_1$ distances\nin the space of the projected points. Comparison with the results obtained in\nthe literature shows that the bound presented here provides an additional\n$36-40\\%$ reduction.\n","vector":null,"chunk_id":"d465ba6645239ed2692f69e826e39a8d"}
{"title":"Refinements of Universal Approximation Results for Deep Belief Networks\n  and Restricted Boltzmann Machines","authors":"Guido Montufar and Nihat Ay","category":"stat.ML","abstract":"  We improve recently published results about resources of Restricted Boltzmann\nMachines (RBM) and Deep Belief Networks (DBN) required to make them Universal\nApproximators. We show that any distribution p on the set of binary vectors of\nlength n can be arbitrarily well approximated by an RBM with k-1 hidden units,\nwhere k is the minimal number of pairs of binary vectors differing in only one\nentry such that their union contains the support set of p. In important cases\nthis number is half of the cardinality of the support set of p. We construct a\nDBN with 2^n/2(n-b), b ~ log(n), hidden layers of width n that is capable of\napproximating any distribution on {0,1}^n arbitrarily well. This confirms a\nconjecture presented by Le Roux and Bengio 2010.\n","text":"Title:Refinements of Universal Approximation Results for Deep Belief Networks\n  and Restricted Boltzmann Machines\nAbstract:  We improve recently published results about resources of Restricted Boltzmann\nMachines (RBM) and Deep Belief Networks (DBN) required to make them Universal\nApproximators. We show that any distribution p on the set of binary vectors of\nlength n can be arbitrarily well approximated by an RBM with k-1 hidden units,\nwhere k is the minimal number of pairs of binary vectors differing in only one\nentry such that their union contains the support set of p. In important cases\nthis number is half of the cardinality of the support set of p. We construct a\nDBN with 2^n/2(n-b), b ~ log(n), hidden layers of width n that is capable of\napproximating any distribution on {0,1}^n arbitrarily well. This confirms a\nconjecture presented by Le Roux and Bengio 2010.\n","vector":null,"chunk_id":"b43a6c94dbff6708a8537633ef1c3073"}
{"title":"Clustering Time Series Data Stream - A Literature Survey","authors":"V.Kavitha, M. Punithavalli","category":"cs.IR","abstract":"  Mining Time Series data has a tremendous growth of interest in today's world.\nTo provide an indication various implementations are studied and summarized to\nidentify the different problems in existing applications. Clustering time\nseries is a trouble that has applications in an extensive assortment of fields\nand has recently attracted a large amount of research. Time series data are\nfrequently large and may contain outliers. In addition, time series are a\nspecial type of data set where elements have a temporal ordering. Therefore\nclustering of such data stream is an important issue in the data mining\nprocess. Numerous techniques and clustering algorithms have been proposed\nearlier to assist clustering of time series data streams. The clustering\nalgorithms and its effectiveness on various applications are compared to\ndevelop a new method to solve the existing problem. This paper presents a\nsurvey on various clustering algorithms available for time series datasets.\nMoreover, the distinctiveness and restriction of previous research are\ndiscussed and several achievable topics for future study are recognized.\nFurthermore the areas that utilize time series clustering are also summarized.\n","text":"Title:Clustering Time Series Data Stream - A Literature Survey\nAbstract:  Mining Time Series data has a tremendous growth of interest in today's world.\nTo provide an indication various implementations are studied and summarized to\nidentify the different problems in existing applications. Clustering time\nseries is a trouble that has applications in an extensive assortment of fields\nand has recently attracted a large amount of research. Time series data are\nfrequently large and may contain outliers. In addition, time series are a\nspecial type of data set where elements have a temporal ordering. Therefore\nclustering of such data stream is an important issue in the data mining\nprocess. Numerous techniques and clustering algorithms have been proposed\nearlier to assist clustering of time series data streams. The clustering\nalgorithms and its effectiveness on various applications are compared to\ndevelop a new method to solve the existing problem. This paper presents a\nsurvey on various clustering algorithms available for time series datasets.\nMoreover, the distinctiveness and restriction of previous research are\ndiscussed and several achievable topics for future study are recognized.\nFurthermore the areas that utilize time series clustering are also summarized.\n","vector":null,"chunk_id":"f9336f761dec8d242da2e7001d045bda"}
{"title":"A Restful Approach for Managing Citizen profiles Using A Semantic\n  Support","authors":"Luis Alvarez Sabucedo and Luis Anido Rifon","category":"cs.IR","abstract":"  Several steps are missing in the current high-speed race towards the holistic\nsupport of citizen needs in the domain of eGovernment. This paper is focused on\nhow to provide support for the citizen profile. This profile, in a wide sense,\nincludes personal information as well documents in possession of the citizen.\nThis also involves the provision of those mechanisms required to publish,\naccess and submit the convenient information to a Public Administration in due\ncurse of a transactional services provided with the last one. Main features of\nthe system are related to interoperability and possibilities for its inclusion\nin a cost effective manner in already developed platforms. To make that\npossible, this approach will take full advantage of semantic technologies and\nthe RESTful paradigm to design the entire system. The paper presents the\noverall system with some notes on the deployment of the solution for its\nfurther reuse in similar contexts.\n","text":"Title:A Restful Approach for Managing Citizen profiles Using A Semantic\n  Support\nAbstract:  Several steps are missing in the current high-speed race towards the holistic\nsupport of citizen needs in the domain of eGovernment. This paper is focused on\nhow to provide support for the citizen profile. This profile, in a wide sense,\nincludes personal information as well documents in possession of the citizen.\nThis also involves the provision of those mechanisms required to publish,\naccess and submit the convenient information to a Public Administration in due\ncurse of a transactional services provided with the last one. Main features of\nthe system are related to interoperability and possibilities for its inclusion\nin a cost effective manner in already developed platforms. To make that\npossible, this approach will take full advantage of semantic technologies and\nthe RESTful paradigm to design the entire system. The paper presents the\noverall system with some notes on the deployment of the solution for its\nfurther reuse in similar contexts.\n","vector":null,"chunk_id":"b229711adf41f5a07be0f46c829b4666"}
{"title":"On the Fly Query Entity Decomposition Using Snippets","authors":"David J. Brenes, Daniel Gayo-Avello and Rodrigo Garcia","category":"cs.IR","abstract":"  One of the most important issues in Information Retrieval is inferring the\nintents underlying users' queries. Thus, any tool to enrich or to better\ncontextualized queries can proof extremely valuable. Entity extraction,\nprovided it is done fast, can be one of such tools. Such techniques usually\nrely on a prior training phase involving large datasets. That training is\ncostly, specially in environments which are increasingly moving towards real\ntime scenarios where latency to retrieve fresh informacion should be minimal.\nIn this paper an `on-the-fly' query decomposition method is proposed. It uses\nsnippets which are mined by means of a na\\\"ive statistical algorithm. An\ninitial evaluation of such a method is provided, in addition to a discussion on\nits applicability to different scenarios.\n","text":"Title:On the Fly Query Entity Decomposition Using Snippets\nAbstract:  One of the most important issues in Information Retrieval is inferring the\nintents underlying users' queries. Thus, any tool to enrich or to better\ncontextualized queries can proof extremely valuable. Entity extraction,\nprovided it is done fast, can be one of such tools. Such techniques usually\nrely on a prior training phase involving large datasets. That training is\ncostly, specially in environments which are increasingly moving towards real\ntime scenarios where latency to retrieve fresh informacion should be minimal.\nIn this paper an `on-the-fly' query decomposition method is proposed. It uses\nsnippets which are mined by means of a na\\\"ive statistical algorithm. An\ninitial evaluation of such a method is provided, in addition to a discussion on\nits applicability to different scenarios.\n","vector":null,"chunk_id":"f6c1ab6baef2a0fa8a0cb32c385ed234"}
{"title":"An Algorithm to Self-Extract Secondary Keywords and Their Combinations\n  Based on Abstracts Collected using Primary Keywords from Online Digital\n  Libraries","authors":"Natarajan Meghanathan, Nataliya Kostyuk, Raphael Isokpehi, Hari Cohly","category":"cs.IR","abstract":"  The high-level contribution of this paper is the development and\nimplementation of an algorithm to selfextract secondary keywords and their\ncombinations (combo words) based on abstracts collected using standard primary\nkeywords for research areas from reputed online digital libraries like IEEE\nExplore, PubMed Central and etc. Given a collection of N abstracts, we\narbitrarily select M abstracts (M<< N; M/N as low as 0.15) and parse each of\nthe M abstracts, word by word. Upon the first-time appearance of a word, we\nquery the user for classifying the word into an Accept-List or non-Accept-List.\nThe effectiveness of the training approach is evaluated by measuring the\npercentage of words for which the user is queried for classification when the\nalgorithm parses through the words of each of the M abstracts. We observed that\nas M grows larger, the percentage of words for which the user is queried for\nclassification reduces drastically. After the list of acceptable words is built\nby parsing the M abstracts, we now parse all the N abstracts, word by word, and\ncount the frequency of appearance of each of the words in Accept-List in these\nN abstracts. We also construct a Combo-Accept-List comprising of all possible\ncombinations of the single keywords in Accept-List and parse all the N\nabstracts, two successive words (combo word) at a time, and count the frequency\nof appearance of each of the combo words in the Combo-Accept-List in these N\nabstracts.\n","text":"Title:An Algorithm to Self-Extract Secondary Keywords and Their Combinations\n  Based on Abstracts Collected using Primary Keywords from Online Digital\n  Libraries\nAbstract:  The high-level contribution of this paper is the development and\nimplementation of an algorithm to selfextract secondary keywords and their\ncombinations (combo words) based on abstracts collected using standard primary\nkeywords for research areas from reputed online digital libraries like IEEE\nExplore, PubMed Central and etc. Given a collection of N abstracts, we\narbitrarily select M abstracts (M<< N; M/N as low as 0.15) and parse each of\nthe M abstracts, word by word. Upon the first-time appearance of a word, we\nquery the user for classifying the word into an Accept-List or non-Accept-List.\nThe effectiveness of the training approach is evaluated by measuring the\npercentage of words for which the user is queried for classification when the\nalgorithm parses through the words of each of the M abstracts. We observed that\nas M grows larger, the percentage of words for which the user is queried for\nclassification reduces drastically. After the list of acceptable words is built\nby parsing the M abstracts, we now parse all the N abstracts, word by word, and\ncount the frequency of appearance of each of the words in Accept-List in these\nN abstracts. We also construct a Combo-Accept-List comprising of all possible\ncombinations of the single keywords in Accept-List and parse all the N\nabstracts, two successive words (combo word) at a time, and count the frequency\nof appearance of each of the combo words in the Combo-Accept-List in these N\nabstracts.\n","vector":null,"chunk_id":"18b930df2ec2263851009ca372d9f72b"}
{"title":"Stability Approach to Regularization Selection (StARS) for High\n  Dimensional Graphical Models","authors":"Han Liu, Kathryn Roeder, Larry Wasserman","category":"stat.ML","abstract":"  A challenging problem in estimating high-dimensional graphical models is to\nchoose the regularization parameter in a data-dependent way. The standard\ntechniques include $K$-fold cross-validation ($K$-CV), Akaike information\ncriterion (AIC), and Bayesian information criterion (BIC). Though these methods\nwork well for low-dimensional problems, they are not suitable in high\ndimensional settings. In this paper, we present StARS: a new stability-based\nmethod for choosing the regularization parameter in high dimensional inference\nfor undirected graphs. The method has a clear interpretation: we use the least\namount of regularization that simultaneously makes a graph sparse and\nreplicable under random sampling. This interpretation requires essentially no\nconditions. Under mild conditions, we show that StARS is partially sparsistent\nin terms of graph estimation: i.e. with high probability, all the true edges\nwill be included in the selected model even when the graph size diverges with\nthe sample size. Empirically, the performance of StARS is compared with the\nstate-of-the-art model selection procedures, including $K$-CV, AIC, and BIC, on\nboth synthetic data and a real microarray dataset. StARS outperforms all these\ncompeting procedures.\n","text":"Title:Stability Approach to Regularization Selection (StARS) for High\n  Dimensional Graphical Models\nAbstract:  A challenging problem in estimating high-dimensional graphical models is to\nchoose the regularization parameter in a data-dependent way. The standard\ntechniques include $K$-fold cross-validation ($K$-CV), Akaike information\ncriterion (AIC), and Bayesian information criterion (BIC). Though these methods\nwork well for low-dimensional problems, they are not suitable in high\ndimensional settings. In this paper, we present StARS: a new stability-based\nmethod for choosing the regularization parameter in high dimensional inference\nfor undirected graphs. The method has a clear interpretation: we use the least\namount of regularization that simultaneously makes a graph sparse and\nreplicable under random sampling. This interpretation requires essentially no\nconditions. Under mild conditions, we show that StARS is partially sparsistent\nin terms of graph estimation: i.e. with high probability, all the true edges\nwill be included in the selected model even when the graph size diverges with\nthe sample size. Empirically, the performance of StARS is compared with the\nstate-of-the-art model selection procedures, including $K$-CV, AIC, and BIC, on\nboth synthetic data and a real microarray dataset. StARS outperforms all these\ncompeting procedures.\n","vector":null,"chunk_id":"b73eb261270c70d72e1dcbc04ccf7184"}
{"title":"Fast and accurate annotation of short texts with Wikipedia pages","authors":"Paolo Ferragina and Ugo Scaiella","category":"cs.IR","abstract":"  We address the problem of cross-referencing text fragments with Wikipedia\npages, in a way that synonymy and polysemy issues are resolved accurately and\nefficiently. We take inspiration from a recent flow of work [Cucerzan 2007,\nMihalcea and Csomai 2007, Milne and Witten 2008, Chakrabarti et al 2009], and\nextend their scenario from the annotation of long documents to the annotation\nof short texts, such as snippets of search-engine results, tweets, news, blogs,\netc.. These short and poorly composed texts pose new challenges in terms of\nefficiency and effectiveness of the annotation process, that we address by\ndesigning and engineering TAGME, the first system that performs an accurate and\non-the-fly annotation of these short textual fragments. A large set of\nexperiments shows that TAGME outperforms state-of-the-art algorithms when they\nare adapted to work on short texts and it results fast and competitive on long\ntexts.\n","text":"Title:Fast and accurate annotation of short texts with Wikipedia pages\nAbstract:  We address the problem of cross-referencing text fragments with Wikipedia\npages, in a way that synonymy and polysemy issues are resolved accurately and\nefficiently. We take inspiration from a recent flow of work [Cucerzan 2007,\nMihalcea and Csomai 2007, Milne and Witten 2008, Chakrabarti et al 2009], and\nextend their scenario from the annotation of long documents to the annotation\nof short texts, such as snippets of search-engine results, tweets, news, blogs,\netc.. These short and poorly composed texts pose new challenges in terms of\nefficiency and effectiveness of the annotation process, that we address by\ndesigning and engineering TAGME, the first system that performs an accurate and\non-the-fly annotation of these short textual fragments. A large set of\nexperiments shows that TAGME outperforms state-of-the-art algorithms when they\nare adapted to work on short texts and it results fast and competitive on long\ntexts.\n","vector":null,"chunk_id":"45c4a017e99bcad9792fced7537c699e"}
{"title":"Gaussian Mixture Modeling with Gaussian Process Latent Variable Models","authors":"Hannes Nickisch and Carl Edward Rasmussen","category":"stat.ML","abstract":"  Density modeling is notoriously difficult for high dimensional data. One\napproach to the problem is to search for a lower dimensional manifold which\ncaptures the main characteristics of the data. Recently, the Gaussian Process\nLatent Variable Model (GPLVM) has successfully been used to find low\ndimensional manifolds in a variety of complex data. The GPLVM consists of a set\nof points in a low dimensional latent space, and a stochastic map to the\nobserved space. We show how it can be interpreted as a density model in the\nobserved space. However, the GPLVM is not trained as a density model and\ntherefore yields bad density estimates. We propose a new training strategy and\nobtain improved generalisation performance and better density estimates in\ncomparative evaluations on several benchmark data sets.\n","text":"Title:Gaussian Mixture Modeling with Gaussian Process Latent Variable Models\nAbstract:  Density modeling is notoriously difficult for high dimensional data. One\napproach to the problem is to search for a lower dimensional manifold which\ncaptures the main characteristics of the data. Recently, the Gaussian Process\nLatent Variable Model (GPLVM) has successfully been used to find low\ndimensional manifolds in a variety of complex data. The GPLVM consists of a set\nof points in a low dimensional latent space, and a stochastic map to the\nobserved space. We show how it can be interpreted as a density model in the\nobserved space. However, the GPLVM is not trained as a density model and\ntherefore yields bad density estimates. We propose a new training strategy and\nobtain improved generalisation performance and better density estimates in\ncomparative evaluations on several benchmark data sets.\n","vector":null,"chunk_id":"374667abb04c0765ec4dbc5dc0bc19f0"}
{"title":"Few Algorithms for ascertaining merit of a document and their\n  applications","authors":"Ka.Shrinivaasan","category":"cs.IR","abstract":"  Existing models for ranking documents(mostly in world wide web) are prestige\nbased. In this article, three algorithms to objectively judge the merit of a\ndocument are proposed - 1) Citation graph maxflow 2) Recursive Gloss Overlap\nbased intrinsic merit scoring and 3) Interview algorithm. A short discussion on\ngeneric judgement and its mathematical treatment is presented in introduction\nto motivate these algorithms.\n","text":"Title:Few Algorithms for ascertaining merit of a document and their\n  applications\nAbstract:  Existing models for ranking documents(mostly in world wide web) are prestige\nbased. In this article, three algorithms to objectively judge the merit of a\ndocument are proposed - 1) Citation graph maxflow 2) Recursive Gloss Overlap\nbased intrinsic merit scoring and 3) Interview algorithm. A short discussion on\ngeneric judgement and its mathematical treatment is presented in introduction\nto motivate these algorithms.\n","vector":null,"chunk_id":"7143998bd3584a3e06046ea615bf73bb"}
{"title":"Studies on Relevance, Ranking and Results Display","authors":"Judith Gelernter, Dong Cao and Jaime Carbonell","category":"cs.IR","abstract":"  This study considers the extent to which users with the same query agree as\nto what is relevant, and how what is considered relevant may translate into a\nretrieval algorithm and results display. To combine user perceptions of\nrelevance with algorithm rank and to present results, we created a prototype\ndigital library of scholarly literature. We confine studies to one population\nof scientists (paleontologists), one domain of scholarly scientific articles\n(paleo-related), and a prototype system (PaleoLit) that we built for the\npurpose. Based on the principle that users do not pre-suppose answers to a\ngiven query but that they will recognize what they want when they see it, our\nsystem uses a rules-based algorithm to cluster results into fuzzy categories\nwith three relevance levels. Our system matches at least 1/3 of our\nparticipants' relevancy ratings 87% of the time. Our subsequent usability study\nfound that participants trusted our uncertainty labels but did not value our\ncolor-coded horizontal results layout above a standard retrieval list. We posit\nthat users make such judgments in limited time, and that time optimization per\ntask might help explain some of our findings.\n","text":"Title:Studies on Relevance, Ranking and Results Display\nAbstract:  This study considers the extent to which users with the same query agree as\nto what is relevant, and how what is considered relevant may translate into a\nretrieval algorithm and results display. To combine user perceptions of\nrelevance with algorithm rank and to present results, we created a prototype\ndigital library of scholarly literature. We confine studies to one population\nof scientists (paleontologists), one domain of scholarly scientific articles\n(paleo-related), and a prototype system (PaleoLit) that we built for the\npurpose. Based on the principle that users do not pre-suppose answers to a\ngiven query but that they will recognize what they want when they see it, our\nsystem uses a rules-based algorithm to cluster results into fuzzy categories\nwith three relevance levels. Our system matches at least 1/3 of our\nparticipants' relevancy ratings 87% of the time. Our subsequent usability study\nfound that participants trusted our uncertainty labels but did not value our\ncolor-coded horizontal results layout above a standard retrieval list. We posit\nthat users make such judgments in limited time, and that time optimization per\ntask might help explain some of our findings.\n","vector":null,"chunk_id":"f94b1937e8e924397ddaaedc0b8b8f98"}
{"title":"Approaches, Challenges and Future Direction of Image Retrieval","authors":"Hui Hui Wang, Dzulkifli Mohamad and N. A. Ismail","category":"cs.IR","abstract":"  This paper attempts to discuss the evolution of the retrieval approaches\nfocusing on development, challenges and future direction of the image\nretrieval. It highlights both the already addressed and outstanding issues. The\nexplosive growth of image data leads to the need of research and development of\nImage Retrieval. However, Image retrieval researches are moving from keyword,\nto low level features and to semantic features. Drive towards semantic features\nis due to the problem of the keywords which can be very subjective and time\nconsuming while low level features cannot always describe high level concepts\nin the users' mind. Hence, introducing an interpretation inconsistency between\nimage descriptors and high level semantics that known as the semantic gap. This\npaper also discusses the semantic gap issues, user query mechanisms as well as\ncommon ways used to bridge the gap in image retrieval.\n","text":"Title:Approaches, Challenges and Future Direction of Image Retrieval\nAbstract:  This paper attempts to discuss the evolution of the retrieval approaches\nfocusing on development, challenges and future direction of the image\nretrieval. It highlights both the already addressed and outstanding issues. The\nexplosive growth of image data leads to the need of research and development of\nImage Retrieval. However, Image retrieval researches are moving from keyword,\nto low level features and to semantic features. Drive towards semantic features\nis due to the problem of the keywords which can be very subjective and time\nconsuming while low level features cannot always describe high level concepts\nin the users' mind. Hence, introducing an interpretation inconsistency between\nimage descriptors and high level semantics that known as the semantic gap. This\npaper also discusses the semantic gap issues, user query mechanisms as well as\ncommon ways used to bridge the gap in image retrieval.\n","vector":null,"chunk_id":"da00685819bc198c0b4476afc254c4bf"}
{"title":"Large scale link based latent Dirichlet allocation for web document\n  classification","authors":"Istv\\'an B\\'ir\\'o and J\\'acint Szab\\'o","category":"cs.IR","abstract":"  In this paper we demonstrate the applicability of latent Dirichlet allocation\n(LDA) for classifying large Web document collections. One of our main results\nis a novel influence model that gives a fully generative model of the document\ncontent taking linkage into account. In our setup, topics propagate along links\nin such a way that linked documents directly influence the words in the linking\ndocument. As another main contribution we develop LDA specific boosting of\nGibbs samplers resulting in a significant speedup in our experiments. The\ninferred LDA model can be applied for classification as dimensionality\nreduction similarly to latent semantic indexing. In addition, the model yields\nlink weights that can be applied in algorithms to process the Web graph; as an\nexample we deploy LDA link weights in stacked graphical learning. By using\nWeka's BayesNet classifier, in terms of the AUC of classification, we achieve\n4% improvement over plain LDA with BayesNet and 18% over tf.idf with SVM. Our\nGibbs sampling strategies yield about 5-10 times speedup with less than 1%\ndecrease in accuracy in terms of likelihood and AUC of classification.\n","text":"Title:Large scale link based latent Dirichlet allocation for web document\n  classification\nAbstract:  In this paper we demonstrate the applicability of latent Dirichlet allocation\n(LDA) for classifying large Web document collections. One of our main results\nis a novel influence model that gives a fully generative model of the document\ncontent taking linkage into account. In our setup, topics propagate along links\nin such a way that linked documents directly influence the words in the linking\ndocument. As another main contribution we develop LDA specific boosting of\nGibbs samplers resulting in a significant speedup in our experiments. The\ninferred LDA model can be applied for classification as dimensionality\nreduction similarly to latent semantic indexing. In addition, the model yields\nlink weights that can be applied in algorithms to process the Web graph; as an\nexample we deploy LDA link weights in stacked graphical learning. By using\nWeka's BayesNet classifier, in terms of the AUC of classification, we achieve\n4% improvement over plain LDA with BayesNet and 18% over tf.idf with SVM. Our\nGibbs sampling strategies yield about 5-10 times speedup with less than 1%\ndecrease in accuracy in terms of likelihood and AUC of classification.\n","vector":null,"chunk_id":"a18088fc4566d651fa148b2c67cb0589"}
{"title":"Euclidean Distances, soft and spectral Clustering on Weighted Graphs","authors":"Fran\\c{c}ois Bavaud","category":"stat.ML","abstract":"  We define a class of Euclidean distances on weighted graphs, enabling to\nperform thermodynamic soft graph clustering. The class can be constructed form\nthe \"raw coordinates\" encountered in spectral clustering, and can be extended\nby means of higher-dimensional embeddings (Schoenberg transformations).\nGeographical flow data, properly conditioned, illustrate the procedure as well\nas visualization aspects.\n","text":"Title:Euclidean Distances, soft and spectral Clustering on Weighted Graphs\nAbstract:  We define a class of Euclidean distances on weighted graphs, enabling to\nperform thermodynamic soft graph clustering. The class can be constructed form\nthe \"raw coordinates\" encountered in spectral clustering, and can be extended\nby means of higher-dimensional embeddings (Schoenberg transformations).\nGeographical flow data, properly conditioned, illustrate the procedure as well\nas visualization aspects.\n","vector":null,"chunk_id":"8bd44c7adbc4e09b4a7b467ea58b3233"}
{"title":"Clustering Stability: An Overview","authors":"Ulrike von Luxburg","category":"stat.ML","abstract":"  A popular method for selecting the number of clusters is based on stability\narguments: one chooses the number of clusters such that the corresponding\nclustering results are \"most stable\". In recent years, a series of papers has\nanalyzed the behavior of this method from a theoretical point of view. However,\nthe results are very technical and difficult to interpret for non-experts. In\nthis paper we give a high-level overview about the existing literature on\nclustering stability. In addition to presenting the results in a slightly\ninformal but accessible way, we relate them to each other and discuss their\ndifferent implications.\n","text":"Title:Clustering Stability: An Overview\nAbstract:  A popular method for selecting the number of clusters is based on stability\narguments: one chooses the number of clusters such that the corresponding\nclustering results are \"most stable\". In recent years, a series of papers has\nanalyzed the behavior of this method from a theoretical point of view. However,\nthe results are very technical and difficult to interpret for non-experts. In\nthis paper we give a high-level overview about the existing literature on\nclustering stability. In addition to presenting the results in a slightly\ninformal but accessible way, we relate them to each other and discuss their\ndifferent implications.\n","vector":null,"chunk_id":"5eac617277a045bea2e1cde8be08648a"}
{"title":"Directional Statistics on Permutations","authors":"Sergey M. Plis and Terran Lane and Vince D. Calhoun","category":"stat.ML","abstract":"  Distributions over permutations arise in applications ranging from\nmulti-object tracking to ranking of instances. The difficulty of dealing with\nthese distributions is caused by the size of their domain, which is factorial\nin the number of considered entities ($n!$). It makes the direct definition of\na multinomial distribution over permutation space impractical for all but a\nvery small $n$. In this work we propose an embedding of all $n!$ permutations\nfor a given $n$ in a surface of a hypersphere defined in\n$\\mathbbm{R}^{(n-1)^2}$. As a result of the embedding, we acquire ability to\ndefine continuous distributions over a hypersphere with all the benefits of\ndirectional statistics. We provide polynomial time projections between the\ncontinuous hypersphere representation and the $n!$-element permutation space.\nThe framework provides a way to use continuous directional probability\ndensities and the methods developed thereof for establishing densities over\npermutations. As a demonstration of the benefits of the framework we derive an\ninference procedure for a state-space model over permutations. We demonstrate\nthe approach with applications.\n","text":"Title:Directional Statistics on Permutations\nAbstract:  Distributions over permutations arise in applications ranging from\nmulti-object tracking to ranking of instances. The difficulty of dealing with\nthese distributions is caused by the size of their domain, which is factorial\nin the number of considered entities ($n!$). It makes the direct definition of\na multinomial distribution over permutation space impractical for all but a\nvery small $n$. In this work we propose an embedding of all $n!$ permutations\nfor a given $n$ in a surface of a hypersphere defined in\n$\\mathbbm{R}^{(n-1)^2}$. As a result of the embedding, we acquire ability to\ndefine continuous distributions over a hypersphere with all the benefits of\ndirectional statistics. We provide polynomial time projections between the\ncontinuous hypersphere representation and the $n!$-element permutation space.\nThe framework provides a way to use continuous directional probability\ndensities and the methods developed thereof for establishing densities over\npermutations. As a demonstration of the benefits of the framework we derive an\ninference procedure for a state-space model over permutations. We demonstrate\nthe approach with applications.\n","vector":null,"chunk_id":"12f48d3e10db1b56be20032e3eae9c5f"}
{"title":"Reduced Rank Vector Generalized Linear Models for Feature Extraction","authors":"Yiyuan She","category":"stat.ML","abstract":"  Supervised linear feature extraction can be achieved by fitting a reduced\nrank multivariate model. This paper studies rank penalized and rank constrained\nvector generalized linear models. From the perspective of thresholding rules,\nwe build a framework for fitting singular value penalized models and use it for\nfeature extraction. Through solving the rank constraint form of the problem, we\npropose progressive feature space reduction for fast computation in high\ndimensions with little performance loss. A novel projective cross-validation is\nproposed for parameter tuning in such nonconvex setups. Real data applications\nare given to show the power of the methodology in supervised dimension\nreduction and feature extraction.\n","text":"Title:Reduced Rank Vector Generalized Linear Models for Feature Extraction\nAbstract:  Supervised linear feature extraction can be achieved by fitting a reduced\nrank multivariate model. This paper studies rank penalized and rank constrained\nvector generalized linear models. From the perspective of thresholding rules,\nwe build a framework for fitting singular value penalized models and use it for\nfeature extraction. Through solving the rank constraint form of the problem, we\npropose progressive feature space reduction for fast computation in high\ndimensions with little performance loss. A novel projective cross-validation is\nproposed for parameter tuning in such nonconvex setups. Real data applications\nare given to show the power of the methodology in supervised dimension\nreduction and feature extraction.\n","vector":null,"chunk_id":"3b3a12fc028f1446e4110131cec31fd9"}
{"title":"Support Vector Machines for Additive Models: Consistency and Robustness","authors":"Andreas Christmann, Robert Hable","category":"stat.ML","abstract":"  Support vector machines (SVMs) are special kernel based methods and belong to\nthe most successful learning methods since more than a decade. SVMs can\ninformally be described as a kind of regularized M-estimators for functions and\nhave demonstrated their usefulness in many complicated real-life problems.\nDuring the last years a great part of the statistical research on SVMs has\nconcentrated on the question how to design SVMs such that they are universally\nconsistent and statistically robust for nonparametric classification or\nnonparametric regression purposes. In many applications, some qualitative prior\nknowledge of the distribution P or of the unknown function f to be estimated is\npresent or the prediction function with a good interpretability is desired,\nsuch that a semiparametric model or an additive model is of interest.\n  In this paper we mainly address the question how to design SVMs by choosing\nthe reproducing kernel Hilbert space (RKHS) or its corresponding kernel to\nobtain consistent and statistically robust estimators in additive models. We\ngive an explicit construction of kernels - and thus of their RKHSs - which\nleads in combination with a Lipschitz continuous loss function to consistent\nand statistically robust SMVs for additive models. Examples are quantile\nregression based on the pinball loss function, regression based on the\nepsilon-insensitive loss function, and classification based on the hinge loss\nfunction.\n","text":"Title:Support Vector Machines for Additive Models: Consistency and Robustness\nAbstract:  Support vector machines (SVMs) are special kernel based methods and belong to\nthe most successful learning methods since more than a decade. SVMs can\ninformally be described as a kind of regularized M-estimators for functions and\nhave demonstrated their usefulness in many complicated real-life problems.\nDuring the last years a great part of the statistical research on SVMs has\nconcentrated on the question how to design SVMs such that they are universally\nconsistent and statistically robust for nonparametric classification or\nnonparametric regression purposes. In many applications, some qualitative prior\nknowledge of the distribution P or of the unknown function f to be estimated is\npresent or the prediction function with a good interpretability is desired,\nsuch that a semiparametric model or an additive model is of interest.\n  In this paper we mainly address the question how to design SVMs by choosing\nthe reproducing kernel Hilbert space (RKHS) or its corresponding kernel to\nobtain consistent and statistically robust estimators in additive models. We\ngive an explicit construction of kernels - and thus of their RKHSs - which\nleads in combination with a Lipschitz continuous loss function to consistent\nand statistically robust SMVs for additive models. Examples are quantile\nregression based on the pinball loss function, regression based on the\nepsilon-insensitive loss function, and classification based on the hinge loss\nfunction.\n","vector":null,"chunk_id":"47129f646de9fef3e2d0104c5bc85f52"}
